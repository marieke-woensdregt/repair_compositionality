{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-08-06 11:19:34  \n",
    "\n",
    "# The evolution of compositionality under environmental noise\n",
    "\n",
    "\n",
    "\n",
    "## Hypothesis:\n",
    "\n",
    "**We hypothesise that** other-initiated repair and a compositional language co-evolve under the following assumptions:\n",
    "\n",
    "1. There is a pressure for communicative success (or more specifically: that the listener makes an effort to find a complete meaning to interpret given the speaker's utterance)\n",
    "2. Listeners have at least two types of repair initiator at their disposal: (i) open request, and (ii) restricted request\n",
    "3. There is a pressure to make repair sequences as efficient as possible (in terms of combined utterance length of repair initiator and response)\n",
    "    \n",
    "This hypothesis is based on the idea that if repair sequences are under a pressure to be efficient, then it becomes useful to be able to initiate repair by asking for clarification of just _part_ of the utterance, specifically by feeding back the part of the meaning that you _did_ get. Under those circumstances, a compositional system is helpful (compared to a holistic system), because parts of the signal (i.e. 'substrings') map to parts of the meaning. _**<span class=\"girk\">Note</span>**_ that this hypothesis is thus based on the assumption that the 'closed request' type of repair initiator works only if the listener has _understood_ part of the _meaning_ of the signal, as opposed to just having received part of the _signal_ and being able to repeat that back verbatim, in order to prompt the speaker to repeat the part that didn't come through. We feel that this is a reasonable assumption based on what we know from empirical work, but it is a little hard to assess given that natural language _is_ compositional.\n",
    "\n",
    "There are of course parts of language that are holistic though. Mark D. and I thought of the example of proper nouns. For instance, if A says \"My favourite writer is Chimamanda Ngozi Adichie\", B could say \"Chima _who_ ?\" in order to prompt A to repeat the name. However, our intuition is that it would be quite unnatural for A to then respond to B's repair request by saying only \"manda Ngozi Adichie\". Most likely A would simply repeat the whole name.\n",
    "\n",
    "When I was talking to Simon Kirby about it being difficult to think about whether repair could work as efficiently in a holistic system, he thought of colour terms as an example of a holistic part of language. And as an example he gave the repair sequence: \n",
    "\n",
    "A: \"the blue shirt\"\n",
    "\n",
    "B: \"you mean the turquoise one?\"\n",
    "\n",
    "In this example, the issue with the holistic signal \"blue\" is not that part of it was covered by noise, but that it is (apparently) not specific enough to pick out one particular referent. So it's not quite the same as what we were thinking about above: which is the question whether one can do efficient repair (i.e. a restricted request) when given a holistic signal that was partly disturbed by noise, in such a way that the listener does not get even _part_ of the meaning. Whereas in the example above, the listener receives the full signal without noise, and therefore is able to interpret part of the meaning (i.e. exclude some potential referents, such as the red shirt), but the problem is that they cannot quite narrow it down to a single unambiguous referent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-08-06 11:19:27 \n",
    "\n",
    "## Predictions:\n",
    "\n",
    "Here we adapt the model of Kirby et al. (2015)---where they show that a compositional language evolves when there is a pressure for both (i) learnability and (ii) expressivity---to incorporate noise (step 1) and repair (step 2).\n",
    "\n",
    "In Kirby et al.'s model, the pressure for expressivity (implemented as speakers aiming to avoid ambiguity when they produce a signal) is what causes populations to move towards languages that have one-to-one mappings between meanings and signals, but these can be either compositional or holistic. The pressure for learnability then (implemented as a prior that favours more compressible languages) is what causes populations to move towards compositional languages rather than holistic languages, because compositional languages are more compressible (and therefore, is the assumption, easier to learn). \n",
    "\n",
    "### Strong prediction (excluding pressure for learnability):\n",
    "\n",
    "Therefore, the strongest prediction given our reasoning above, would be that if we take away the pressure for learnability, but add in environmental noise and the possibility of repair (combined with a pressure for efficient repair sequences), populations would also converge on compositional languages. This may be a bit of a weird prediction in itself, because we _do_ believe a pressure for learnability will have played a role. However, by isolating the effects of the two pressures (learnability vs. noise+repair) we could say something about their individual contributions.\n",
    "\n",
    "### 'Weaker' prediction (including pressure for learnability):\n",
    "\n",
    "Not entirely sure what to predict here. Maybe the effects of a noise+repair+efficiency pressure on the one hand, and a pressure for learnability on the other hand, will be additive? So that when combined, they cause populations to converge on a compositional language faster than when only one of these pressures is at play?\n",
    "\n",
    "\n",
    "### Which predictions to test?\n",
    "\n",
    "We'd like to explore both predictions, starting with the stronger one.\n",
    "\n",
    "### References  \n",
    "Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87â€“102. https://doi.org/10.1016/j.cognition.2015.03.016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-08-26 17:29:31 \n",
    "\n",
    "### Question: How to implement the pressure for communicative success that we have in mind?\n",
    "\n",
    "What I'm wondering here is whether the pressure for expressivity, as it was implemented by Kirby et al. (2015), in combination with a pressure for efficiency (and the possibility of repair), will have the effect of a pressure for communicative success as we have in mind, or whether we'll have to incorporate an extra or different pressure for communicative success.\n",
    "\n",
    "Firstly, the way the pressure for expressivity is implemented by Kirby et al. (2015):\n",
    "speakers try to avoid using ambiguous signals. ---> this is sufficient for pushing populations towards having either holistic or compositional languages (as opposed to degenerate ones). \n",
    "\n",
    "---> What could the effect of environmental noise be in this? If data consists of <meaning, form> pairs that were produced by a pair of agents from the previous generation, as in the Kirby et al. (2015) model, noise _without_ repair will mean that learners sometimes receive incomplete data (i.e. an incomplete form). Meanings in the <meaning, form> pairs that make up the data should not be incomplete, because (if I understand the Kirby et al., 2015, paper correctly) these represent the speaker's communicative intention, rather than the listener's interpretation.\n",
    "\n",
    "---> What will that mean for how likely learners are to infer different languages? In general we'd just expect that learners will need more observations in order to reach a given amount of posterior belief in the correct language hypothesis. This means that given a particular bottleneck width, populations who are exposed to noise will be more likely to transition from one language into another than populations who aren't. Will the addition of noise push learners in any particular direction though? I can't think of why that would be the case if learners have a flat prior. However, if learners have a compression-based prior as in the Kirby et al. (2015) model, they would be pushed towards degenerate languages in general, in the same way as it happens in Kirby et al.\n",
    "\n",
    "---> The pressure for communicative success that Mark D. and I had in mind (I think) is that speaker and hearer try to cooperate to reach mutual understanding; that is, so that the speaker can get their intended meaning across. **<span class=\"mark\">Design decision:</span>** I think this pressure is captured if we assume that the hearer will initiate repair whenever (i) they are aware that noise happened, and (ii) they can't derive a single unambiguous meaning based on the form they received. The first condition is a bit artificial I think, because in natural language people do initiate repair even if they have received the full signal perfectly, but can't disambiguate the meaning of (some part of) it. <span class=\"mark\">Maybe a better model would be if the listener at least _considers_ initiating repair every time they cannot find a single unique meaning that corresponds to the speaker's utterance, but that whether or not the listener _actually_ initiates repair depends on (i) just how ambiguous the meaning still is, and (ii) the cost of the required repair sequence (assuming an open request is more costly overall than a restricted request?)?</span>\n",
    "\n",
    "\n",
    "Assuming we run with the first model (where listeners only initiate repair if they know that noise happened), this would mean that a hearer will not have to initiate repair even when they received an incomplete (i.e. noisy) form. That is, if the hearer is using a language of the 'other' type, in which there is only one form that starts with a 'b', and that form maps to one particular meaning, then they will be able to infer a full meaning, and will therefore not initiate repair. However, such a language will necessarily also contain some degenerate forms, such as 'aa' mapping to both meaning '02' and meaning '03'. _That_ is where the pressure for expressivity comes in: speakers will try to avoid using such ambiguous forms, and therefore learners will be more likely to infer either a holistic or compositional language when receiving data from such a language of the 'other' type.\n",
    "\n",
    "---> So in our 'strong prediction' scenario, where we _replace_ Kirby et al.'s learnability pressure with a pressure for efficient repair (and assuming repair is only initiated when the listener knows that noise happened), we'd expect that populations get pushed in the direction of languages of the 'other' type, as well as languages of the compositional type rather than languages of the degenerate type. I'm not sure which language type would come out as best fit to a pressure for efficient repair however: 'other' or 'compositional'. So let's work through an example: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-08-26 17:43:38 \n",
    "\n",
    "### Worked example to figure out how each language type would fare under pressure for efficient repair.\n",
    "\n",
    "Here's a language of the 'other' type which might be specifically useful under a pressure for efficient repair:\n",
    "- 02 --> aa\n",
    "- 03 --> ab\n",
    "- 12 --> aa\n",
    "- 13 --> ba\n",
    "\n",
    "1. Noisy form a_ --> open request (because a_ does not map reliably to either meaning 0_ or 1_)\n",
    "2. Noisy form b_ --> no repair (because it maps unambiguously to meaning 13)\n",
    "3. Noisy form \\_a --> open request (because \\_a does not map reliably to either meaning \\_2 or \\_3)\n",
    "4. Noisy form \\_b --> no repair (because it maps unambiguously to meaning 03)\n",
    "\n",
    "So all in all, a listener using this language would initiate the more costly form of repair in 50% of the cases where there is noise, and no repair at all in 50% of the cases where there's noise.\n",
    "\n",
    "Now let's compare this to a case of a listener who uses a compositional language, such as this one:\n",
    "- 02 --> aa\n",
    "- 03 --> ab\n",
    "- 12 --> ba\n",
    "- 13 --> bb\n",
    "\n",
    "1. Noisy form a_ --> restricted request (because a_ maps unambiguously to meaning 0_)\n",
    "2. Noisy form b_ --> restricted request (because b_ maps unambiguously to meaning 0_)\n",
    "3. Noisy form \\_a --> restricted request (because \\_a maps unambiguously to meaning \\_2)\n",
    "4. Noisy form \\_b --> restricted request (because \\_b maps unambiguously to meaning \\_3)\n",
    "\n",
    "So which language type allows for the most efficient repair depends on how costly we assume open requests are compared to restricted requests. If, for simplicity's sake, we assume an open request is twice as costly as a restricted request (say c=1.0 for an open request, c=0.5 for a restricted request, and c=0.0 for no repair), a compositional language would be _exactly_ as efficient as the other-type language above. Specifically, the average cost of repair for the other-type language would be: $(0.25*1.0)+(0.25*0.0)+(0.25*1.0)+(0.25*0.0) = 0.5$. And the average cost of repair for the compositional language would be: $(0.25*0.5)+(0.25*0.5)+(0.25*0.5)+(0.25*0.5) = 0.5$.\n",
    "\n",
    "However, <span class=\"mark\">if we assume that there is some baseline cost for doing repair at all (which seems more reasonable to me?)</span>, so that the cost of an open versus a restricted request are relatively closer together than the cost of a restricted request versus no repair at all, the other-type language above would win from the compositional language in terms of repair efficiency. To give a simple example, imagine that no repair costs 0.0, a restricted request costs 0.75, and an open request costs 1.0. Then the average cost of repair for the other-type language would still be: $(0.25*1.0)+(0.25*0.0)+(0.25*1.0)+(0.25*0.0) = 0.5$, whereas the average cost of repair of the compositional language would now be: $(0.25*0.75)+(0.25*0.75)+(0.25*0.75)+(0.25*0.75) = 0.75$.\n",
    "\n",
    "<span class=\"mark\">Perhaps we could base how we parameterise these costs on some empirical findings?</span>\n",
    "\n",
    "Combining our pressure for efficient repair with Kirby et al.'s pressure for expressivity, should then get rid of the languages of the 'other' type. However, the extent to which this happens should depend on the relative strengths of the pressure for expressivity and the pressure for efficiency.\n",
    "\n",
    "---> So I guess what I'm saying is that in our model, the pressure for communicative success should be extended somewhat, to not only consist of the (parameterised) assumption that speakers try to avoid ambiguity, but also the assumption that listeners try to arrive at a complete, unambiguous interpretation, rather than just settling for a (wholly or partly) ambiguous meaning.\n",
    "\n",
    "<span class=\"mark\">**Importantly however**</span>, this analysis does depend on the assumption that the listener is aware of whether noise occured or not, and only initiates repair when that is the case, not for disambiguation in general. If listeners also initiated repair for _general_ disambiguation purposes, in the absence of noise (even though their language has a homonym in it, which one could assume the listener could be aware of?), a listener who uses the 'other' type language above would also initiate repair _whenever_ they receive the 'aa' form. This would mean they would also initiate repair 50% of the time in the _absence_ of noise, making the overall rate of repair higher for languages of the 'other' type than for languages of the 'compositional' type.\n",
    "\n",
    "\n",
    "Now let's work through a similar example comparing languages of the 'holistic' and 'compositional' type:\n",
    "First, an example of a holistic language:\n",
    "- 02 --> aa\n",
    "- 03 --> bb\n",
    "- 12 --> ba\n",
    "- 13 --> ab\n",
    "\n",
    "Here, the listener would also have to initiate repair in all cases where there is partial noise, just like with a compositional language. However, repair is less efficient because even if noise is only partial, the listener will have to use an open request in all cases, because they cannot make it more specific, because they don't have any part of the meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How and where to implement efficiency pressure on repair?\n",
    "\n",
    "It makes most sense that this pressure would be at play during communication, rather than during learning. We can't assume that learners prefer languages that allow for more efficient repair because they have some kind of a priori awareness of how efficient each of the languages would be in the case that repair is needed.\n",
    "\n",
    "Therefore, instead of being located in the learner's prior, as the learnability pressure in the Kirby et al. (2015) model, the efficiency pressure should act somewhere during communication, just like Kirby et al.'s expressivity pressure. We could start by creating a simple cost function, where an open request is more costly than a restricted request (and exactly how much more costly it is could be regulated by some parameter?), and we could make the listener's process of whether or not to initiate repair a stochastic one, where they are less likely to initiate repair the more costly the repair sequence is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-09-06 13:20:50 \n",
    "\n",
    "## Decisions about how to implement repair made on 05/09/2019:\n",
    "\n",
    "- A listener should initiate repair probabilistically, depending on the amount of ambiguity (i.e. uncertainty about the intended referent) that is left after receiving the signal (this could also be expressed as the inverse of the amount of bits of information that the signal provides)\n",
    "- The reason for initiating repair is therefore always to reduce ambiguity, not just because noise happened. That means that if the listener received a noisy form but is still able to narrow their interpretation down to a single meaning, they will never initiate repair. Similarly, if the listener uses a degenerate language to interpret utterances, they will always initiate repair, even if they've received a complete form. We can however cap the number of repeated repair initiators within a given interaction to 3, because that's where we see people stop trying in real linguistic interactions as well.\n",
    "- What is the cost of repair? No repair should have no cost (in terms of efficiency), while an open request should have a slightly higher cost than a restricted request. The reasoning behind that being that in a restricted request sequence, parts of the trouble source turn can be 're-used': we assume that the listener can only use a restricted request if they have understood at least part of the meaning (which, in the case of our simple toy languages, is equivalent to being able to narrow down the possible interpretations to two candidates). The listener can then ask for clarification about only the other part of the meaning, which means that the speaker only has to repeat part of the signal.\n",
    "\n",
    "- Another idea (that might be worth looking into later) is that the costs of repair could be expressed in terms of how many repair sequences are necessary within a given interaction. There you would expect that the costs of repair are higher when using a degenerate language than when using a different type of language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2019-08-26 17:29:10 2019-07-31 15:27:33 \n",
    "\n",
    "## Production in the Kirby et al. (2015) model:\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l\\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $f$ stands for form (in the sense of a complete signal, such as *aa*), $l$ stands for language, $t$ stands for topic, $a$ stands for ambiguity (i.e. the number of meanings in $M$ that map to form $f$ in $l$), $\\gamma$ specifies the extent to which ambiguous utterances are penalised, and $\\epsilon$ stands for production error.\n",
    "\n",
    "\n",
    "From Kirby et al. (2015, pp. 92-93):\n",
    "> \"If $a$ = 1 ($f$ is unambiguous) and/or $\\gamma$ = 0 then this yields a model of production where the 'correct' form is produced with probability $1-\\epsilon$. However, when $\\gamma$ > 0 and $f$ is ambiguous (i.e., $a$ > 1), then the 'correct' mapping from $t$ to $f$ is less likely to be produced (the probability $P(f \\mid l, t)$ is reduced by the factor $(\\frac{1}{a})^\\gamma$ ) and the remaining probability mass is spread equally over the other possible forms, leading to increased probability of producing $f'$ $\\neq$ $f$. Therefore, $\\gamma$ > 0 introduces a penalty for languages whose utterances are ambiguous.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2019-07-31 15:27:52 \n",
    "## How do we add *environmental* noise to this production model?\n",
    "\n",
    "\n",
    "If we leave Kirby et al.'s production error $\\epsilon$ aside for a moment, we could add environmental noise by allowing production to have two possible outcomes. Given a particular topic *t*, a speaker could produce either:\n",
    "- form $f$ if $t$ is mapped to $f$ in $l$, with probability $1-n$ (where $n$ stands for the probability of noise happening)\n",
    "- form $f_{noisy}$, where $f_{noisy}$ is a 'noisy variant' of $f$ *and* $t$ is mapped to $f$ in $l$. This should happen with probability $\\frac{n}{|F_{noisy}|}$ (where $F_{noisy}$ is the full set of possible noisy variants of the form $f$ that maps to $t$ in $l$)\n",
    "\n",
    "To give an example, using the example compositional grammar from Kirby et al. (2015, p. 92):\n",
    "\n",
    "S   --> A B   \n",
    "A:0 --> a   \n",
    "A:1 --> b   \n",
    "B:2 --> a   \n",
    "B:3 --> b   \n",
    "\n",
    "Given this grammar, the only form that maps to topic 02 is *aa*.   \n",
    "_**<span class=\"mark\"><span class=\"mark\">Design decision:</span></span>**_ If we allow for both 'partial' and 'full' noise,   \n",
    " this form has three possible noisy variants:\n",
    "- a_\n",
    "- _a\n",
    "- \\__\n",
    "\n",
    "where _ stands for a noisy part of the signal that the listener could not perceive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span class=\"girk\">_**Decision**_ Mark D. + Marieke (01/08/2019):</span> Only partial noise is enough for now. We do want to allow for both open and restricted repair requests being a possibility, because the intuition that our hypothesis is based on, is that whereas a compositional system allows for the usage of both open and restricted requests, a holistic system only allows for open requests. However, to allow for open requests we don't necessarily need a form that is entirely noise (i.e. \\_\\_); we only need to assume that if the listener is able to disambiguate _part_ of the meaning, they use a restricted request, and if they cannot disambiguate any of the meaning they us an open request.\n",
    "\n",
    "So if 02 is the speaker's intended topic (and if we exclude the possibility of the speaker making a production error for now), the speaker will produce the following signals with the following probabilities:\n",
    "- aa with probability $1-n$\n",
    "- [a_, \\_a] with probability $\\frac{n}{2}$ (because there are 2 possible noisy variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's add the probability of making a production error $\\epsilon$ back in, which gives us: \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\, (1-n) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact} \\\\ \n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\; \\; \\textrm{if part of} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact} \\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\, (1-n) \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact}\\\\\n",
    "\\frac{\\epsilon}{|F|-1} \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\quad \\quad \\quad \\textrm{if no part of} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where *n* stands for the probability of noise.\n",
    "\n",
    "<span class=\"burk\">Actually, the probability for when f is _not_ mapped to t, _and_ f is a noisy variant, is not quite as simple as stated above.</span> Depending on how many forms f there are that _don't_ map to t, some of those error forms will generate the same noisy variant. For example, if the form that _is_ mapped to the intended topic is 'aa', and the forms ['ab', 'ba', 'bb'] form the set of forms that _aren't_ mapped to t, some noisy variants will only occur given one of these error forms being what the speaker produced ('a_' in this case), while others will occur given multiple possible error forms that the speaker might produce (e.g. '\\_b' can be a noisy variant of either 'ab' or 'bb'). The probabilities for the different noisy variants should reflect this. (I.e. in the example above, noisy variant '\\_b' should be twice as probable as noisy variant 'a_'.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-07-31 15:28:14 \n",
    "\n",
    "## First, let's reproduce Kirby et al.'s (2015) model of production:\n",
    "\n",
    "In order to do that we first need to set some general parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.212070Z",
     "start_time": "2019-09-09T14:44:19.208009Z"
    }
   },
   "outputs": [],
   "source": [
    "# First some parameters:\n",
    "meanings = ['02', '03', '12', '13']  # all possible meanings\n",
    "forms_without_noise = ['aa', 'ab', 'ba', 'bb']  # all possible forms, excluding their possible 'noisy variants'\n",
    "noisy_forms = ['a_', 'b_', '_a', '_b']  # all possible noisy variants of the forms above\n",
    "all_forms_including_noisy_variants = forms_without_noise+noisy_forms  # all possible forms, including both complete \n",
    "                                                                    # forms and noisy variants\n",
    "error = 0.05  # the probability of making a production error\n",
    "gamma = 2  # parameter that determines strength of ambiguity penalty (Kirby et al. used gamma = 2 for communication\n",
    "            # condition and gamma = 0 for condition without communication)\n",
    "turnover = True  # determines whether new individuals enter the population or not\n",
    "b = 20  # the bottleneck (i.e. number of meaning-form pairs the each pair gets to see during training (Kirby et al.\n",
    "        # used a bottleneck of 20 in the body of the paper.\n",
    "rounds = 1*b  # Kirby et al. (2015) used rounds = 2*b, but SimLang lab 21 uses 1*b\n",
    "popsize = 2  # If I understand it correctly, Kirby et al. (2015) used a population size of 2: each generation is simply\n",
    "            #  a pair of agents.\n",
    "runs = 50  # the number of independent simulation runs (Kirby et al., 2015 used 100)\n",
    "gens = 150  # the number of generations (Kirby et al., 2015 used 100)\n",
    "noise = False  # parameter that determines whether environmental noise is on or off\n",
    "noise_prob = 0.1  # the probability of environmental noise masking part of an utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need some functions to generate all possible languages and classify languages according to the categories specified by Kirby et al. (2015):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.300517Z",
     "start_time": "2019-09-09T14:44:19.290931Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Some functions to create and classify all possible languages:\n",
    "def create_all_possible_languages(meanings, forms):\n",
    "    \"\"\"Creates all possible languages\n",
    "\n",
    "    :param meanings: list of strings corresponding to all possible meanings\n",
    "    :type meanings: list\n",
    "    :param forms: list of strings corresponding to all possible forms_without_noisy_variants\n",
    "    :type forms: list\n",
    "    :returns: list of tuples which represent languages, where each tuple consists of forms_without_noisy_variants and\n",
    "    has length len(meanings)\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    all_possible_languages = list(\n",
    "        itertools.product(forms, repeat=len(meanings)))\n",
    "    return all_possible_languages\n",
    "\n",
    "\n",
    "def classify_language(lang, forms, meanings):\n",
    "    \"\"\"\n",
    "    Classify one particular language as either 'degenerate' (0), 'holistic' (1), 'other' (2)\n",
    "    or 'compositional' (3) (Kirby et al., 2015)\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :type lang: tuple\n",
    "    :param forms: list of strings corresponding to all possible forms_without_noisy_variants\n",
    "    :type forms: list\n",
    "    :returns: integer corresponding to category that language belongs to:\n",
    "    0 = degenerate, 1 = holistic, 2 = other, 3 = compositional (here I'm following the\n",
    "    numbering used in SimLang lab 21)\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    # TODO: See if I can modify this function so that it can deal with any number of forms_without_noisy_variants and\n",
    "    #  meanings.\n",
    "    class_degenerate = 0\n",
    "    class_holistic = 1\n",
    "    class_other = 2\n",
    "    class_compositional = 3\n",
    "    # First check whether some conditions are met, bc this function hasn't been coded up in the most general way yet:\n",
    "    if len(forms) != 4:\n",
    "        raise ValueError(\n",
    "            \"This function only works for a world in which there are 4 possible forms_without_noisy_variants\"\n",
    "        )\n",
    "    if len(forms[0]) != 2:\n",
    "        raise ValueError(\n",
    "            \"This function only works when each form consists of 2 elements\")\n",
    "    if len(lang) != len(meanings):\n",
    "        raise ValueError(\"Lang should have same length as meanings\")\n",
    "\n",
    "    # lang is degenerate if it uses the same form for every meaning:\n",
    "    if lang[0] == lang[1] and lang[1] == lang[2] and lang[2] == lang[3]:\n",
    "        return class_degenerate\n",
    "\n",
    "    # lang is compositional if it makes use of all possible forms_without_noisy_variants, *and* each form element maps\n",
    "    # to the same meaning element for each form:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[\n",
    "        3] in lang and lang[0][0] == lang[1][0] and lang[2][0] == lang[3][0] and lang[0][\n",
    "        1] == lang[2][1] and lang[1][1] == lang[3][1]:\n",
    "        return class_compositional\n",
    "\n",
    "    # lang is holistic if it is *not* compositional, but *does* make use of all possible forms_without_noisy_variants:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[3] in lang:\n",
    "        return class_holistic\n",
    "\n",
    "    # In all other cases, a language belongs to the 'other' category:\n",
    "    else:\n",
    "        return class_other\n",
    "\n",
    "\n",
    "def classify_all_languages(language_list):\n",
    "    \"\"\"\n",
    "    Classify all languages as either 'degenerate' (0), 'holistic' (1), 'other' (2) or 'compositional' (3) \n",
    "    (Kirby et al., 2015)\n",
    "\n",
    "    :param language_list: list of all languages\n",
    "    :type language_list: list\n",
    "    :returns: 1D numpy array containing integer corresponding to category of corresponding\n",
    "    language index: 0 = degenerate, 1 = holistic, 2 = other, 3 = compositional\n",
    "    (here I'm following the numbering used in SimLang lab 21)\n",
    "    :rtype: 1D numpy array\n",
    "    \"\"\"\n",
    "    class_per_lang = np.zeros(len(language_list))\n",
    "    for l in range(len(language_list)):\n",
    "        class_per_lang[l] = classify_language(language_list[l], forms_without_noise, meanings)\n",
    "    return class_per_lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.304933Z",
     "start_time": "2019-09-09T14:44:19.302071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible languages is:\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "# Let's try out our create_all_possible_languages() function:\n",
    "\n",
    "all_possible_languages = create_all_possible_languages(meanings, forms_without_noise)\n",
    "# print(\"all_possible_languages are:\")\n",
    "# print(all_possible_languages)\n",
    "print(\"number of possible languages is:\")\n",
    "print(len(all_possible_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.314042Z",
     "start_time": "2019-09-09T14:44:19.306573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "degenerate_lang is:\n",
      "('aa', 'aa', 'aa', 'aa')\n",
      "class_degenerate_lang is:\n",
      "0\n",
      "\n",
      "holistic_lang is:\n",
      "('aa', 'ab', 'bb', 'ba')\n",
      "class_holistic_lang is:\n",
      "1\n",
      "\n",
      "other_lang is:\n",
      "('aa', 'aa', 'aa', 'ab')\n",
      "class_other_lang is:\n",
      "2\n",
      "\n",
      "compositional_lang is:\n",
      "('aa', 'ab', 'ba', 'bb')\n",
      "class_compositional_lang is:\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Let's test our classify_language() function using some example languages \n",
    "# from the Kirby et al. (2015) paper (and the numbering of classes used \n",
    "# in SimLang lab 21):\n",
    "\n",
    "degenerate_lang = ('aa', 'aa', 'aa', 'aa')\n",
    "print('')\n",
    "print(\"degenerate_lang is:\")\n",
    "print(degenerate_lang)\n",
    "class_degenerate_lang = classify_language(degenerate_lang, forms_without_noise, meanings)\n",
    "print(\"class_degenerate_lang is:\")\n",
    "print(class_degenerate_lang)\n",
    "\n",
    "holistic_lang = ('aa', 'ab', 'bb', 'ba')\n",
    "print('')\n",
    "print(\"holistic_lang is:\")\n",
    "print(holistic_lang)\n",
    "class_holistic_lang = classify_language(holistic_lang, forms_without_noise, meanings)\n",
    "print(\"class_holistic_lang is:\")\n",
    "print(class_holistic_lang)\n",
    "\n",
    "other_lang = ('aa', 'aa', 'aa', 'ab')\n",
    "print('')\n",
    "print(\"other_lang is:\")\n",
    "print(other_lang)\n",
    "class_other_lang = classify_language(other_lang, forms_without_noise, meanings)\n",
    "print(\"class_other_lang is:\")\n",
    "print(class_other_lang)\n",
    "\n",
    "compositional_lang = ('aa', 'ab', 'ba', 'bb')\n",
    "print('')\n",
    "print(\"compositional_lang is:\")\n",
    "print(compositional_lang)\n",
    "class_compositional_lang = classify_language(compositional_lang, forms_without_noise,\n",
    "                                             meanings)\n",
    "print(\"class_compositional_lang is:\")\n",
    "print(class_compositional_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's start with implementing the production function. Here is the equation from Kirby et al. (2015) again:\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l\\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $f$ stands for form (in the sense of a complete signal, such as *aa*), $l$ stands for language, $t$ stands for topic, and $\\epsilon$ stands for production error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.377946Z",
     "start_time": "2019-09-09T14:44:19.370448Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# A reproduction of the production function of Kirby et al. (2015):\n",
    "\n",
    "\n",
    "# first we need to write a quick function that removes every instance of a given element from a list (to use for\n",
    "# removing the 'correct' forms_without_noisy_variants from a list of possible forms_without_noisy_variants for a given\n",
    "# topic:\n",
    "def remove_all_instances(my_list, element_to_be_removed):\n",
    "    \"\"\"\n",
    "    Takes a list, and removes all instances of a given element from it\n",
    "\n",
    "    :param my_list: a list\n",
    "    :param element_to_be_removed: the element to be removed; can be of any type\n",
    "    :return: the list with all instances of the target element removed\n",
    "    \"\"\"\n",
    "    i = 0  # loop counter\n",
    "    length = len(my_list)  # list length\n",
    "    while (i < len(my_list)):\n",
    "        if (my_list[i] == element_to_be_removed):\n",
    "            my_list.remove(my_list[i])\n",
    "            # as an element is removed\n",
    "            # so decrease the length by 1\n",
    "            length = length - 1\n",
    "            # run loop again to check element\n",
    "            # at same index, when item removed\n",
    "            # next item will shift to the left\n",
    "            continue\n",
    "        i = i + 1\n",
    "    return my_list\n",
    "\n",
    "\n",
    "# Now let's define a function that calculates the probabilities of producing each of the possible forms_without_noisy_\n",
    "# variants, given a particular language and topic:\n",
    "def production_likelihoods_kirby_et_al(language, topic, gamma, error):\n",
    "    \"\"\"\n",
    "    Calculates the production probabilities for each of the possible forms_without_noisy_variants given a language and\n",
    "    topic, as defined by Kirby et al. (2015)\n",
    "\n",
    "    :param language: list of forms_without_noisy_variants that has same length as list of meanings (global variable),\n",
    "    where each form is mapped to the meaning at the corresponding index\n",
    "    :param topic: the index of the topic (corresponding to an index in the globally defined meaning list) that the\n",
    "    speaker intends to communicate\n",
    "    :param gamma: parameter that determines the strength of the penalty on ambiguity\n",
    "    :param error: the probability of making an error in production\n",
    "    :return: 1D numpy array containing a production probability for each possible form (where the index of the\n",
    "    probability corresponds to the index of the form in the global variable \"forms_without_noisy_variants\")\n",
    "    \"\"\"\n",
    "    for m in range(len(meanings)):\n",
    "        if meanings[m] == topic:\n",
    "            topic_index = m\n",
    "    correct_form = language[topic_index]\n",
    "    error_forms = list(forms_without_noise)  #This may seem a bit weird, but a speaker should be able to produce *any*\n",
    "                                            # form as an error form right? Not limited to only the other forms that\n",
    "                                            # exist within their language? (Otherwise a speaker with a degenerate\n",
    "                                            # language could never make a production error).\n",
    "    error_forms = remove_all_instances(error_forms, correct_form)\n",
    "    if len(error_forms) == 0:  # if the list of error_forms is empty because the language is degenerate\n",
    "        error_forms = language  # simply choose an error_form from the whole language\n",
    "    ambiguity = 0\n",
    "    for f in language:\n",
    "        if f == correct_form:\n",
    "            ambiguity += 1\n",
    "    prop_to_prob_correct_form = ((1./ambiguity)**gamma)*(1.-error)\n",
    "    prop_to_prob_error_form = error / (len(forms_without_noise) - 1)\n",
    "    prop_to_prob_per_form_array = np.zeros(len(forms_without_noise))\n",
    "    for i in range(len(forms_without_noise)):\n",
    "        if forms_without_noise[i] == correct_form:\n",
    "            prop_to_prob_per_form_array[i] = prop_to_prob_correct_form\n",
    "        else:\n",
    "            prop_to_prob_per_form_array[i] = prop_to_prob_error_form\n",
    "    return prop_to_prob_per_form_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's implement noisy production:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this by creating a variant of the production_likelihoods_kirby_et_al() function above, where we add the possibility of noisy variants ('a_' etc.), and assigns them a likelihood as well (depending on the settings of the parameters 'noise' and 'noise_prob'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.522676Z",
     "start_time": "2019-09-09T14:44:19.514594Z"
    }
   },
   "outputs": [],
   "source": [
    "def production_likelihoods_with_noise(language, topic, gamma, error, noise_prob):\n",
    "    \"\"\"\n",
    "    Calculates the production probabilities for each of the possible forms (including both forms without noise and all\n",
    "    possible noisy variants) given a language and topic, and the probability of environmental noise\n",
    "\n",
    "    :param language: list of forms that has same length as list of meanings (global variable), where each form is\n",
    "    mapped to the meaning at the corresponding index\n",
    "    :param topic: the index of the topic (corresponding to an index in the globally defined meaning list) that the\n",
    "    speaker intends to communicate\n",
    "    :param gamma: parameter that determines the strength of the penalty on ambiguity\n",
    "    :param error: the probability of making an error in production\n",
    "    :param noise_prob: the probability of environmental noise masking part of the utterance\n",
    "    :return: 1D numpy array containing a production probability for each possible form (where the index of the\n",
    "    probability corresponds to the index of the form in the global variable \"all_forms_including_noisy_variants\")\n",
    "    \"\"\"\n",
    "    for m in range(len(meanings)):\n",
    "        if meanings[m] == topic:\n",
    "            topic_index = m\n",
    "    correct_form = language[topic_index]\n",
    "    error_forms = list(forms_without_noise)  # This may seem a bit weird, but a speaker should be able to produce *any*\n",
    "    # form as an error form right? Not limited to only the other forms that exist within their language? (Otherwise a\n",
    "    # speaker with a degenerate language could never make a production error).\n",
    "    error_forms = remove_all_instances(error_forms, correct_form)\n",
    "    if len(error_forms) == 0:  # if the list of error_forms is empty because the language is degenerate\n",
    "        error_forms = language  # simply choose an error_form from the whole language\n",
    "    noisy_variants_correct_form = create_noisy_variants(correct_form)\n",
    "    noisy_variants_error_forms = []\n",
    "    for error_form in error_forms:\n",
    "        noisy_variants = create_noisy_variants(error_form)\n",
    "        noisy_variants_error_forms = noisy_variants_error_forms+noisy_variants\n",
    "    ambiguity = 0\n",
    "    for f in language:\n",
    "        if f == correct_form:\n",
    "            ambiguity += 1\n",
    "    prop_to_prob_correct_form_complete = ((1./ambiguity)**gamma)*(1.-error)*(1 - noise_prob)\n",
    "    prop_to_prob_error_form_complete = error / (len(forms_without_noise) - 1)*(1 - noise_prob)\n",
    "    prop_to_prob_correct_form_noisy = ((1. / ambiguity) ** gamma) * (1. - error) * (noise_prob / len(noisy_forms))\n",
    "    prop_to_prob_error_form_noisy = error / (len(forms_without_noise) - 1) * (1 - noise_prob) * (noise_prob / len(noisy_forms))\n",
    "    prop_to_prob_per_form_array = np.zeros(len(all_forms_including_noisy_variants))\n",
    "    for i in range(len(all_forms_including_noisy_variants)):\n",
    "        if all_forms_including_noisy_variants[i] == correct_form:\n",
    "            prop_to_prob_per_form_array[i] = prop_to_prob_correct_form_complete\n",
    "        elif all_forms_including_noisy_variants[i] in noisy_variants_correct_form:\n",
    "            prop_to_prob_per_form_array[i] = prop_to_prob_correct_form_noisy\n",
    "        elif all_forms_including_noisy_variants[i] in noisy_variants_error_forms:\n",
    "            prop_to_prob_per_form_array[i] = prop_to_prob_error_form_noisy\n",
    "        else:\n",
    "            prop_to_prob_per_form_array[i] = prop_to_prob_error_form_complete\n",
    "    return prop_to_prob_per_form_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's write a function that actually produces an utterance, given a language and a topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.598713Z",
     "start_time": "2019-09-09T14:44:19.594405Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def produce(language, topic, gamma, error):\n",
    "    \"\"\"\n",
    "    Produces an actual utterance, given a language and a topic\n",
    "\n",
    "    :param language: list of forms_without_noisy_variants that has same length as list of meanings (global variable),\n",
    "    where each form is mapped to the meaning at the corresponding index\n",
    "    :param topic: the index of the topic (corresponding to an index in the globally defined meaning list) that the\n",
    "    speaker intends to communicate\n",
    "    :param gamma: parameter that determines the strength of the penalty on ambiguity\n",
    "    :param error: the probability of making an error in production\n",
    "    :return: an utterance. That is, a single form chosen from either the global variable \"forms_without_noise\" (if\n",
    "    noise is False) or the global variable \"all_forms_including_noisy_variants\" (if noise is True).\n",
    "        \"\"\"\n",
    "    if noise:\n",
    "        prop_to_prob_per_form_array = production_likelihoods_with_noise(language, topic, gamma, error, noise_prob)\n",
    "        prob_per_form_array = np.divide(prop_to_prob_per_form_array, np.sum(prop_to_prob_per_form_array))\n",
    "        utterance = np.random.choice(all_forms_including_noisy_variants, p=prob_per_form_array)\n",
    "    else:\n",
    "        prop_to_prob_per_form_array = production_likelihoods_kirby_et_al(language, topic, gamma, error)\n",
    "        prob_per_form_array = np.divide(prop_to_prob_per_form_array, np.sum(prop_to_prob_per_form_array))\n",
    "        utterance = np.random.choice(forms_without_noise, p=prob_per_form_array)\n",
    "    return utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our production function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.677382Z",
     "start_time": "2019-09-09T14:44:19.669980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produce(compositional_lang, \"02\", gamma, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function that interprets an utterance (given a language):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.748560Z",
     "start_time": "2019-09-09T14:44:19.745062Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def receive_without_repair(language, utterance):\n",
    "    \"\"\"\n",
    "    Takes a language and an utterance, and returns an interpretation of that utterance, following the language\n",
    "\n",
    "    :param language: list of forms_without_noisy_variants that has same length as list of meanings (global variable),\n",
    "    where each form is mapped to the meaning at the corresponding index\n",
    "    :param utterance: a form (string)\n",
    "    :return: an interpretation (string)\n",
    "    \"\"\"\n",
    "    possible_interpretations = []\n",
    "    for i in range(len(language)):\n",
    "        if language[i] == utterance:\n",
    "            possible_interpretations.append(meanings[i])\n",
    "    interpretation = random.choice(possible_interpretations)\n",
    "    return interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our reception function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.818974Z",
     "start_time": "2019-09-09T14:44:19.816279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receive_without_repair(compositional_lang, 'aa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement our repair initiation function\n",
    "\n",
    "Above we decided that repair should be initiated probabilistically, depending on the level of ambiguity that is left after having received the speaker's utterance. We should also implement the possibility of turning on or off an efficiency pressure which, when on, makes the listener decide to initiate repair not just based on ambiguity, but also on the costs of the type of repair initiator that is presumed to be necessary. To start us off, we'll start with an open request costing 2/3, a restricted request costing 1/3, and no repair costing 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a function that takes a noisy form and works back to determine which possible complete forms it could have stemmed from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:19.964143Z",
     "start_time": "2019-09-09T14:44:19.960767Z"
    }
   },
   "outputs": [],
   "source": [
    "def noisy_to_complete_forms(noisy_form, forms_without_noise):\n",
    "    possible_complete_forms = []\n",
    "    amount_of_noise = noisy_form.count('_')\n",
    "    for complete_form in forms_without_noise:\n",
    "        similarity_score = 0\n",
    "        for i in range(len(noisy_form)):\n",
    "            if noisy_form[i] == complete_form[i]:\n",
    "                similarity_score += 1\n",
    "        if similarity_score == len(complete_form)-amount_of_noise:\n",
    "            possible_complete_forms.append(complete_form)\n",
    "    return possible_complete_forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our noisy_to_complete_forms() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:20.033578Z",
     "start_time": "2019-09-09T14:44:20.030888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'ab']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_to_complete_forms('a_', forms_without_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:44:52.725483Z",
     "start_time": "2019-09-09T14:44:52.717714Z"
    }
   },
   "outputs": [],
   "source": [
    "def receive_with_repair(language, utterance):\n",
    "    print(\"meanings are:\")\n",
    "    print(meanings)\n",
    "    print(\"language is:\")\n",
    "    print(language)\n",
    "    possible_interpretations = []\n",
    "    if '_' in utterance:\n",
    "       possible_complete_forms = noisy_to_complete_forms(utterance, forms_without_noise) \n",
    "    print(\"possible_complete_forms are:\")\n",
    "    print(possible_complete_forms)\n",
    "    for i in range(len(language)):\n",
    "        if language[i] in possible_complete_forms:\n",
    "            possible_interpretations.append(meanings[i])\n",
    "    print('possible_interpretations')\n",
    "    print(possible_interpretations)\n",
    "    ambiguity = len(possible_interpretations)/len(meanings)\n",
    "    print(\"ambiguity is:\")\n",
    "    print(ambiguity)\n",
    "    part_meaning_score = 0\n",
    "    for j in range(len(possible_interpretations)):\n",
    "        for k in range(len(possible_interpretations)):\n",
    "            for l in range(len(possible_interpretations[0])):\n",
    "                print('')\n",
    "                print(\"possible_interpretations[j][l] is:\")\n",
    "                print(possible_interpretations[j][l])\n",
    "                print(\"possible_interpretations[k][l] is:\")\n",
    "                print(possible_interpretations[k][l])\n",
    "                if possible_interpretations[j][l] == possible_interpretations[k][l]:\n",
    "                    part_meaning_score += 1\n",
    "                print(\"part_meaning_score is:\")\n",
    "                print(part_meaning_score)\n",
    "    print('')\n",
    "    print('')\n",
    "    print(\"part_meaning_score is:\")\n",
    "    print(part_meaning_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T13:41:22.011849Z",
     "start_time": "2019-09-06T13:41:22.002772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanings are:\n",
      "['02', '03', '12', '13']\n",
      "language is:\n",
      "('aa', 'ab', 'ba', 'bb')\n",
      "possible_complete_forms are:\n",
      "['aa', 'ab']\n",
      "possible_interpretations\n",
      "['02', '03']\n",
      "ambiguity is:\n",
      "0.5\n",
      "\n",
      "possible_interpretations[j][l] is:\n",
      "0\n",
      "possible_interpretations[k][l] is:\n",
      "0\n",
      "part_meaning_score is:\n",
      "1\n",
      "\n",
      "possible_interpretations[j][l] is:\n",
      "2\n",
      "possible_interpretations[k][l] is:\n",
      "2\n",
      "part_meaning_score is:\n",
      "2\n",
      "\n",
      "possible_interpretations[j][l] is:\n",
      "0\n",
      "possible_interpretations[k][l] is:\n",
      "0\n",
      "part_meaning_score is:\n",
      "3\n",
      "\n",
      "possible_interpretations[j][l] is:\n",
      "2\n",
      "possible_interpretations[k][l] is:\n",
      "3\n",
      "part_meaning_score is:\n",
      "3\n",
      "\n",
      "possible_interpretations[j][l] is:\n",
      "0\n",
      "possible_interpretations[k][l] is:\n",
      "0\n",
      "part_meaning_score is:\n",
      "4\n",
      "\n",
      "possible_interpretations[j][l] is:\n",
      "3\n",
      "possible_interpretations[k][l] is:\n",
      "2\n",
      "part_meaning_score is:\n",
      "4\n",
      "\n",
      "possible_interpretations[j][l] is:\n",
      "0\n",
      "possible_interpretations[k][l] is:\n",
      "0\n",
      "part_meaning_score is:\n",
      "5\n",
      "\n",
      "possible_interpretations[j][l] is:\n",
      "3\n",
      "possible_interpretations[k][l] is:\n",
      "3\n",
      "part_meaning_score is:\n",
      "6\n",
      "\n",
      "\n",
      "part_meaning_score is:\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "receive_with_repair(compositional_lang, 'a_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the rest of the code, see evolution_compositionality_under_noise.py (i.e. the python file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  \n",
    "Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87â€“102. https://doi.org/10.1016/j.cognition.2015.03.016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
