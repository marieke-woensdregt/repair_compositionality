{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-08-06 11:19:34  \n",
    "\n",
    "# The evolution of compositionality under environmental noise\n",
    "\n",
    "\n",
    "\n",
    "## Hypothesis:\n",
    "\n",
    "**We hypothesise that** other-initiated repair and a compositional language co-evolve under the following assumptions:\n",
    "\n",
    "1. There is a pressure for communicative success\n",
    "2. Listeners have at least two types of repair initiator at their disposal: (i) open request, and (ii) restricted request\n",
    "3. There is a pressure to make repair sequences as efficient as possible (in terms of combined utterance length of repair initiator and response)\n",
    "    \n",
    "This hypothesis is based on the idea that if repair sequences are under a pressure to be efficient, then it becomes useful to be able to initiate repair by asking for clarification of just _part_ of the utterance, specifically by feeding back the part of the meaning that you _did_ get. Under those circumstances, a compositional system is helpful (compared to a holistic system), because parts of the signal (i.e. 'substrings') map to parts of the meaning. _**<span class=\"mark\">Note</span>**_ that this hypothesis is thus based on the assumption that the 'closed request' type of repair initiator works only if the listener has _understood_ part of the _meaning_ of the signal, as opposed to just having received part of the _signal_ and being able to repeat that back verbatim, in order to prompt the speaker to repeat the part that didn't come through. We feel that this is a reasonable assumption based on what we know from empirical work, but it is a little hard to assess given that natural language _is_ compositional.\n",
    "\n",
    "There are of course parts of language that are holistic though. Mark D. and I thought of the example of proper nouns. For instance, if A says \"My favourite writer is Chimamanda Ngozi Adichie\", B could say \"Chima _who_ ?\" in order to prompt A to repeat the name. However, our intuition is that it would be quite unnatural for A to then respond to B's repair request by saying only \"manda Ngozi Adichie\". Most likely A would simply repeat the whole name.\n",
    "\n",
    "When I was talking to Simon Kirby about it being difficult to think about whether repair could work as efficiently in a holistic system, he thought of colour terms as an example of a holistic part of language. And as an example he gave the repair sequence: \n",
    "\n",
    "A: \"the blue shirt\"\n",
    "\n",
    "B: \"you mean the turquoise one?\"\n",
    "\n",
    "In this example, the issue with the holistic signal \"blue\" is not that part of it was covered by noise, but that it is (apparently) not specific enough to pick out one particular referent. So it's not quite the same as what we were thinking about above: which is the question whether one can do efficient repair (i.e. a restricted request) when given a holistic signal that was partly disturbed by noise, in such a way that the listener does not get even _part_ of the meaning. Whereas in the example above, the listener receives the full signal without noise, and therefore is able to interpret part of the meaning (i.e. exclude some potential referents, such as the red shirt), but the problem is that they cannot quite narrow it down to a single unambiguous referent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-08-06 11:19:27 \n",
    "\n",
    "## Predictions:\n",
    "\n",
    "Here we adapt the model of Kirby et al. (2015)---where they show that a compositional language evolves when there is a pressure for both (i) learnability and (ii) expressivity---to incorporate noise (step 1) and repair (step 2).\n",
    "\n",
    "In Kirby et al.'s model, the pressure for expressivity (implemented as speakers aiming to avoid ambiguity when they produce a signal) is what causes populations to move towards languages that have one-to-one mappings between meanings and signals, but these can be either compositional or holistic. The pressure for learnability then (implemented as a prior that favours more compressible languages) is what causes populations to move towards compositional languages rather than holistic languages, because compositional languages are more compressible (and therefore, is the assumption, easier to learn). \n",
    "\n",
    "### Strong prediction (excluding pressure for learnability):\n",
    "\n",
    "Therefore, the strongest prediction given our reasoning above, would be that if we take away the pressure for learnability, but add in environmental noise and the possibility of repair (combined with a pressure for efficient repair sequences), populations would also converge on compositional languages. This may be a bit of a weird prediction in itself, because we _do_ believe a pressure for learnability will have played a role. However, by isolating the effects of the two pressures (learnability vs. noise+repair) we could say something about their individual contributions.\n",
    "\n",
    "### 'Weaker' prediction (including pressure for learnability):\n",
    "\n",
    "Not entirely sure what to predict here. Maybe the effects of a noise+repair+efficiency pressure on the one hand, and a pressure for learnability on the other hand, will be additive? So that when combined, they cause populations to converge on a compositional language faster than when only one of these pressures is at play?\n",
    "\n",
    "\n",
    "### Which predictions to test?\n",
    "\n",
    "We'd like to explore both predictions, starting with the stronger one.\n",
    "\n",
    "### References  \n",
    "Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87â€“102. https://doi.org/10.1016/j.cognition.2015.03.016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How to implement the pressure for communicative success that we have in mind?\n",
    "\n",
    "What I'm wondering here is whether the pressure for expressivity, as it was implemented by Kirby et al. (2015), in combination with a pressure for efficiency (and the possibility of repair), will have the effect of a pressure for communicative success as we have in mind, or whether we'll have to incorporate an extra or different pressure for communicative success.\n",
    "\n",
    "Firstly, the way the pressure for expressivity is implemented by Kirby et al. (2015):\n",
    "speakers try to avoid using ambiguous signals. ---> this is sufficient for pushing populations towards having either holistic or compositional languages (as opposed to degenerate ones). \n",
    "\n",
    "---> What could the effect of environmental noise be in this? If data consists of <meaning, form> pairs that were produced by a pair of agents from the previous generation, as in the Kirby et al. (2015) model, noise _without_ repair will mean that learners sometimes receive incomplete data (an incomplete form). Meanings in the <meaning, form> pairs that make up the data should not be incomplete, because (if I understand the Kirby et al., 2015, paper correctly) these represent the speaker's communicative intention, rather than the listener's interpretation.\n",
    "\n",
    "---> What will that mean for how likely learners are to infer different languages? In general we'd just expect that learners will need more observations in order to reach a given amount of posterior belief in the correct language hypothesis. This means that given a particular bottleneck width, populations who are exposed to noise will be more likely to transition from one language into another than populations who aren't. Will the addition of noise push learners in any particular direction though? I can't think why that would happen if learners have a flat prior. However, if learners have a compression-based prior as in the Kirby et al. (2015) model, they would be pushed towards degenerate languages in general, in the same way as it happens in Kirby et al.\n",
    "\n",
    "---> The pressure for communicative success that Mark D. and I had in mind is that speaker and hearer try to cooperate so that the speaker can get their intended meaning across. I think this pressure is captured if we assume that the hearer will initiate repair when they can't derive a full meaning based on the form they received. In some cases, that means a hearer will not have to initiate repair even when they received an incomplete form. That is, if the hearer is using a language of the 'other' type, in which there is only one form that starts with a 'b', and that form maps to one particular meaning, then they will be able to infer a full meaning, and will therefore not initiate repair. However, such a language will necessarily also contain some degenerate forms, such as 'aa' mapping to both meaning '02' and meaning '03'. _That_ is where the pressure for expressivity comes in: speakers will try to avoid using such ambiguous forms, and therefore learners will be more likely to infer either a holistic or compositional language when receiving data from such a language of the 'other' type.\n",
    "\n",
    "---> So in our 'strong prediction' scenario, where we _replace_ Kirby et al.'s learnability pressure with a pressure for efficient repair, we'd expect that populations get pushed in the direction of languages of the 'other' type, as well as languages of the compositional type (not entirely sure which one would be best?) rather than languages of the degenerate type. Combining this pressure for efficient repair with Kirby et al.'s pressure for expressivity, will then get rid of the languages of the 'other' type.\n",
    "\n",
    "---> So I guess what I'm saying is that in our model, the pressure for communicative success should be extended somewhat, to not only consist of the (parameterised) assumption that speakers try to avoid ambiguity, but also the assumption that listeners try to arrive at a complete, unambiguous interpretation, rather than just settling for a (wholly or partly) ambiguous meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2019-07-31 15:27:33 \n",
    "\n",
    "## Production in the Kirby et al. (2015) model:\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l\\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $f$ stands for form (in the sense of a complete signal, such as *aa*), $l$ stands for language, $t$ stands for topic, $a$ stands for ambiguity (i.e. the number of meanings in $M$ that map to form $f$ in $l$), $\\gamma$ specifies the extent to which ambiguous utterances are penalised, and $\\epsilon$ stands for production error.\n",
    "\n",
    "\n",
    "From Kirby et al. (2015, pp. 92-93):\n",
    "> \"If $a$ = 1 ($f$ is unambiguous) and/or $\\gamma$ = 0 then this yields a model of production where the 'correct' form is produced with probability $1-\\epsilon$. However, when $\\gamma$ > 0 and $f$ is ambiguous (i.e., $a$ > 1), then the 'correct' mapping from $t$ to $f$ is less likely to be produced (the probability $P(f \\mid l, t)$ is reduced by the factor $(\\frac{1}{a})^\\gamma$ ) and the remaining probability mass is spread equally over the other possible forms, leading to increased probability of producing $f'$ $\\neq$ $f$. Therefore, $\\gamma$ > 0 introduces a penalty for languages whose utterances are ambiguous.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2019-07-31 15:27:52 \n",
    "## How do we add *environmental* noise to this production model?\n",
    "\n",
    "\n",
    "If we leave Kirby et al.'s production error $\\epsilon$ aside for a moment, we could add environmental noise by allowing production to have two possible outcomes. Given a particular topic *t*, a speaker could produce either:\n",
    "- form $f$ if $t$ is mapped to $f$ in $l$, with probability $1-n$ (where $n$ stands for the probability of noise happening)\n",
    "- form $f_{noisy}$, where $f_{noisy}$ is a 'noisy variant' of $f$ *and* $t$ is mapped to $f$ in $l$. This should happen with probability $\\frac{n}{|F_{noisy}|}$ (where $F_{noisy}$ is the full set of possible noisy variants of the form $f$ that maps to $t$ in $l$\n",
    "\n",
    "To give an example, using the example compositional grammar from Kirby et al. (2015, p. 92):\n",
    "\n",
    "S   --> A B   \n",
    "A:0 --> a   \n",
    "A:1 --> b   \n",
    "B:2 --> a   \n",
    "B:3 --> b   \n",
    "\n",
    "Given this grammar, the only form that maps to topic 02 is *aa*.   \n",
    "_**<span class=\"mark\"><span class=\"mark\">Design decision:</span></span>**_ If we allow for both 'partial' and 'full' noise,   \n",
    " this form has three possible noisy variants:\n",
    "- a_\n",
    "- _a\n",
    "- \\__\n",
    "\n",
    "where _ stands for a noisy part of the signal that the listener could not perceive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_**<span class=\"girk\">Decision**_ Mark D. + Marieke (01/08/2019):</span> It is interesting to allow for both open requests and closed requests in this model, because the intuition that our hypothesis is based on, is that whereas a compositional system allows for the usage of both open and closed requests, a holistic system only allows for open requests. \n",
    "\n",
    "\n",
    "So if 02 is the speaker's intended topic (and if we exclude the possibility of the speaker making a production error for now), they will produce the following signals with the following probabilities:\n",
    "- aa with probability $1-n$\n",
    "- [a_, \\_a, \\__ \\] with probability $\\frac{n}{3}$ (because there are 3 possible noisy variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_**<span class=\"mark\">Design decision:</span>**_ should noisy variant \\__ be less likely than other noisy variants, because it means that really *none* of the signal came through? Intuitively it would make sense if a_ was twice as likely to happen as \\__, but that would make the production function yet more complicated. So to keep it simple I propose making all the noisy variants equally probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's add the probability of making a production error $\\epsilon$ back in, which gives us: \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\, (1-n) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact} \\\\ \n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\; \\; \\textrm{if part of} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact} \\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\, (1-n) \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact}\\\\\n",
    "\\frac{\\epsilon}{|F|-1} \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\quad \\quad \\quad \\textrm{if no part of} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where *n* stands for the probability of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-07-31 15:28:14 \n",
    "\n",
    "### First, let's reproduce Kirby et al.'s (2015) model of production:\n",
    "\n",
    "In order to do that we first need to set some general parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T14:09:51.234154Z",
     "start_time": "2019-08-12T14:09:51.231300Z"
    }
   },
   "outputs": [],
   "source": [
    "meanings = ['02', '03', '12', '13']  # all possible meanings\n",
    "forms = ['aa', 'ab', 'ba', 'bb']  # all possible forms\n",
    "error = 0.05  # the probability of making a production error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need some functions to generate all possible languages and classify languages according to the categories specified by Kirby et al. (2015:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T14:09:51.331184Z",
     "start_time": "2019-08-12T14:09:51.317148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible languages is:\n",
      "256\n",
      "\n",
      "degenerate_lang is:\n",
      "('aa', 'aa', 'aa', 'aa')\n",
      "class_degenerate_lang is:\n",
      "0\n",
      "\n",
      "compositional_lang is:\n",
      "('aa', 'ab', 'ba', 'bb')\n",
      "class_compositional_lang is:\n",
      "3\n",
      "\n",
      "holistic_lang is:\n",
      "('aa', 'ab', 'bb', 'ba')\n",
      "class_holistic_lang is:\n",
      "1\n",
      "\n",
      "other_lang is:\n",
      "('aa', 'aa', 'aa', 'ab')\n",
      "class_other_lang is:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Some functions to create and classify all possible languages:\n",
    "def create_all_possible_languages(meanings, forms):\n",
    "    \"\"\"Creates all possible languages\n",
    "\n",
    "    :param meanings: list of strings corresponding to all possible meanings\n",
    "    :type meanings: list\n",
    "    :param forms: list of strings corresponding to all possible forms\n",
    "    :type forms: list\n",
    "    :returns: list of tuples which represent languages, where each tuple consists of forms and has length len(meanings)\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    all_possible_languages = list(\n",
    "        itertools.product(forms, repeat=len(meanings)))\n",
    "    return all_possible_languages\n",
    "\n",
    "\n",
    "def classify_language(lang, forms, meanings):\n",
    "    \"\"\"\n",
    "    Classify one particular language as either 'degenerate' (0), 'holistic' (1), 'other' (2)\n",
    "    or 'compositional' (3) (Kirby et al., 2015)\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms, where each form index maps to same index in meanings\n",
    "    :type lang: tuple\n",
    "    :param forms: list of strings corresponding to all possible forms\n",
    "    :type forms: list\n",
    "    :returns: integer corresponding to category that language belongs to:\n",
    "    0 = degenerate, 1 = holistic, 2 = other, 3 = compositional (here I'm following the\n",
    "    numbering used in SimLang lab 21)\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    # TODO: See if I can modify this function so that it can deal with any number of forms and meanings.\n",
    "    class_degenerate = 0\n",
    "    class_holistic = 1\n",
    "    class_other = 2\n",
    "    class_compositional = 3\n",
    "    # First check whether some conditions are met, bc this function hasn't been coded up in the most general way yet:\n",
    "    if len(forms) != 4:\n",
    "        raise ValueError(\n",
    "            \"This function only works for a world in which there are 4 possible forms\"\n",
    "        )\n",
    "    if len(forms[0]) != 2:\n",
    "        raise ValueError(\n",
    "            \"This function only works when each form consists of 2 elements\")\n",
    "    if len(lang) != len(meanings):\n",
    "        raise ValueError(\"Lang should have same length as meanings\")\n",
    "\n",
    "    # lang is degenerate if it uses the same form for every meaning:\n",
    "    if lang[0] == lang[1] and lang[1] == lang[2] and lang[2] == lang[3]:\n",
    "        return class_degenerate\n",
    "\n",
    "    # lang is compositional if it makes use of all possible forms, *and* each form element maps to the same meaning\n",
    "    # element for each form:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[\n",
    "        3] in lang and lang[0][0] == lang[1][0] and lang[2][0] == lang[3][0] and lang[0][\n",
    "        1] == lang[2][1] and lang[1][1] == lang[3][1]:\n",
    "        return class_compositional\n",
    "\n",
    "    # lang is holistic if it is *not* compositional, but *does* make use of all possible forms:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[\n",
    "        3] in lang:\n",
    "        return class_holistic\n",
    "\n",
    "    # In all other cases, a language belongs to the 'other' category:\n",
    "    else:\n",
    "        return class_other\n",
    "\n",
    "\n",
    "# Let's try out our create_all_possible_languages() function:\n",
    "all_possible_languages = create_all_possible_languages(meanings, forms)\n",
    "# print(\"all_possible_languages are:\")\n",
    "# print(all_possible_languages)\n",
    "print(\"number of possible languages is:\")\n",
    "print(len(all_possible_languages))\n",
    "\n",
    "# Let's test our classify_language() function using some example languages from the Kirby et al. (2015) paper:\n",
    "degenerate_lang = ('aa', 'aa', 'aa', 'aa')\n",
    "print('')\n",
    "print(\"degenerate_lang is:\")\n",
    "print(degenerate_lang)\n",
    "class_degenerate_lang = classify_language(degenerate_lang, forms, meanings)\n",
    "print(\"class_degenerate_lang is:\")\n",
    "print(class_degenerate_lang)\n",
    "\n",
    "compositional_lang = ('aa', 'ab', 'ba', 'bb')\n",
    "print('')\n",
    "print(\"compositional_lang is:\")\n",
    "print(compositional_lang)\n",
    "class_compositional_lang = classify_language(compositional_lang, forms,\n",
    "                                             meanings)\n",
    "print(\"class_compositional_lang is:\")\n",
    "print(class_compositional_lang)\n",
    "\n",
    "holistic_lang = ('aa', 'ab', 'bb', 'ba')\n",
    "print('')\n",
    "print(\"holistic_lang is:\")\n",
    "print(holistic_lang)\n",
    "class_holistic_lang = classify_language(holistic_lang, forms, meanings)\n",
    "print(\"class_holistic_lang is:\")\n",
    "print(class_holistic_lang)\n",
    "\n",
    "other_lang = ('aa', 'aa', 'aa', 'ab')\n",
    "print('')\n",
    "print(\"other_lang is:\")\n",
    "print(other_lang)\n",
    "class_other_lang = classify_language(other_lang, forms, meanings)\n",
    "print(\"class_other_lang is:\")\n",
    "print(class_other_lang)\n",
    "\n",
    "\n",
    "def classify_all_languages(language_list):\n",
    "    \"\"\"\n",
    "    Classify all languages as either 'degenerate' (0), 'holistic' (1), 'other' (2)\n",
    "    or 'compositional' (3) (Kirby et al., 2015)\n",
    "\n",
    "    :param language_list: list of all languages\n",
    "    :type language_list: list\n",
    "    :returns: 1D numpy array containing integer corresponding to category of corresponding\n",
    "    language index: 0 = degenerate, 1 = holistic, 2 = other, 3 = compositional\n",
    "    (here I'm following the numbering used in SimLang lab 21)\n",
    "    :rtype: 1D numpy array\n",
    "    \"\"\"\n",
    "    class_per_lang = np.zeros(len(language_list))\n",
    "    for l in range(len(language_list)):\n",
    "        class_per_lang[l] = classify_language(language_list[l], forms, meanings)\n",
    "    return class_per_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the functions in this cell work correctly by comparing the number of languages of each type we get with the SimLang lab 21:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T14:09:51.541210Z",
     "start_time": "2019-08-12T14:09:51.399602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "no_of_each_type is:\n",
      "[  4  16 228   8]\n",
      "\n",
      "\n",
      "\n",
      "no_of_each_class is:\n",
      "[  4  20 228   4]\n",
      "\n",
      "\n",
      "len(all_langs_as_in_simlang) is:\n",
      "256\n",
      "len(all_langs_as_in_simlang[0]) is:\n",
      "4\n",
      "len(all_langs_as_in_simlang[0][0]) is:\n",
      "2\n",
      "\n",
      "\n",
      "checks_per_language is:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "np.sum(checks_per_language) is:\n",
      "256.0\n",
      "\n",
      "\n",
      "compositional_langs_indices_my_code MY CODE are:\n",
      "[ 27  78 177 228]\n",
      "len(compositional_langs_indices_my_code) MY CODE are:\n",
      "4\n",
      "\n",
      "index MY CODE is:\n",
      "27\n",
      "all_possible_languages[index] MY CODE is:\n",
      "('aa', 'ab', 'ba', 'bb')\n",
      "\n",
      "index MY CODE is:\n",
      "78\n",
      "all_possible_languages[index] MY CODE is:\n",
      "('ab', 'aa', 'bb', 'ba')\n",
      "\n",
      "index MY CODE is:\n",
      "177\n",
      "all_possible_languages[index] MY CODE is:\n",
      "('ba', 'bb', 'aa', 'ab')\n",
      "\n",
      "index MY CODE is:\n",
      "228\n",
      "all_possible_languages[index] MY CODE is:\n",
      "('bb', 'ba', 'ab', 'aa')\n",
      "\n",
      "\n",
      "compositional_langs_indices_simlang SIMLANG CODE are:\n",
      "[ 27  39  78 114 141 177 216 228]\n",
      "len(compositional_langs_indices_simlang) SIMLANG CODE are:\n",
      "8\n",
      "\n",
      "index SIMLANG CODE is:\n",
      "27\n",
      "languages[index] SIMLANG CODE is:\n",
      "[('02', 'aa'), ('03', 'ab'), ('12', 'ba'), ('13', 'bb')]\n",
      "\n",
      "index SIMLANG CODE is:\n",
      "39\n",
      "languages[index] SIMLANG CODE is:\n",
      "[('02', 'aa'), ('03', 'ba'), ('12', 'ab'), ('13', 'bb')]\n",
      "\n",
      "index SIMLANG CODE is:\n",
      "78\n",
      "languages[index] SIMLANG CODE is:\n",
      "[('02', 'ab'), ('03', 'aa'), ('12', 'bb'), ('13', 'ba')]\n",
      "\n",
      "index SIMLANG CODE is:\n",
      "114\n",
      "languages[index] SIMLANG CODE is:\n",
      "[('02', 'ab'), ('03', 'bb'), ('12', 'aa'), ('13', 'ba')]\n",
      "\n",
      "index SIMLANG CODE is:\n",
      "141\n",
      "languages[index] SIMLANG CODE is:\n",
      "[('02', 'ba'), ('03', 'aa'), ('12', 'bb'), ('13', 'ab')]\n",
      "\n",
      "index SIMLANG CODE is:\n",
      "177\n",
      "languages[index] SIMLANG CODE is:\n",
      "[('02', 'ba'), ('03', 'bb'), ('12', 'aa'), ('13', 'ab')]\n",
      "\n",
      "index SIMLANG CODE is:\n",
      "216\n",
      "languages[index] SIMLANG CODE is:\n",
      "[('02', 'bb'), ('03', 'ab'), ('12', 'ba'), ('13', 'aa')]\n",
      "\n",
      "index SIMLANG CODE is:\n",
      "228\n",
      "languages[index] SIMLANG CODE is:\n",
      "[('02', 'bb'), ('03', 'ba'), ('12', 'ab'), ('13', 'aa')]\n"
     ]
    }
   ],
   "source": [
    "# FROM SIMLANG LAB 21:\n",
    "languages = [[('02', 'aa'), ('03', 'aa'), ('12', 'aa'), ('13', 'aa')], [('02', 'aa'), ('03', 'aa'), ('12', 'aa'), ('13', 'ab')], [('02', 'aa'), ('03', 'aa'), ('12', 'aa'), ('13', 'ba')], [('02', 'aa'), ('03', 'aa'), ('12', 'aa'), ('13', 'bb')], [('02', 'aa'), ('03', 'aa'), ('12', 'ab'), ('13', 'aa')], [('02', 'aa'), ('03', 'aa'), ('12', 'ab'), ('13', 'ab')], [('02', 'aa'), ('03', 'aa'), ('12', 'ab'), ('13', 'ba')], [('02', 'aa'), ('03', 'aa'), ('12', 'ab'), ('13', 'bb')], [('02', 'aa'), ('03', 'aa'), ('12', 'ba'), ('13', 'aa')], [('02', 'aa'), ('03', 'aa'), ('12', 'ba'), ('13', 'ab')], [('02', 'aa'), ('03', 'aa'), ('12', 'ba'), ('13', 'ba')], [('02', 'aa'), ('03', 'aa'), ('12', 'ba'), ('13', 'bb')], [('02', 'aa'), ('03', 'aa'), ('12', 'bb'), ('13', 'aa')], [('02', 'aa'), ('03', 'aa'), ('12', 'bb'), ('13', 'ab')], [('02', 'aa'), ('03', 'aa'), ('12', 'bb'), ('13', 'ba')], [('02', 'aa'), ('03', 'aa'), ('12', 'bb'), ('13', 'bb')], [('02', 'aa'), ('03', 'ab'), ('12', 'aa'), ('13', 'aa')], [('02', 'aa'), ('03', 'ab'), ('12', 'aa'), ('13', 'ab')], [('02', 'aa'), ('03', 'ab'), ('12', 'aa'), ('13', 'ba')], [('02', 'aa'), ('03', 'ab'), ('12', 'aa'), ('13', 'bb')], [('02', 'aa'), ('03', 'ab'), ('12', 'ab'), ('13', 'aa')], [('02', 'aa'), ('03', 'ab'), ('12', 'ab'), ('13', 'ab')], [('02', 'aa'), ('03', 'ab'), ('12', 'ab'), ('13', 'ba')], [('02', 'aa'), ('03', 'ab'), ('12', 'ab'), ('13', 'bb')], [('02', 'aa'), ('03', 'ab'), ('12', 'ba'), ('13', 'aa')], [('02', 'aa'), ('03', 'ab'), ('12', 'ba'), ('13', 'ab')], [('02', 'aa'), ('03', 'ab'), ('12', 'ba'), ('13', 'ba')], [('02', 'aa'), ('03', 'ab'), ('12', 'ba'), ('13', 'bb')], [('02', 'aa'), ('03', 'ab'), ('12', 'bb'), ('13', 'aa')], [('02', 'aa'), ('03', 'ab'), ('12', 'bb'), ('13', 'ab')], [('02', 'aa'), ('03', 'ab'), ('12', 'bb'), ('13', 'ba')], [('02', 'aa'), ('03', 'ab'), ('12', 'bb'), ('13', 'bb')], [('02', 'aa'), ('03', 'ba'), ('12', 'aa'), ('13', 'aa')], [('02', 'aa'), ('03', 'ba'), ('12', 'aa'), ('13', 'ab')], [('02', 'aa'), ('03', 'ba'), ('12', 'aa'), ('13', 'ba')], [('02', 'aa'), ('03', 'ba'), ('12', 'aa'), ('13', 'bb')], [('02', 'aa'), ('03', 'ba'), ('12', 'ab'), ('13', 'aa')], [('02', 'aa'), ('03', 'ba'), ('12', 'ab'), ('13', 'ab')], [('02', 'aa'), ('03', 'ba'), ('12', 'ab'), ('13', 'ba')], [('02', 'aa'), ('03', 'ba'), ('12', 'ab'), ('13', 'bb')], [('02', 'aa'), ('03', 'ba'), ('12', 'ba'), ('13', 'aa')], [('02', 'aa'), ('03', 'ba'), ('12', 'ba'), ('13', 'ab')], [('02', 'aa'), ('03', 'ba'), ('12', 'ba'), ('13', 'ba')], [('02', 'aa'), ('03', 'ba'), ('12', 'ba'), ('13', 'bb')], [('02', 'aa'), ('03', 'ba'), ('12', 'bb'), ('13', 'aa')], [('02', 'aa'), ('03', 'ba'), ('12', 'bb'), ('13', 'ab')], [('02', 'aa'), ('03', 'ba'), ('12', 'bb'), ('13', 'ba')], [('02', 'aa'), ('03', 'ba'), ('12', 'bb'), ('13', 'bb')], [('02', 'aa'), ('03', 'bb'), ('12', 'aa'), ('13', 'aa')], [('02', 'aa'), ('03', 'bb'), ('12', 'aa'), ('13', 'ab')], [('02', 'aa'), ('03', 'bb'), ('12', 'aa'), ('13', 'ba')], [('02', 'aa'), ('03', 'bb'), ('12', 'aa'), ('13', 'bb')], [('02', 'aa'), ('03', 'bb'), ('12', 'ab'), ('13', 'aa')], [('02', 'aa'), ('03', 'bb'), ('12', 'ab'), ('13', 'ab')], [('02', 'aa'), ('03', 'bb'), ('12', 'ab'), ('13', 'ba')], [('02', 'aa'), ('03', 'bb'), ('12', 'ab'), ('13', 'bb')], [('02', 'aa'), ('03', 'bb'), ('12', 'ba'), ('13', 'aa')], [('02', 'aa'), ('03', 'bb'), ('12', 'ba'), ('13', 'ab')], [('02', 'aa'), ('03', 'bb'), ('12', 'ba'), ('13', 'ba')], [('02', 'aa'), ('03', 'bb'), ('12', 'ba'), ('13', 'bb')], [('02', 'aa'), ('03', 'bb'), ('12', 'bb'), ('13', 'aa')], [('02', 'aa'), ('03', 'bb'), ('12', 'bb'), ('13', 'ab')], [('02', 'aa'), ('03', 'bb'), ('12', 'bb'), ('13', 'ba')], [('02', 'aa'), ('03', 'bb'), ('12', 'bb'), ('13', 'bb')], [('02', 'ab'), ('03', 'aa'), ('12', 'aa'), ('13', 'aa')], [('02', 'ab'), ('03', 'aa'), ('12', 'aa'), ('13', 'ab')], [('02', 'ab'), ('03', 'aa'), ('12', 'aa'), ('13', 'ba')], [('02', 'ab'), ('03', 'aa'), ('12', 'aa'), ('13', 'bb')], [('02', 'ab'), ('03', 'aa'), ('12', 'ab'), ('13', 'aa')], [('02', 'ab'), ('03', 'aa'), ('12', 'ab'), ('13', 'ab')], [('02', 'ab'), ('03', 'aa'), ('12', 'ab'), ('13', 'ba')], [('02', 'ab'), ('03', 'aa'), ('12', 'ab'), ('13', 'bb')], [('02', 'ab'), ('03', 'aa'), ('12', 'ba'), ('13', 'aa')], [('02', 'ab'), ('03', 'aa'), ('12', 'ba'), ('13', 'ab')], [('02', 'ab'), ('03', 'aa'), ('12', 'ba'), ('13', 'ba')], [('02', 'ab'), ('03', 'aa'), ('12', 'ba'), ('13', 'bb')], [('02', 'ab'), ('03', 'aa'), ('12', 'bb'), ('13', 'aa')], [('02', 'ab'), ('03', 'aa'), ('12', 'bb'), ('13', 'ab')], [('02', 'ab'), ('03', 'aa'), ('12', 'bb'), ('13', 'ba')], [('02', 'ab'), ('03', 'aa'), ('12', 'bb'), ('13', 'bb')], [('02', 'ab'), ('03', 'ab'), ('12', 'aa'), ('13', 'aa')], [('02', 'ab'), ('03', 'ab'), ('12', 'aa'), ('13', 'ab')], [('02', 'ab'), ('03', 'ab'), ('12', 'aa'), ('13', 'ba')], [('02', 'ab'), ('03', 'ab'), ('12', 'aa'), ('13', 'bb')], [('02', 'ab'), ('03', 'ab'), ('12', 'ab'), ('13', 'aa')], [('02', 'ab'), ('03', 'ab'), ('12', 'ab'), ('13', 'ab')], [('02', 'ab'), ('03', 'ab'), ('12', 'ab'), ('13', 'ba')], [('02', 'ab'), ('03', 'ab'), ('12', 'ab'), ('13', 'bb')], [('02', 'ab'), ('03', 'ab'), ('12', 'ba'), ('13', 'aa')], [('02', 'ab'), ('03', 'ab'), ('12', 'ba'), ('13', 'ab')], [('02', 'ab'), ('03', 'ab'), ('12', 'ba'), ('13', 'ba')], [('02', 'ab'), ('03', 'ab'), ('12', 'ba'), ('13', 'bb')], [('02', 'ab'), ('03', 'ab'), ('12', 'bb'), ('13', 'aa')], [('02', 'ab'), ('03', 'ab'), ('12', 'bb'), ('13', 'ab')], [('02', 'ab'), ('03', 'ab'), ('12', 'bb'), ('13', 'ba')], [('02', 'ab'), ('03', 'ab'), ('12', 'bb'), ('13', 'bb')], [('02', 'ab'), ('03', 'ba'), ('12', 'aa'), ('13', 'aa')], [('02', 'ab'), ('03', 'ba'), ('12', 'aa'), ('13', 'ab')], [('02', 'ab'), ('03', 'ba'), ('12', 'aa'), ('13', 'ba')], [('02', 'ab'), ('03', 'ba'), ('12', 'aa'), ('13', 'bb')], [('02', 'ab'), ('03', 'ba'), ('12', 'ab'), ('13', 'aa')], [('02', 'ab'), ('03', 'ba'), ('12', 'ab'), ('13', 'ab')], [('02', 'ab'), ('03', 'ba'), ('12', 'ab'), ('13', 'ba')], [('02', 'ab'), ('03', 'ba'), ('12', 'ab'), ('13', 'bb')], [('02', 'ab'), ('03', 'ba'), ('12', 'ba'), ('13', 'aa')], [('02', 'ab'), ('03', 'ba'), ('12', 'ba'), ('13', 'ab')], [('02', 'ab'), ('03', 'ba'), ('12', 'ba'), ('13', 'ba')], [('02', 'ab'), ('03', 'ba'), ('12', 'ba'), ('13', 'bb')], [('02', 'ab'), ('03', 'ba'), ('12', 'bb'), ('13', 'aa')], [('02', 'ab'), ('03', 'ba'), ('12', 'bb'), ('13', 'ab')], [('02', 'ab'), ('03', 'ba'), ('12', 'bb'), ('13', 'ba')], [('02', 'ab'), ('03', 'ba'), ('12', 'bb'), ('13', 'bb')], [('02', 'ab'), ('03', 'bb'), ('12', 'aa'), ('13', 'aa')], [('02', 'ab'), ('03', 'bb'), ('12', 'aa'), ('13', 'ab')], [('02', 'ab'), ('03', 'bb'), ('12', 'aa'), ('13', 'ba')], [('02', 'ab'), ('03', 'bb'), ('12', 'aa'), ('13', 'bb')], [('02', 'ab'), ('03', 'bb'), ('12', 'ab'), ('13', 'aa')], [('02', 'ab'), ('03', 'bb'), ('12', 'ab'), ('13', 'ab')], [('02', 'ab'), ('03', 'bb'), ('12', 'ab'), ('13', 'ba')], [('02', 'ab'), ('03', 'bb'), ('12', 'ab'), ('13', 'bb')], [('02', 'ab'), ('03', 'bb'), ('12', 'ba'), ('13', 'aa')], [('02', 'ab'), ('03', 'bb'), ('12', 'ba'), ('13', 'ab')], [('02', 'ab'), ('03', 'bb'), ('12', 'ba'), ('13', 'ba')], [('02', 'ab'), ('03', 'bb'), ('12', 'ba'), ('13', 'bb')], [('02', 'ab'), ('03', 'bb'), ('12', 'bb'), ('13', 'aa')], [('02', 'ab'), ('03', 'bb'), ('12', 'bb'), ('13', 'ab')], [('02', 'ab'), ('03', 'bb'), ('12', 'bb'), ('13', 'ba')], [('02', 'ab'), ('03', 'bb'), ('12', 'bb'), ('13', 'bb')], [('02', 'ba'), ('03', 'aa'), ('12', 'aa'), ('13', 'aa')], [('02', 'ba'), ('03', 'aa'), ('12', 'aa'), ('13', 'ab')], [('02', 'ba'), ('03', 'aa'), ('12', 'aa'), ('13', 'ba')], [('02', 'ba'), ('03', 'aa'), ('12', 'aa'), ('13', 'bb')], [('02', 'ba'), ('03', 'aa'), ('12', 'ab'), ('13', 'aa')], [('02', 'ba'), ('03', 'aa'), ('12', 'ab'), ('13', 'ab')], [('02', 'ba'), ('03', 'aa'), ('12', 'ab'), ('13', 'ba')], [('02', 'ba'), ('03', 'aa'), ('12', 'ab'), ('13', 'bb')], [('02', 'ba'), ('03', 'aa'), ('12', 'ba'), ('13', 'aa')], [('02', 'ba'), ('03', 'aa'), ('12', 'ba'), ('13', 'ab')], [('02', 'ba'), ('03', 'aa'), ('12', 'ba'), ('13', 'ba')], [('02', 'ba'), ('03', 'aa'), ('12', 'ba'), ('13', 'bb')], [('02', 'ba'), ('03', 'aa'), ('12', 'bb'), ('13', 'aa')], [('02', 'ba'), ('03', 'aa'), ('12', 'bb'), ('13', 'ab')], [('02', 'ba'), ('03', 'aa'), ('12', 'bb'), ('13', 'ba')], [('02', 'ba'), ('03', 'aa'), ('12', 'bb'), ('13', 'bb')], [('02', 'ba'), ('03', 'ab'), ('12', 'aa'), ('13', 'aa')], [('02', 'ba'), ('03', 'ab'), ('12', 'aa'), ('13', 'ab')], [('02', 'ba'), ('03', 'ab'), ('12', 'aa'), ('13', 'ba')], [('02', 'ba'), ('03', 'ab'), ('12', 'aa'), ('13', 'bb')], [('02', 'ba'), ('03', 'ab'), ('12', 'ab'), ('13', 'aa')], [('02', 'ba'), ('03', 'ab'), ('12', 'ab'), ('13', 'ab')], [('02', 'ba'), ('03', 'ab'), ('12', 'ab'), ('13', 'ba')], [('02', 'ba'), ('03', 'ab'), ('12', 'ab'), ('13', 'bb')], [('02', 'ba'), ('03', 'ab'), ('12', 'ba'), ('13', 'aa')], [('02', 'ba'), ('03', 'ab'), ('12', 'ba'), ('13', 'ab')], [('02', 'ba'), ('03', 'ab'), ('12', 'ba'), ('13', 'ba')], [('02', 'ba'), ('03', 'ab'), ('12', 'ba'), ('13', 'bb')], [('02', 'ba'), ('03', 'ab'), ('12', 'bb'), ('13', 'aa')], [('02', 'ba'), ('03', 'ab'), ('12', 'bb'), ('13', 'ab')], [('02', 'ba'), ('03', 'ab'), ('12', 'bb'), ('13', 'ba')], [('02', 'ba'), ('03', 'ab'), ('12', 'bb'), ('13', 'bb')], [('02', 'ba'), ('03', 'ba'), ('12', 'aa'), ('13', 'aa')], [('02', 'ba'), ('03', 'ba'), ('12', 'aa'), ('13', 'ab')], [('02', 'ba'), ('03', 'ba'), ('12', 'aa'), ('13', 'ba')], [('02', 'ba'), ('03', 'ba'), ('12', 'aa'), ('13', 'bb')], [('02', 'ba'), ('03', 'ba'), ('12', 'ab'), ('13', 'aa')], [('02', 'ba'), ('03', 'ba'), ('12', 'ab'), ('13', 'ab')], [('02', 'ba'), ('03', 'ba'), ('12', 'ab'), ('13', 'ba')], [('02', 'ba'), ('03', 'ba'), ('12', 'ab'), ('13', 'bb')], [('02', 'ba'), ('03', 'ba'), ('12', 'ba'), ('13', 'aa')], [('02', 'ba'), ('03', 'ba'), ('12', 'ba'), ('13', 'ab')], [('02', 'ba'), ('03', 'ba'), ('12', 'ba'), ('13', 'ba')], [('02', 'ba'), ('03', 'ba'), ('12', 'ba'), ('13', 'bb')], [('02', 'ba'), ('03', 'ba'), ('12', 'bb'), ('13', 'aa')], [('02', 'ba'), ('03', 'ba'), ('12', 'bb'), ('13', 'ab')], [('02', 'ba'), ('03', 'ba'), ('12', 'bb'), ('13', 'ba')], [('02', 'ba'), ('03', 'ba'), ('12', 'bb'), ('13', 'bb')], [('02', 'ba'), ('03', 'bb'), ('12', 'aa'), ('13', 'aa')], [('02', 'ba'), ('03', 'bb'), ('12', 'aa'), ('13', 'ab')], [('02', 'ba'), ('03', 'bb'), ('12', 'aa'), ('13', 'ba')], [('02', 'ba'), ('03', 'bb'), ('12', 'aa'), ('13', 'bb')], [('02', 'ba'), ('03', 'bb'), ('12', 'ab'), ('13', 'aa')], [('02', 'ba'), ('03', 'bb'), ('12', 'ab'), ('13', 'ab')], [('02', 'ba'), ('03', 'bb'), ('12', 'ab'), ('13', 'ba')], [('02', 'ba'), ('03', 'bb'), ('12', 'ab'), ('13', 'bb')], [('02', 'ba'), ('03', 'bb'), ('12', 'ba'), ('13', 'aa')], [('02', 'ba'), ('03', 'bb'), ('12', 'ba'), ('13', 'ab')], [('02', 'ba'), ('03', 'bb'), ('12', 'ba'), ('13', 'ba')], [('02', 'ba'), ('03', 'bb'), ('12', 'ba'), ('13', 'bb')], [('02', 'ba'), ('03', 'bb'), ('12', 'bb'), ('13', 'aa')], [('02', 'ba'), ('03', 'bb'), ('12', 'bb'), ('13', 'ab')], [('02', 'ba'), ('03', 'bb'), ('12', 'bb'), ('13', 'ba')], [('02', 'ba'), ('03', 'bb'), ('12', 'bb'), ('13', 'bb')], [('02', 'bb'), ('03', 'aa'), ('12', 'aa'), ('13', 'aa')], [('02', 'bb'), ('03', 'aa'), ('12', 'aa'), ('13', 'ab')], [('02', 'bb'), ('03', 'aa'), ('12', 'aa'), ('13', 'ba')], [('02', 'bb'), ('03', 'aa'), ('12', 'aa'), ('13', 'bb')], [('02', 'bb'), ('03', 'aa'), ('12', 'ab'), ('13', 'aa')], [('02', 'bb'), ('03', 'aa'), ('12', 'ab'), ('13', 'ab')], [('02', 'bb'), ('03', 'aa'), ('12', 'ab'), ('13', 'ba')], [('02', 'bb'), ('03', 'aa'), ('12', 'ab'), ('13', 'bb')], [('02', 'bb'), ('03', 'aa'), ('12', 'ba'), ('13', 'aa')], [('02', 'bb'), ('03', 'aa'), ('12', 'ba'), ('13', 'ab')], [('02', 'bb'), ('03', 'aa'), ('12', 'ba'), ('13', 'ba')], [('02', 'bb'), ('03', 'aa'), ('12', 'ba'), ('13', 'bb')], [('02', 'bb'), ('03', 'aa'), ('12', 'bb'), ('13', 'aa')], [('02', 'bb'), ('03', 'aa'), ('12', 'bb'), ('13', 'ab')], [('02', 'bb'), ('03', 'aa'), ('12', 'bb'), ('13', 'ba')], [('02', 'bb'), ('03', 'aa'), ('12', 'bb'), ('13', 'bb')], [('02', 'bb'), ('03', 'ab'), ('12', 'aa'), ('13', 'aa')], [('02', 'bb'), ('03', 'ab'), ('12', 'aa'), ('13', 'ab')], [('02', 'bb'), ('03', 'ab'), ('12', 'aa'), ('13', 'ba')], [('02', 'bb'), ('03', 'ab'), ('12', 'aa'), ('13', 'bb')], [('02', 'bb'), ('03', 'ab'), ('12', 'ab'), ('13', 'aa')], [('02', 'bb'), ('03', 'ab'), ('12', 'ab'), ('13', 'ab')], [('02', 'bb'), ('03', 'ab'), ('12', 'ab'), ('13', 'ba')], [('02', 'bb'), ('03', 'ab'), ('12', 'ab'), ('13', 'bb')], [('02', 'bb'), ('03', 'ab'), ('12', 'ba'), ('13', 'aa')], [('02', 'bb'), ('03', 'ab'), ('12', 'ba'), ('13', 'ab')], [('02', 'bb'), ('03', 'ab'), ('12', 'ba'), ('13', 'ba')], [('02', 'bb'), ('03', 'ab'), ('12', 'ba'), ('13', 'bb')], [('02', 'bb'), ('03', 'ab'), ('12', 'bb'), ('13', 'aa')], [('02', 'bb'), ('03', 'ab'), ('12', 'bb'), ('13', 'ab')], [('02', 'bb'), ('03', 'ab'), ('12', 'bb'), ('13', 'ba')], [('02', 'bb'), ('03', 'ab'), ('12', 'bb'), ('13', 'bb')], [('02', 'bb'), ('03', 'ba'), ('12', 'aa'), ('13', 'aa')], [('02', 'bb'), ('03', 'ba'), ('12', 'aa'), ('13', 'ab')], [('02', 'bb'), ('03', 'ba'), ('12', 'aa'), ('13', 'ba')], [('02', 'bb'), ('03', 'ba'), ('12', 'aa'), ('13', 'bb')], [('02', 'bb'), ('03', 'ba'), ('12', 'ab'), ('13', 'aa')], [('02', 'bb'), ('03', 'ba'), ('12', 'ab'), ('13', 'ab')], [('02', 'bb'), ('03', 'ba'), ('12', 'ab'), ('13', 'ba')], [('02', 'bb'), ('03', 'ba'), ('12', 'ab'), ('13', 'bb')], [('02', 'bb'), ('03', 'ba'), ('12', 'ba'), ('13', 'aa')], [('02', 'bb'), ('03', 'ba'), ('12', 'ba'), ('13', 'ab')], [('02', 'bb'), ('03', 'ba'), ('12', 'ba'), ('13', 'ba')], [('02', 'bb'), ('03', 'ba'), ('12', 'ba'), ('13', 'bb')], [('02', 'bb'), ('03', 'ba'), ('12', 'bb'), ('13', 'aa')], [('02', 'bb'), ('03', 'ba'), ('12', 'bb'), ('13', 'ab')], [('02', 'bb'), ('03', 'ba'), ('12', 'bb'), ('13', 'ba')], [('02', 'bb'), ('03', 'ba'), ('12', 'bb'), ('13', 'bb')], [('02', 'bb'), ('03', 'bb'), ('12', 'aa'), ('13', 'aa')], [('02', 'bb'), ('03', 'bb'), ('12', 'aa'), ('13', 'ab')], [('02', 'bb'), ('03', 'bb'), ('12', 'aa'), ('13', 'ba')], [('02', 'bb'), ('03', 'bb'), ('12', 'aa'), ('13', 'bb')], [('02', 'bb'), ('03', 'bb'), ('12', 'ab'), ('13', 'aa')], [('02', 'bb'), ('03', 'bb'), ('12', 'ab'), ('13', 'ab')], [('02', 'bb'), ('03', 'bb'), ('12', 'ab'), ('13', 'ba')], [('02', 'bb'), ('03', 'bb'), ('12', 'ab'), ('13', 'bb')], [('02', 'bb'), ('03', 'bb'), ('12', 'ba'), ('13', 'aa')], [('02', 'bb'), ('03', 'bb'), ('12', 'ba'), ('13', 'ab')], [('02', 'bb'), ('03', 'bb'), ('12', 'ba'), ('13', 'ba')], [('02', 'bb'), ('03', 'bb'), ('12', 'ba'), ('13', 'bb')], [('02', 'bb'), ('03', 'bb'), ('12', 'bb'), ('13', 'aa')], [('02', 'bb'), ('03', 'bb'), ('12', 'bb'), ('13', 'ab')], [('02', 'bb'), ('03', 'bb'), ('12', 'bb'), ('13', 'ba')], [('02', 'bb'), ('03', 'bb'), ('12', 'bb'), ('13', 'bb')]]\n",
    "types = [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n",
    "priors = [-0.9178860550328204, -10.749415928290118, -10.749415928290118, -11.272664072079987, -10.749415928290118, -10.749415928290118, -16.95425710594061, -17.294055179550075, -10.749415928290118, -16.95425710594061, -10.749415928290118, -17.294055179550075, -11.272664072079987, -17.294055179550075, -17.294055179550075, -11.272664072079987, -10.749415928290118, -10.749415928290118, -16.95425710594061, -17.294055179550075, -10.749415928290118, -10.749415928290118, -16.95425710594061, -17.294055179550075, -16.95425710594061, -16.95425710594061, -16.95425710594061, -12.460704095246543, -17.294055179550075, -17.294055179550075, -20.83821243446749, -17.294055179550075, -10.749415928290118, -16.95425710594061, -10.749415928290118, -17.294055179550075, -16.95425710594061, -16.95425710594061, -16.95425710594061, -12.460704095246543, -10.749415928290118, -16.95425710594061, -10.749415928290118, -17.294055179550075, -17.294055179550075, -20.83821243446749, -17.294055179550075, -17.294055179550075, -11.272664072079987, -17.294055179550075, -17.294055179550075, -11.272664072079987, -17.294055179550075, -17.294055179550075, -20.83821243446749, -17.294055179550075, -17.294055179550075, -20.83821243446749, -17.294055179550075, -17.294055179550075, -11.272664072079987, -17.294055179550075, -17.294055179550075, -11.272664072079987, -10.749415928290118, -10.749415928290118, -16.95425710594061, -17.294055179550075, -10.749415928290118, -10.749415928290118, -16.95425710594061, -17.294055179550075, -16.95425710594061, -16.95425710594061, -16.95425710594061, -20.83821243446749, -17.294055179550075, -17.294055179550075, -12.460704095246543, -17.294055179550075, -10.749415928290118, -10.749415928290118, -16.95425710594061, -17.294055179550075, -10.749415928290118, -2.304180416152711, -11.272664072079987, -10.749415928290118, -16.95425710594061, -11.272664072079987, -11.272664072079987, -16.95425710594061, -17.294055179550075, -10.749415928290118, -16.95425710594061, -10.749415928290118, -16.95425710594061, -16.95425710594061, -16.95425710594061, -20.83821243446749, -16.95425710594061, -11.272664072079987, -11.272664072079987, -16.95425710594061, -16.95425710594061, -11.272664072079987, -11.272664072079987, -16.95425710594061, -20.83821243446749, -16.95425710594061, -16.95425710594061, -16.95425710594061, -17.294055179550075, -17.294055179550075, -12.460704095246543, -17.294055179550075, -17.294055179550075, -10.749415928290118, -16.95425710594061, -10.749415928290118, -20.83821243446749, -16.95425710594061, -16.95425710594061, -16.95425710594061, -17.294055179550075, -10.749415928290118, -16.95425710594061, -10.749415928290118, -10.749415928290118, -16.95425710594061, -10.749415928290118, -17.294055179550075, -16.95425710594061, -16.95425710594061, -16.95425710594061, -20.83821243446749, -10.749415928290118, -16.95425710594061, -10.749415928290118, -17.294055179550075, -17.294055179550075, -12.460704095246543, -17.294055179550075, -17.294055179550075, -16.95425710594061, -16.95425710594061, -16.95425710594061, -20.83821243446749, -16.95425710594061, -11.272664072079987, -11.272664072079987, -16.95425710594061, -16.95425710594061, -11.272664072079987, -11.272664072079987, -16.95425710594061, -20.83821243446749, -16.95425710594061, -16.95425710594061, -16.95425710594061, -10.749415928290118, -16.95425710594061, -10.749415928290118, -17.294055179550075, -16.95425710594061, -11.272664072079987, -11.272664072079987, -16.95425710594061, -10.749415928290118, -11.272664072079987, -2.304180416152711, -10.749415928290118, -17.294055179550075, -16.95425710594061, -10.749415928290118, -10.749415928290118, -17.294055179550075, -12.460704095246543, -17.294055179550075, -17.294055179550075, -20.83821243446749, -16.95425710594061, -16.95425710594061, -16.95425710594061, -17.294055179550075, -16.95425710594061, -10.749415928290118, -10.749415928290118, -17.294055179550075, -16.95425710594061, -10.749415928290118, -10.749415928290118, -11.272664072079987, -17.294055179550075, -17.294055179550075, -11.272664072079987, -17.294055179550075, -17.294055179550075, -20.83821243446749, -17.294055179550075, -17.294055179550075, -20.83821243446749, -17.294055179550075, -17.294055179550075, -11.272664072079987, -17.294055179550075, -17.294055179550075, -11.272664072079987, -17.294055179550075, -17.294055179550075, -20.83821243446749, -17.294055179550075, -17.294055179550075, -10.749415928290118, -16.95425710594061, -10.749415928290118, -12.460704095246543, -16.95425710594061, -16.95425710594061, -16.95425710594061, -17.294055179550075, -10.749415928290118, -16.95425710594061, -10.749415928290118, -17.294055179550075, -20.83821243446749, -17.294055179550075, -17.294055179550075, -12.460704095246543, -16.95425710594061, -16.95425710594061, -16.95425710594061, -17.294055179550075, -16.95425710594061, -10.749415928290118, -10.749415928290118, -17.294055179550075, -16.95425710594061, -10.749415928290118, -10.749415928290118, -11.272664072079987, -17.294055179550075, -17.294055179550075, -11.272664072079987, -17.294055179550075, -10.749415928290118, -16.95425710594061, -10.749415928290118, -17.294055179550075, -16.95425710594061, -10.749415928290118, -10.749415928290118, -11.272664072079987, -10.749415928290118, -10.749415928290118, -0.9178860550328204]\n",
    "\n",
    "\n",
    "types = np.array(types)\n",
    "no_of_each_type = np.bincount(types)\n",
    "print('')\n",
    "print(\"no_of_each_type is:\")\n",
    "print(no_of_each_type)\n",
    "\n",
    "class_per_lang = classify_all_languages(all_possible_languages)\n",
    "print('')\n",
    "print('')\n",
    "# print(\"class_per_lang is:\")\n",
    "# print(class_per_lang)\n",
    "no_of_each_class = np.bincount(class_per_lang.astype(int))\n",
    "print('')\n",
    "print(\"no_of_each_class is:\")\n",
    "print(no_of_each_class)\n",
    "\n",
    "\n",
    "# Hmmm, that gives us slightly different numbers! Is that caused by a problem in our\n",
    "# create_all_languages() function, or in our classify_lang() function?\n",
    "# To find out, let's compare our list of all languages to that from SimLang lab 21:\n",
    "\n",
    "# First, we need to change the way we represent the list of all languages to match\n",
    "# that of lab 21:\n",
    "\n",
    "def transform_all_languages_to_simlang_format(language_list):\n",
    "    \"\"\"\n",
    "    Takes a list of languages as represented by me (with only the forms listed\n",
    "    for each language, assuming the meaning for each form is specified by the\n",
    "    form's index), and turning it into a list of languages as represented in\n",
    "    SimLang lab 21 (which in turn is based on Kirby et al., 2015), in which a\n",
    "    <meaning, form> pair forms a tuple, and four of those tuples in a list form\n",
    "    a language\n",
    "\n",
    "    :param language_list: list of all languages\n",
    "    :type language_list: list\n",
    "    :returns: list of the input languages in the format of SimLang lab 21\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    all_langs_as_in_simlang = []\n",
    "    for l in range(len(all_possible_languages)):\n",
    "        lang_as_in_simlang = [(meanings[x], all_possible_languages[l][x]) for x in range(len(meanings))]\n",
    "        all_langs_as_in_simlang.append(lang_as_in_simlang)\n",
    "    return all_langs_as_in_simlang\n",
    "\n",
    "\n",
    "all_langs_as_in_simlang = transform_all_languages_to_simlang_format(all_possible_languages)\n",
    "print('')\n",
    "print('')\n",
    "# print(\"all_langs_as_in_simlang is:\")\n",
    "# print(all_langs_as_in_simlang)\n",
    "print(\"len(all_langs_as_in_simlang) is:\")\n",
    "print(len(all_langs_as_in_simlang))\n",
    "print(\"len(all_langs_as_in_simlang[0]) is:\")\n",
    "print(len(all_langs_as_in_simlang[0]))\n",
    "print(\"len(all_langs_as_in_simlang[0][0]) is:\")\n",
    "print(len(all_langs_as_in_simlang[0][0]))\n",
    "\n",
    "\n",
    "def check_all_lang_lists_against_each_other(language_list_a, language_list_b):\n",
    "    if len(language_list_a) != len(language_list_b):\n",
    "        raise ValueError(\"The two language lists should be of the same size\")\n",
    "    checks_per_lang = np.zeros(len(language_list_a))\n",
    "    for i in range(len(language_list_a)):\n",
    "        for j in range(len(language_list_b)):\n",
    "            if language_list_a[i] == language_list_b[j]:\n",
    "                checks_per_lang[i] = 1.\n",
    "    return checks_per_lang\n",
    "\n",
    "\n",
    "checks_per_language = check_all_lang_lists_against_each_other(all_langs_as_in_simlang, languages)\n",
    "print('')\n",
    "print('')\n",
    "print(\"checks_per_language is:\")\n",
    "print(checks_per_language)\n",
    "print(\"np.sum(checks_per_language) is:\")\n",
    "print(np.sum(checks_per_language))\n",
    "\n",
    "\n",
    "# Ok, this shows that for each language in the list of all_possible_languages generated by my own code, there is a\n",
    "# corresponding languages in the code from SimLang lab 21, so instead there must be something wrong with the way I\n",
    "# categorise the languages. Firstly, it looks like my classify_language() function overestimates the number of\n",
    "# compositional languages. So let's first have a look at which languages it classifies as compositional:\n",
    "\n",
    "\n",
    "compositional_langs_indices_my_code = np.where(class_per_lang==3)[0]\n",
    "print('')\n",
    "print('')\n",
    "print(\"compositional_langs_indices_my_code MY CODE are:\")\n",
    "print(compositional_langs_indices_my_code)\n",
    "print(\"len(compositional_langs_indices_my_code) MY CODE are:\")\n",
    "print(len(compositional_langs_indices_my_code))\n",
    "\n",
    "\n",
    "for index in compositional_langs_indices_my_code:\n",
    "    print('')\n",
    "    print(\"index MY CODE is:\")\n",
    "    print(index)\n",
    "    print(\"all_possible_languages[index] MY CODE is:\")\n",
    "    print(all_possible_languages[index])\n",
    "\n",
    "\n",
    "# And now let's do the same for the languages from SimLang Lab 21:\n",
    "\n",
    "compositional_langs_indices_simlang = np.where(np.array(types)==3)[0]\n",
    "print('')\n",
    "print('')\n",
    "print(\"compositional_langs_indices_simlang SIMLANG CODE are:\")\n",
    "print(compositional_langs_indices_simlang)\n",
    "print(\"len(compositional_langs_indices_simlang) SIMLANG CODE are:\")\n",
    "print(len(compositional_langs_indices_simlang))\n",
    "\n",
    "\n",
    "for index in compositional_langs_indices_simlang:\n",
    "    print('')\n",
    "    print(\"index SIMLANG CODE is:\")\n",
    "    print(index)\n",
    "    print(\"languages[index] SIMLANG CODE is:\")\n",
    "    print(languages[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, so it looks like instead of there being a bug in my code, there's actually a bug in the SimLang lab 21 code (or rather, in the code that generated the list of types that was copied into SimLang lab 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's implement the actual production function. Here is the equation from Kirby et al. (2015) again:\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l\\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $f$ stands for form (in the sense of a complete signal, such as *aa*), $l$ stands for language, $t$ stands for topic, and $\\epsilon$ stands for production error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T14:09:51.573083Z",
     "start_time": "2019-08-12T14:09:51.564845Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is the production() function at work:\n",
      "\n",
      "language is:\n",
      "('aa', 'ab', 'ba', 'bb')\n",
      "\n",
      "meanings are:\n",
      "['02', '03', '12', '13']\n",
      "\n",
      "topic_index is:\n",
      "0\n",
      "correct_form is:\n",
      "aa\n",
      "error_forms is:\n",
      "['ab', 'ba', 'bb']\n",
      "Check that removing the correct form didn't mess up the language:\n",
      "language is:\n",
      "('aa', 'ab', 'ba', 'bb')\n"
     ]
    }
   ],
   "source": [
    "# first we need to write a quick function that removes every instance of a given element from a list:\n",
    "def remove_all_instances(my_list, element_to_be_removed):\n",
    "    i = 0  # loop counter\n",
    "    length = len(my_list)  # list length\n",
    "    while (i < len(my_list)):\n",
    "        if (my_list[i] == element_to_be_removed):\n",
    "            my_list.remove(my_list[i])\n",
    "            # as an element is removed\n",
    "            # so decrease the length by 1\n",
    "            length = length - 1\n",
    "            # run loop again to check element\n",
    "            # at same index, when item removed\n",
    "            # next item will shift to the left\n",
    "            continue\n",
    "        i = i + 1\n",
    "    return my_list\n",
    "\n",
    "\n",
    "# and now for the actual production function:\n",
    "def production(language, topic):\n",
    "    print('')\n",
    "    print('This is the production() function at work:')\n",
    "    print('')\n",
    "    print(\"language is:\")\n",
    "    print(language)\n",
    "    print('')\n",
    "    print(\"meanings are:\")\n",
    "    print(meanings)\n",
    "    for m in range(len(meanings)):\n",
    "        if meanings[m] == topic:\n",
    "            topic_index = m\n",
    "    print('')\n",
    "    print(\"topic_index is:\")\n",
    "    print(topic_index)\n",
    "    correct_form = language[topic_index]\n",
    "    print(\"correct_form is:\")\n",
    "    print(correct_form)\n",
    "    error_forms = list(language)\n",
    "    error_forms = remove_all_instances(error_forms, correct_form)\n",
    "    if len(\n",
    "            error_forms\n",
    "    ) == 0:  # if the list of error_forms is empty because the language is degenerate\n",
    "        error_forms = language  # simply choose an error_form from the whole language\n",
    "    print(\"error_forms is:\")\n",
    "    print(error_forms)\n",
    "    print(\"Check that removing the correct form didn't mess up the language:\")\n",
    "    print(\"language is:\")\n",
    "    print(language)\n",
    "\n",
    "\n",
    "production(compositional_lang, \"02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's implement noisy production:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  \n",
    "Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87â€“102. https://doi.org/10.1016/j.cognition.2015.03.016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
