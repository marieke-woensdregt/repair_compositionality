{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-08-06 11:19:34  \n",
    "\n",
    "# The evolution of compositionality under environmental noise\n",
    "\n",
    "\n",
    "\n",
    "## Hypothesis:\n",
    "\n",
    "**We hypothesise that** other-initiated repair and a compositional language co-evolve under the following assumptions:\n",
    "\n",
    "1. There is a pressure for communicative success\n",
    "2. Listeners have at least two types of repair initiator at their disposal: (i) open request, and (ii) closed request\n",
    "3. There is a pressure to make repair sequences as efficient as possible (in terms of combined utterance length of repair initiator and response)\n",
    "    \n",
    "This hypothesis is based on the idea that if repair sequences are under a pressure to be efficient, then it becomes useful to be able to initiate repair by asking for clarification of just _part_ of the utterance, specifically by feeding back the part of the meaning that you _did_ get. Under those circumstances, a compositional system is helpful (compared to a holistic system), because parts of the signal (i.e. 'substrings') map to parts of the meaning. _**<span class=\"mark\">Note</span>**_ that this hypothesis is thus based on the assumption that the 'closed request' type of repair initiator works only if the listener has _understood_ part of the _meaning_ of the signal, as opposed to just having received part of the _signal_ and being able to repeat that back verbatim, in order to prompt the speaker to repeat the part that didn't come through. We feel that this is a reasonable assumption based on what we know from empirical work, but it is a little hard to assess given that natural language _is_ compositional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-08-06 11:19:27 \n",
    "\n",
    "## Predictions:\n",
    "\n",
    "Here we adapt the model of Kirby et al. (2015)---where they show that a compositional language evolves when there is a pressure for both (i) learnability and (ii) expressivity---to incorporate noise (step 1) and repair (step 2).\n",
    "\n",
    "In Kirby et al.'s model, the pressure for expressivity (implemented as speakers aiming to avoid ambiguity when they produce a signal) is what causes populations to move towards languages that have one-to-one mappings between meanings and signals, but these can be either compositional or holistic languages. The pressure for learnability then (implemented as a prior that favours more compressible languages) is what causes populations to move towards compositional languages rather than holistic languages, because compositional languages are more compressible (and therefore, is the assumption, easier to learn). \n",
    "\n",
    "### Strong prediction (excluding pressure for learnability):\n",
    "\n",
    "Therefore, _**the strongest prediction**_ given our reasoning above, would be that if we take away the pressure for learnability, but add in environmental noise and the possibility of repair (combined with a pressure for efficient repair sequences), populations would also converge on a compositional language. This may be a bit of a weird prediction in itself, because I think the assumption of a pressure for learnability makes sense, but by isolating the effect of the two (learnability vs. noise+repair) we could say something about whether they might both contribute to making compositionality a good solution for the pressures that are at play in language evolution.\n",
    "\n",
    "### More reasonable prediction (including pressure for learnability):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### References  \n",
    "Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87â€“102. https://doi.org/10.1016/j.cognition.2015.03.016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2019-07-31 15:27:33 \n",
    "\n",
    "## Production in the Kirby et al. (2015) model:\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l\\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $f$ stands for form (in the sense of a complete signal, such as *aa*), $l$ stands for language, $t$ stands for topic, $a$ stands for ambiguity (i.e. the number of meanings in $M$ that map to form $f$ in $l$), $\\gamma$ specifies the extent to which ambiguous utterances are penalised, and $\\epsilon$ stands for production error.\n",
    "\n",
    "\n",
    "From Kirby et al. (2015, pp. 92-93):\n",
    "> \"If $a$ = 1 ($f$ is unambiguous) and/or $\\gamma$ = 0 then this yields a model of production where the 'correct' form is produced with probability $1-\\epsilon$. However, when $\\gamma$ > 0 and $f$ is ambiguous (i.e., $a$ > 1), then the 'correct' mapping from $t$ to $f$ is less likely to be produced (the probability $P(f \\mid l, t)$ is reduced by the factor $(\\frac{1}{a})^\\gamma$ ) and the remaining probability mass is spread equally over the other possible forms, leading to increased probability of producing $f'$ $\\neq$ $f$. Therefore, $\\gamma$ > 0 introduces a penalty for languages whose utterances are ambiguous.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2019-07-31 15:27:52 \n",
    "## How do we add *environmental* noise to this production model?\n",
    "\n",
    "\n",
    "If we leave Kirby et al.'s production error $\\epsilon$ aside for a moment, we could add environmental noise by allowing production to have two possible outcomes. Given a particular topic *t*, a speaker could produce either:\n",
    "- form $f$ if $t$ is mapped to $f$ in $l$, with probability $1-n$ (where $n$ stands for the probability of noise happening)\n",
    "- form $f_{noisy}$, where $f_{noisy}$ is a 'noisy variant' of $f$ *and* $t$ is mapped to $f$ in $l$. This should happen with probability $\\frac{n}{|F_{noisy}|}$ (where $F_{noisy}$ is the full set of possible noisy variants of the form $f$ that maps to $t$ in $l$\n",
    "\n",
    "To give an example, using the example compositional grammar from Kirby et al. (2015, p. 92):\n",
    "\n",
    "S   --> A B   \n",
    "A:0 --> a   \n",
    "A:1 --> b   \n",
    "B:2 --> a   \n",
    "B:3 --> b   \n",
    "\n",
    "Given this grammar, the only form that maps to topic 02 is *aa*.   \n",
    "_**<span class=\"mark\"><span class=\"mark\">Design decision:</span></span>**_ If we allow for both 'partial' and 'full' noise,   \n",
    " this form has three possible noisy variants:\n",
    "- a_\n",
    "- _a\n",
    "- \\__\n",
    "\n",
    "where _ stands for a noisy part of the signal that the listener could not perceive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_**<span class=\"girk\">Decision**_ Mark D. + Marieke (01/08/2019):</span> It is interesting to allow for both open requests and closed requests in this model, because the intuition that our hypothesis is based on, is that whereas a compositional system allows for the usage of both open and closed requests, a holistic system only allows for open requests. \n",
    "\n",
    "\n",
    "So if 02 is the speaker's intended topic (and if we exclude the possibility of the speaker making a production error for now), they will produce the following signals with the following probabilities:\n",
    "- aa with probability $1-n$\n",
    "- [a_, \\_a, \\__ \\] with probability $\\frac{n}{3}$ (because there are 3 possible noisy variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_**<span class=\"mark\">Design decision:</span>**_ should noisy variant \\__ be less likely than other noisy variants, because it means that really *none* of the signal came through? Intuitively it would make sense if a_ was twice as likely to happen as \\__, but that would make the production function yet more complicated. So to keep it simple I propose making all the noisy variants equally probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's add the probability of making a production error $\\epsilon$ back in, which gives us: \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\, (1-n) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact} \\\\ \n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\; \\; \\textrm{if part of} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact} \\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\, (1-n) \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact}\\\\\n",
    "\\frac{\\epsilon}{|F|-1} \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\quad \\quad \\quad \\textrm{if no part of} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where *n* stands for the probability of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-07-31 15:28:14 \n",
    "\n",
    "### First, let's reproduce Kirby et al.'s (2015) model of production:\n",
    "\n",
    "In order to do that we first need to set some general parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T11:05:53.784718Z",
     "start_time": "2019-08-01T11:05:53.782018Z"
    }
   },
   "outputs": [],
   "source": [
    "meanings = ['02', '03', '12', '13']  # all possible meanings\n",
    "forms = ['aa', 'ab', 'ba', 'bb']  # all possible forms\n",
    "error = 0.05  # the probability of making a production error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need some functions to generate all possible languages and classify languages according to the categories specified by Kirby et al. (2015:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T11:06:00.984220Z",
     "start_time": "2019-08-01T11:06:00.968009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_possible_languages are:\n",
      "[('aa', 'aa', 'aa', 'aa'), ('aa', 'aa', 'aa', 'ab'), ('aa', 'aa', 'aa', 'ba'), ('aa', 'aa', 'aa', 'bb'), ('aa', 'aa', 'ab', 'ab'), ('aa', 'aa', 'ab', 'ba'), ('aa', 'aa', 'ab', 'bb'), ('aa', 'aa', 'ba', 'ba'), ('aa', 'aa', 'ba', 'bb'), ('aa', 'aa', 'bb', 'bb'), ('aa', 'ab', 'ab', 'ab'), ('aa', 'ab', 'ab', 'ba'), ('aa', 'ab', 'ab', 'bb'), ('aa', 'ab', 'ba', 'ba'), ('aa', 'ab', 'ba', 'bb'), ('aa', 'ab', 'bb', 'bb'), ('aa', 'ba', 'ba', 'ba'), ('aa', 'ba', 'ba', 'bb'), ('aa', 'ba', 'bb', 'bb'), ('aa', 'bb', 'bb', 'bb'), ('ab', 'ab', 'ab', 'ab'), ('ab', 'ab', 'ab', 'ba'), ('ab', 'ab', 'ab', 'bb'), ('ab', 'ab', 'ba', 'ba'), ('ab', 'ab', 'ba', 'bb'), ('ab', 'ab', 'bb', 'bb'), ('ab', 'ba', 'ba', 'ba'), ('ab', 'ba', 'ba', 'bb'), ('ab', 'ba', 'bb', 'bb'), ('ab', 'bb', 'bb', 'bb'), ('ba', 'ba', 'ba', 'ba'), ('ba', 'ba', 'ba', 'bb'), ('ba', 'ba', 'bb', 'bb'), ('ba', 'bb', 'bb', 'bb'), ('bb', 'bb', 'bb', 'bb')]\n",
      "number of possible languages is:\n",
      "35\n",
      "\n",
      "degenerate_lang is:\n",
      "('aa', 'aa', 'aa', 'aa')\n",
      "class_degenerate_lang is:\n",
      "degenerate\n",
      "\n",
      "compositional_lang is:\n",
      "('aa', 'ab', 'ba', 'bb')\n",
      "class_compositional_lang is:\n",
      "compositional\n",
      "\n",
      "holistic_lang is:\n",
      "('aa', 'ab', 'bb', 'ba')\n",
      "class_holistic_lang is:\n",
      "holistic\n",
      "\n",
      "other_lang is:\n",
      "('aa', 'aa', 'aa', 'ab')\n",
      "class_other_lang is:\n",
      "other\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def create_all_possible_languages(meanings, forms):\n",
    "    \"\"\"Creates all possible languages\n",
    "\n",
    "    :param meanings: list of strings corresponding to all possible meanings\n",
    "    :type meanings: list\n",
    "    :param forms: list of strings corresponding to all possible forms\n",
    "    :type forms: list\n",
    "    :returns: list of tuples which represent languages, where each tuple consists of forms and has length len(meanings)\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    all_possible_languages = list(\n",
    "        itertools.combinations_with_replacement(forms, len(meanings)))\n",
    "    return all_possible_languages\n",
    "\n",
    "\n",
    "def classify_language(lang, forms, meanings):\n",
    "    \"\"\"\n",
    "    Classify a given language as either 'compositional', 'holistic', 'degenerate' or 'other' (Kirby et al., 2015)\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms, where each form index maps to same index in meanings\n",
    "    :type lang: tuple\n",
    "    :param forms: list of strings corresponding to all possible forms\n",
    "    :type forms: list\n",
    "    :returns: the category that the language falls into\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # TODO: See if I can modify this function so that it can deal with any number of forms and meanings.\n",
    "    # First check whether some conditions are met, bc this function hasn't been coded up in the most general way yet:\n",
    "    if len(forms) != 4:\n",
    "        raise ValueError(\n",
    "            \"This function only works for a world in which there are 4 possible forms\"\n",
    "        )\n",
    "    if len(forms[0]) != 2:\n",
    "        raise ValueError(\n",
    "            \"This function only works when each form consists of 2 elements\")\n",
    "    if len(lang) != len(meanings):\n",
    "        raise ValueError(\"Lang should have same length as meanings\")\n",
    "\n",
    "    # lang is degenerate if it uses the same form for every meaning:\n",
    "    if lang[0] == lang[1] and lang[1] == lang[2] and lang[2] == lang[3]:\n",
    "        return 'degenerate'\n",
    "\n",
    "    # lang is compositional if it's *not* degenerate, and each form element maps to the same meaning element for each form:\n",
    "    elif lang[0][0] == lang[1][0] and lang[2][0] == lang[3][0] and lang[0][\n",
    "            1] == lang[2][1] and lang[1][1] == lang[3][1]:\n",
    "        return 'compositional'\n",
    "\n",
    "    # lang is holistic if it is *not* compositional, but *does* make us of all possible forms:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[\n",
    "            3] in lang:\n",
    "        return 'holistic'\n",
    "\n",
    "    # In all other cases, a language belongs to the 'other' category:\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "\n",
    "# Let's try out our create_all_possible_languages() function:\n",
    "all_possible_languages = create_all_possible_languages(meanings, forms)\n",
    "print(\"all_possible_languages are:\")\n",
    "print(all_possible_languages)\n",
    "print(\"number of possible languages is:\")\n",
    "print(len(all_possible_languages))\n",
    "\n",
    "# Let's test our classify_language() function using some example languages from the Kirby et al. (2015) paper:\n",
    "degenerate_lang = ('aa', 'aa', 'aa', 'aa')\n",
    "print('')\n",
    "print(\"degenerate_lang is:\")\n",
    "print(degenerate_lang)\n",
    "class_degenerate_lang = classify_language(degenerate_lang, forms, meanings)\n",
    "print(\"class_degenerate_lang is:\")\n",
    "print(class_degenerate_lang)\n",
    "\n",
    "compositional_lang = ('aa', 'ab', 'ba', 'bb')\n",
    "print('')\n",
    "print(\"compositional_lang is:\")\n",
    "print(compositional_lang)\n",
    "class_compositional_lang = classify_language(compositional_lang, forms,\n",
    "                                             meanings)\n",
    "print(\"class_compositional_lang is:\")\n",
    "print(class_compositional_lang)\n",
    "\n",
    "holistic_lang = ('aa', 'ab', 'bb', 'ba')\n",
    "print('')\n",
    "print(\"holistic_lang is:\")\n",
    "print(holistic_lang)\n",
    "class_holistic_lang = classify_language(holistic_lang, forms, meanings)\n",
    "print(\"class_holistic_lang is:\")\n",
    "print(class_holistic_lang)\n",
    "\n",
    "other_lang = ('aa', 'aa', 'aa', 'ab')\n",
    "print('')\n",
    "print(\"other_lang is:\")\n",
    "print(other_lang)\n",
    "class_other_lang = classify_language(other_lang, forms, meanings)\n",
    "print(\"class_other_lang is:\")\n",
    "print(class_other_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's implement the actual production function. Here is the equation from Kirby et al. (2015) again:\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l\\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $f$ stands for form (in the sense of a complete signal, such as *aa*), $l$ stands for language, $t$ stands for topic, and $\\epsilon$ stands for production error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T11:06:10.288427Z",
     "start_time": "2019-08-01T11:06:10.278439Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is the production() function at work:\n",
      "\n",
      "language is:\n",
      "('aa', 'ab', 'ba', 'bb')\n",
      "\n",
      "meanings are:\n",
      "['02', '03', '12', '13']\n",
      "\n",
      "topic_index is:\n",
      "0\n",
      "correct_form is:\n",
      "aa\n",
      "error_forms is:\n",
      "['ab', 'ba', 'bb']\n",
      "Check that removing the correct form didn't mess up the language:\n",
      "language is:\n",
      "('aa', 'ab', 'ba', 'bb')\n"
     ]
    }
   ],
   "source": [
    "# first we need to write a quick function that removes every instance of a given element from a list:\n",
    "def remove_all_instances(my_list, element_to_be_removed):\n",
    "    i = 0  # loop counter\n",
    "    length = len(my_list)  # list length\n",
    "    while (i < len(my_list)):\n",
    "        if (my_list[i] == element_to_be_removed):\n",
    "            my_list.remove(my_list[i])\n",
    "            # as an element is removed\n",
    "            # so decrease the length by 1\n",
    "            length = length - 1\n",
    "            # run loop again to check element\n",
    "            # at same index, when item removed\n",
    "            # next item will shift to the left\n",
    "            continue\n",
    "        i = i + 1\n",
    "    return my_list\n",
    "\n",
    "\n",
    "# and now for the actual production function:\n",
    "def production(language, topic):\n",
    "    print('')\n",
    "    print('This is the production() function at work:')\n",
    "    print('')\n",
    "    print(\"language is:\")\n",
    "    print(language)\n",
    "    print('')\n",
    "    print(\"meanings are:\")\n",
    "    print(meanings)\n",
    "    for m in range(len(meanings)):\n",
    "        if meanings[m] == topic:\n",
    "            topic_index = m\n",
    "    print('')\n",
    "    print(\"topic_index is:\")\n",
    "    print(topic_index)\n",
    "    correct_form = language[topic_index]\n",
    "    print(\"correct_form is:\")\n",
    "    print(correct_form)\n",
    "    error_forms = list(language)\n",
    "    error_forms = remove_all_instances(error_forms, correct_form)\n",
    "    if len(\n",
    "            error_forms\n",
    "    ) == 0:  # if the list of error_forms is empty because the language is degenerate\n",
    "        error_forms = language  # simply choose an error_form from the whole language\n",
    "    print(\"error_forms is:\")\n",
    "    print(error_forms)\n",
    "    print(\"Check that removing the correct form didn't mess up the language:\")\n",
    "    print(\"language is:\")\n",
    "    print(language)\n",
    "\n",
    "\n",
    "production(compositional_lang, \"02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  \n",
    "Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87â€“102. https://doi.org/10.1016/j.cognition.2015.03.016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
