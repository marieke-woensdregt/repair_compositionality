{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different strategies for dealing with noise\n",
    "\n",
    "\n",
    "\n",
    "The idea here is that there are different possible strategies of dealing with noise that are attested in natural communication systems:\n",
    "\n",
    "* **Reduplication**: simply repeat the signal several times. This is compatible with a compositional system. It is not costly in terms of learnability, because the only extra thing that needs to be learned is a single extra rule that applies to all signals. However, it is relatively costly in terms of utterance length (it would thus not do well under a pressure for minimal effort). \n",
    "* **Diversify signal**: make the individual segments that each signal consists of as distinct as possible. For example, in the language shown below, all four signals can be distinguished from each other in all cases where one character is obscured by noise. This strategy however is not compatible with compositionality, because it relies on making each of the segments as distinct from each other as possible. That means these languages are necessarily holistic, and therefore less easy to learn (so they would do less well under a pressure for learnability). Below is an example of a language for which, if noise obscures a single character, each signal would still be uniquely identifiable under noise.\n",
    "    - 02 --> 'aaaa'\n",
    "    - 03 --> 'bbbb'\n",
    "    - 12 --> 'abba'\n",
    "    - 13 --> 'baab'\n",
    "* **Repair**: This strategy could be seen as a form of redundancy across turns, instead of within a signal. However, it will be initiated only when neccesary, and should therefore fare slightly better than the reduplication strategy under a pressure for minimal effort (where effort is measured as total shared utterance length of both interlocutors across a set number of interactions).\n",
    "\n",
    "The predictions of Vinicius & Seán (2016 Evolang abstract titled \"Language adapts to signal disruption in interaction\"), are that although the reduplication strategy and the repair strategy should do equally well under a pressure for learnability, adding the possibility of repair will 'lift the pressure for redundancy', such that receivers can request that speakers repeat a signal only after a problem occurs.\n",
    "---> However, we would add that in the absence of a pressure for minimal effort, the repair strategy does not have an advantage over the reduplication strategy. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions under different selection pressures:\n",
    "\n",
    "We predict that under the following assumptions:\n",
    "- There is a pressure for expressivity/mutual understanding (or rather: a pressure to get ones signal/message across; which feels like a better way to describe the pressure that frogs and song birds are under)\n",
    "- Noise regularly disrupts part of the signal (Vinicius & Seán used a 0.5 probability in their experiment)\n",
    "- Repair is a possibility\n",
    "\n",
    "\n",
    "the following strategies will become dominant under the following combinations of the presense/absence of a pressure for learnability and a pressure for minimal effort:\n",
    "\n",
    "|                | - minimal effort                                              | + minimal effort          |\n",
    "|----------------|---------------------------------------------------------------|---------------------------|\n",
    "| **- learnability** | Any of the three strategies above will do                                         | Repair + Compositional OR Holistic         |\n",
    "| **+ learnability** | Reduplication + Compositional OR Repair + Compositional | Repair + Compositional |\n",
    "\n",
    "<span class=\"mark\">Note</span> that the prediction in the {-learnability, +minimal effort} condition above only holds if we do not distinguish between open and closed requests. Because if we do, as in the model we submitted to evolang, we'd expect the Repair + compositional strategy to fare best in this condition, without the need for a pressure for learnability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to represent languages?\n",
    "\n",
    "### Possibility 1: Different form lengths\n",
    "\n",
    "If we continue with Kirby et al.'s (2015) way of representing meanings and forms (which is a minimal way of creating languages that we can classify as compositional, holistic or degenerate), where meanings consist of $f=2$ features, which can each have $v=2$ values, we can allow for each of the language strategies specified above ('reduplication' and 'diversify signal'), by simply allowing for multiple string lengths $l$, while keeping the alphabet size $|\\Sigma|$ at 2.\n",
    "\n",
    "For example, where Kirby et al. (2015) only allowed for a single possible string length, and specified $f = v = l = |\\Sigma| = 2$, we could minimally allow for two possible string lengths: one being equal to $f$ (i.e. the minimum string length required to uniquely specify each meaning feature), and one being equal to $2*f$, to enable reduplication of the signal.\n",
    "\n",
    "That would yield the following types of languages:\n",
    "\n",
    "\n",
    "**Reduplication + compositional:**\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> abab\n",
    "\n",
    "12 --> baba\n",
    "\n",
    "13 --> bbbb\n",
    "\n",
    "\n",
    "**Diversify signal + holistic:**\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> bbbb\n",
    "\n",
    "12 --> abba\n",
    "\n",
    "13 --> baab\n",
    "\n",
    "\n",
    "**Repair + compositional:**\n",
    "\n",
    "02 --> aa\n",
    "\n",
    "03 --> ab\n",
    "\n",
    "12 --> ba\n",
    "\n",
    "13 --> bb\n",
    "\n",
    "\n",
    "In order to still make it possible for iterated learning chains to transition from a language that uses forms of length 2 into a language that uses forms of length 4 and vice versa, we need to then also allow for languages that use a mixture of form lengths (e.g. three forms of length 2, and one form of length 4). This yields the following number of possible languages:\n",
    "\n",
    "$$ (2^2+2^4)^4 = 160000$$\n",
    "\n",
    "which means that compared to the Kirby et al. (2015) model (where there were ($(2^2)^4 = 256$ possible languages), the hypothesis space expands by a factor of 625. That is not ideal, because if we assume that simulation run times increase linearly with the size of the hypothesis space, a simulation that took 1 hour to run in our previous model would now take almost 4 weeks to run.\n",
    "\n",
    "However, this linear relationship between the simulation run times and the size of the hypothesis space holds when during learning, we actually loop through each hypothesis and update its posterior probability based on the data. There are a couple of ways in which this process can be optimised:\n",
    "\n",
    "1. **memoisation**: This would require enumerating all possible data points (i.e. <meaning, form> pairs) (including all possible noisy forms), and for each of them calculating its likelihood for all possible hypotheses **once**, and caching the result. Whenever the same <meaning, form> pair is then encountered by any learner, the corresponding likelihood vector is then simply retrieved from memory and multiplied with the learner's current posterior. This should be doable given that the total number of meanings is 4, and the total number of forms (including all possible noisy variants, assuming that noise is restricted to a single character) is 56; which makes 4\\*56 = 224 possible <meaning, form> pairs. For each of those 224 possible datapoints, we would then calculate its likelihood for all 160,000 hypotheses, and cache these values in a 224\\*160,000 matrix. (That matrix thus has 224\\*160,000 = 35,840,000 entries.)\n",
    "2. Intergenerational learning could be sped up by representing data as simple counts of <meaning, form> pairs, and simply updating the posterior probability distribution for the full data set in one step, by multiplying the prior of the hypothesis with the likelihood of the <meaning, form> pair to the power of the number of times it occurs in the data set. This should speed things up a little in intergenerational learning, but won't make a difference in *intra*generational learning, because there we assume that the hearer updates their posterior in each interaction. \n",
    "3. Do not do exact inference over the full hypothesis space at all, but instead use an MCMC sampling technique (e.g. Burkett & Griffiths, 2010, and Kirby et al., 2015 use Gibbs sampling) --> This would require a bit more time to figure out, and is hopefully not necessary once optimisations 1 and 2 above have been implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possibility 2: Allow reduplication as grammatical rule, and increase alphabet size\n",
    "\n",
    "If instead of allowing for multiple form lengths, we instead increase the size of the alphabet $\\Sigma$ from 2 to 4, that will make the diversify signal strategy possible. More concretely, that would mean that instead of there being an alphabet $[a, b]$, there would be an alphabet $[a, b, c, d]$. \n",
    "That would allow for the following example languages, where the bit at the end of the signal specifies whether the signal should be repeated (1) or not (0).\n",
    "\n",
    "\n",
    "**Reduplication + compositional:**\n",
    "\n",
    "02 --> aa1\n",
    "\n",
    "03 --> ab1\n",
    "\n",
    "12 --> ba1\n",
    "\n",
    "13 --> bb1\n",
    "\n",
    "\n",
    "**Diversify signal + holistic:**\n",
    "\n",
    "02 --> aa0\n",
    "\n",
    "03 --> bb0\n",
    "\n",
    "12 --> cc0\n",
    "\n",
    "13 --> dd0\n",
    "\n",
    "\n",
    "**Repair + compositional:**\n",
    "\n",
    "02 --> aa0\n",
    "\n",
    "03 --> ab0\n",
    "\n",
    "12 --> ba0\n",
    "\n",
    "13 --> bb0\n",
    "\n",
    "\n",
    "Choosing for this option would mean that instead of there being $(2ˆ2 + 2ˆ4) = 20$ possible forms, there would be $4ˆ2 = 16$ possible forms, and therefore $(4^2)^4 = 65536$ possible languages. In addition however, we'd need languages to have an extra bit that specifies whether signals are reduplicated or not (assuming there are only two options: reduplication ON versus reduplication OFF). That means that there'd be a total of $((4^2)^4)*2 = 131072$. So compared to possibility 1, possibility 2 only reduces the size of the hypothesis space by a factor of 1.22. That is not much, but an added advantage of this way of representing languages is that it allows for a straightforward way of capturing the assumed simplicity of a reduplication rule in the coding of the languages, and therefore into the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we find that despite the optimisation strategies outlined above it is still not feasible to run simulations within a reasonable time-frame, we could consider tackling the different possible strategies for dealing with noise separately. I.e. one model where we allow for the possibility to add reduplication to signals vs. repair, and another model where we allow for diversification of signal segments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressibility measure for two different language representation possibilities\n",
    "\n",
    "\n",
    "### Possibility 1: Different form lengths\n",
    "\n",
    "#### Rewrite rules:\n",
    "\n",
    "**Reduplication + compositional:**\n",
    "\n",
    "There are in fact two different ways of reduplicating a compositional language: either reduplicating the whole signal, or reduplicating each of the segments. In both cases the length of the minimally redundant form, and therefore the language type's compressibility, will be the same however, as shown below.\n",
    "\n",
    "_**Reduplicate whole signal:**_\n",
    "\n",
    "*Language:*\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> abab\n",
    "\n",
    "12 --> baba\n",
    "\n",
    "13 --> bbbb\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S --> ABAB\n",
    "\n",
    "A:0 --> a\n",
    "\n",
    "A:1 --> b\n",
    "\n",
    "B:2 --> a\n",
    "\n",
    "B:3 --> b\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "SABAB.A0a.A1b.B2a.B3b\n",
    "\n",
    "\n",
    "_**Reduplicate each segment:**_\n",
    "\n",
    "*Language:*\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> aabb\n",
    "\n",
    "12 --> bbaa\n",
    "\n",
    "13 --> bbbb\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S --> AABB\n",
    "\n",
    "A:0 --> a\n",
    "\n",
    "A:1 --> b\n",
    "\n",
    "B:2 --> a\n",
    "\n",
    "B:3 --> b\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "SAABB.A0a.A1b.B2a.B3b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Diversify signal + holistic:**\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> bbbb\n",
    "\n",
    "12 --> abba\n",
    "\n",
    "13 --> baab\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S:02 --> aaaa\n",
    "\n",
    "S:03 --> bbbb\n",
    "\n",
    "S:12 --> abba\n",
    "\n",
    "S:13 --> baab\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "S02aaaa.S03bbbb.S12abba.S13baab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Repair + compositional:**\n",
    "\n",
    "02 --> aa\n",
    "\n",
    "03 --> ab\n",
    "\n",
    "12 --> ba\n",
    "\n",
    "13 --> bb\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S --> AB\n",
    "\n",
    "A:0 --> a\n",
    "\n",
    "A:1 --> b\n",
    "\n",
    "B:2 --> a\n",
    "\n",
    "B:3 --> b\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "SAB.A0a.A1b.B2a.B3b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's calculate the actual compressibility in terms of coding length, given the strings in minimally redundant form specified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T12:10:30.669381Z",
     "start_time": "2019-11-15T12:10:30.660703Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_language_four_forms(lang, forms, meaning_list):\n",
    "    \"\"\"\n",
    "    Classify one particular language as either 0 = degenerate, 1 = holistic, 2 = hybrid, 3 = compositional, 4 = other\n",
    "    (Kirby et al., 2015). NOTE that this function is specific to classifying languages that consist of exactly 4 forms,\n",
    "    where each form consists of exactly 2 characters. For a more general version of this function, see\n",
    "    classify_language_general() below.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param forms: list of strings corresponding to all possible forms_without_noisy_variants\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :returns: integer corresponding to category that language belongs to:\n",
    "    0 = degenerate, 1 = holistic, 2 = hybrid, 3 = compositional, 4 = other (here I'm following the\n",
    "    ordering used in the Kirby et al., 2015 paper; NOT the ordering from SimLang lab 21)\n",
    "    \"\"\"\n",
    "    class_degenerate = 0\n",
    "    class_holistic = 1\n",
    "    class_hybrid = 2  # this is a hybrid between a holistic and a compositional language; where *half* of the partial\n",
    "    # forms is mapped consistently to partial meanings (instead of that being the case for *all* partial forms)\n",
    "    class_compositional = 3\n",
    "    class_other = 4\n",
    "\n",
    "    # First check whether some conditions are met, bc this function hasn't been coded up in the most general way yet:\n",
    "    if len(forms) != 4:\n",
    "        raise ValueError(\n",
    "            \"This function only works for a world in which there are 4 possible forms_without_noisy_variants\"\n",
    "        )\n",
    "    if len(forms[0]) != 2:\n",
    "        raise ValueError(\n",
    "            \"This function only works when each form consists of 2 elements\")\n",
    "    if len(lang) != len(meaning_list):\n",
    "        raise ValueError(\"Lang should have same length as meanings\")\n",
    "\n",
    "    # lang is degenerate if it uses the same form for every meaning:\n",
    "    if lang[0] == lang[1] and lang[1] == lang[2] and lang[2] == lang[3]:\n",
    "        return class_degenerate\n",
    "\n",
    "    # lang is compositional if it makes use of all possible forms_without_noisy_variants, *and* each form element maps\n",
    "    # to the same meaning element for each form:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[\n",
    "        3] in lang and lang[0][0] == lang[1][0] and lang[2][0] == lang[3][0] and lang[0][\n",
    "        1] == lang[2][1] and lang[1][1] == lang[3][1]:\n",
    "        return class_compositional\n",
    "\n",
    "    # lang is holistic if it is *not* compositional, but *does* make use of all possible forms_without_noisy_variants:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[3] in lang:\n",
    "        # within holistic languages, we can distinguish between those in which at least one part form is mapped\n",
    "        # consistently onto one part meaning. This class we will call 'hybrid' (because for the purposes of repair, it\n",
    "        # is a hybrid between a holistic and a compositional language, because for half of the possible noisy forms that\n",
    "        # a listener could receive it allows the listener to figure out *part* of the meaning, and therefore use a\n",
    "        # restricted request for repair instead of an open request.\n",
    "        if lang[0][0] == lang[1][0] and lang[2][0] == lang[3][0]:\n",
    "            return class_hybrid\n",
    "        elif lang[0][1] == lang[2][1] and lang[1][1] == lang[3][1]:\n",
    "            return class_hybrid\n",
    "        else:\n",
    "            return class_holistic\n",
    "\n",
    "    # In all other cases, a language belongs to the 'other' category:\n",
    "    else:\n",
    "        return class_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T12:11:01.053927Z",
     "start_time": "2019-11-15T12:11:01.032438Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import string\n",
    "\n",
    "\n",
    "def mrf_degenerate(lang, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a degenerate language and returns a minimally redundant form description of the language's context free\n",
    "    grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    mrf_string = 'S'\n",
    "    for i in range(len(meaning_list)):\n",
    "        meaning = meaning_list[i]\n",
    "        if i != len(meaning_list) - 1:\n",
    "            mrf_string += str(meaning) + ','\n",
    "        else:\n",
    "            mrf_string += str(meaning)\n",
    "    mrf_string += lang[0]\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def mrf_holistic(lang, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a holistic OR hybrid language and returns a minimally redundant form description of the language's context\n",
    "    free grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    mrf_string = ''\n",
    "    for i in range(len(meaning_list)):\n",
    "        meaning = meaning_list[i]\n",
    "        form = lang[i]\n",
    "        if i != len(meaning_list) - 1:\n",
    "            mrf_string += 'S' + meaning + form + '.'\n",
    "        else:\n",
    "            mrf_string += 'S' + meaning + form\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def mrf_compositional(lang, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a compositional language and returns a minimally redundant form description of the language's context free\n",
    "    grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    n_features = len(meaning_list[0])\n",
    "    non_terminals = string.ascii_uppercase[:n_features]\n",
    "    mrf_string = 'S' + non_terminals\n",
    "    for i in range(len(non_terminals)):\n",
    "        non_terminal_symbol = non_terminals[i]\n",
    "        feature_values = []\n",
    "        feature_value_segments = []\n",
    "        for j in range(len(meaning_list)):\n",
    "            if meaning_list[j][i] not in feature_values:\n",
    "                feature_values.append(meaning_list[j][i])\n",
    "                feature_value_segments.append(lang[j][i])\n",
    "        for k in range(len(feature_values)):\n",
    "            value = feature_values[k]\n",
    "            segment = feature_value_segments[k]\n",
    "            mrf_string += \".\" + non_terminal_symbol + value + segment\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def mrf_other(lang, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a language of the 'other' category and returns a minimally redundant form description of the language's\n",
    "    context free grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    mapping_dict = {}\n",
    "    for i in range(len(lang)):\n",
    "        mapping_dict.setdefault(lang[i], []).append(meaning_list[i])\n",
    "    mrf_string = 'S'\n",
    "    counter = 0\n",
    "    for form in mapping_dict.keys():\n",
    "        for k in range(len(mapping_dict[form])):\n",
    "            meaning = mapping_dict[form][k]\n",
    "            if k != len(mapping_dict[form]) - 1:\n",
    "                mrf_string += meaning + ','\n",
    "            else:\n",
    "                mrf_string += meaning\n",
    "        if counter != len(mapping_dict.keys()) - 1:\n",
    "            mrf_string += form + '.S'\n",
    "        else:\n",
    "            mrf_string += form\n",
    "        counter += 1\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def minimally_redundant_form(lang, complete_forms, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a language of any class and returns a minimally redundant form description of its context free grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param complete_forms: list containing all possible complete forms; corresponds to global variable\n",
    "    'forms_without_noise'\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    lang_class = classify_language_four_forms(lang, complete_forms, meaning_list)\n",
    "    if lang_class == 0:  # the language is 'degenerate'\n",
    "        mrf_string = mrf_degenerate(lang, meaning_list)\n",
    "    elif lang_class == 1 or lang_class == 2:  # the language is 'holistic' or 'hybrid'\n",
    "        mrf_string = mrf_holistic(lang, meaning_list)\n",
    "    elif lang_class == 3:  # the language is 'compositional'\n",
    "        mrf_string = mrf_compositional(lang, meaning_list)\n",
    "    elif lang_class == 4:  # the language is of the 'other' category\n",
    "        mrf_string = mrf_other(lang, meaning_list)\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def character_probs(mrf_string):\n",
    "    \"\"\"\n",
    "    Takes a string in minimally redundant form and generates a dictionary specifying the probability of each of the\n",
    "    symbols used in the string\n",
    "\n",
    "    :param mrf_string: a string in minimally redundant form\n",
    "    :return: a dictionary with the symbols as keys and their corresponding probabilities as values\n",
    "    \"\"\"\n",
    "    count_dict = {}\n",
    "    for character in mrf_string:\n",
    "        if character in count_dict.keys():\n",
    "            count_dict[character] += 1\n",
    "        else:\n",
    "            count_dict[character] = 1\n",
    "    prob_dict = {}\n",
    "    for character in count_dict.keys():\n",
    "        char_prob = count_dict[character] / len(mrf_string)\n",
    "        prob_dict[character] = char_prob\n",
    "    return prob_dict\n",
    "\n",
    "\n",
    "def coding_length(mrf_string):\n",
    "    \"\"\"\n",
    "    Takes a string in minimally redundant form and returns its coding length in bits\n",
    "\n",
    "    :param mrf_string: a string in minimally redundant form\n",
    "    :return: coding length in bits\n",
    "    \"\"\"\n",
    "    char_prob_dict = character_probs(mrf_string)\n",
    "    coding_len = 0\n",
    "    for character in mrf_string:\n",
    "        coding_len += log2(char_prob_dict[character])\n",
    "    return -coding_len\n",
    "\n",
    "\n",
    "def prior_single_lang(lang, complete_forms, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a language and returns its PROPORTIONAL prior probability; this still needs to be normalized over all\n",
    "    languages in order to give the real prior probability.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param complete_forms: list containing all possible complete forms; corresponds to global variable\n",
    "    'forms_without_noise'\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: PROPORTIONAL prior probability (float)\n",
    "    \"\"\"\n",
    "    mrf_string = minimally_redundant_form(lang, complete_forms, meaning_list)\n",
    "    coding_len = coding_length(mrf_string)\n",
    "    prior = 2 ** -coding_len\n",
    "    return prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T12:11:01.818916Z",
     "start_time": "2019-11-15T12:11:01.808517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "lang_class_text is:\n",
      "degenerate\n",
      "lang is:\n",
      "['aa', 'aa', 'aa', 'aa']\n",
      "mrf_string is:\n",
      "S02,03,12,13aa\n",
      "coding_len is:\n",
      "38.55\n",
      "prior this lang is:\n",
      "2.488119421993922e-12\n",
      "\n",
      "1\n",
      "lang_class_text is:\n",
      "degenerate\n",
      "lang is:\n",
      "['ab', 'ab', 'ab', 'ab']\n",
      "mrf_string is:\n",
      "S02,03,12,13ab\n",
      "coding_len is:\n",
      "40.55\n",
      "prior this lang is:\n",
      "6.220298554984805e-13\n",
      "\n",
      "2\n",
      "lang_class_text is:\n",
      "other\n",
      "lang is:\n",
      "['aa', 'aa', 'aa', 'ab']\n",
      "mrf_string is:\n",
      "S02,03,12aa.S13ab\n",
      "coding_len is:\n",
      "52.73\n",
      "prior this lang is:\n",
      "1.3368788379305763e-16\n",
      "\n",
      "3\n",
      "lang_class_text is:\n",
      "other\n",
      "lang is:\n",
      "['aa', 'aa', 'aa', 'bb']\n",
      "mrf_string is:\n",
      "S02,03,12aa.S13bb\n",
      "coding_len is:\n",
      "53.49\n",
      "prior this lang is:\n",
      "7.922244965514519e-17\n",
      "\n",
      "4\n",
      "lang_class_text is:\n",
      "compositional\n",
      "lang is:\n",
      "['aa', 'ab', 'ba', 'bb']\n",
      "mrf_string is:\n",
      "SAB.A0a.A1b.B2a.B3b\n",
      "coding_len is:\n",
      "59.2\n",
      "prior this lang is:\n",
      "1.5092773625944422e-18\n",
      "\n",
      "5\n",
      "lang_class_text is:\n",
      "other\n",
      "lang is:\n",
      "['aa', 'aa', 'ab', 'ba']\n",
      "mrf_string is:\n",
      "S02,03aa.S12ab.S13ba\n",
      "coding_len is:\n",
      "61.68\n",
      "prior this lang is:\n",
      "2.700000000000012e-19\n",
      "\n",
      "6\n",
      "lang_class_text is:\n",
      "other\n",
      "lang is:\n",
      "['aa', 'aa', 'ab', 'bb']\n",
      "mrf_string is:\n",
      "S02,03aa.S12ab.S13bb\n",
      "coding_len is:\n",
      "62.17\n",
      "prior this lang is:\n",
      "1.9221679687499936e-19\n",
      "\n",
      "7\n",
      "lang_class_text is:\n",
      "hybrid\n",
      "lang is:\n",
      "['aa', 'ab', 'bb', 'ba']\n",
      "mrf_string is:\n",
      "S02aa.S03ab.S12bb.S13ba\n",
      "coding_len is:\n",
      "67.29\n",
      "prior this lang is:\n",
      "5.553712540966203e-21\n"
     ]
    }
   ],
   "source": [
    "# First, let's check whether the functions defined above work correctly\n",
    "# for the example languages given in Kirby et al. (2015):\n",
    "\n",
    "\n",
    "## First some parameter settings:\n",
    "meanings = ['02', '03', '12', '13']\n",
    "forms_without_noisy_variants = ['aa', 'ab', 'ba', 'bb']\n",
    "\n",
    "\n",
    "## Then let's specify the example languages from Kirby et al. (2015)\n",
    "example_languages = [['aa', 'aa', 'aa', 'aa'],\n",
    "                     ['ab', 'ab', 'ab', 'ab'],\n",
    "                     ['aa', 'aa', 'aa', 'ab'],\n",
    "                     ['aa', 'aa', 'aa', 'bb'],\n",
    "                     ['aa', 'ab', 'ba', 'bb'],\n",
    "                     ['aa', 'aa', 'ab', 'ba'],\n",
    "                     ['aa', 'aa', 'ab', 'bb'],\n",
    "                     ['aa', 'ab', 'bb', 'ba']]\n",
    "\n",
    "\n",
    "## And now let's calculate their coding lengths:\n",
    "lang_classes_text = ['degenerate', 'holistic', 'hybrid', 'compositional', 'other']\n",
    "for i in range(len(example_languages)):\n",
    "    lang = example_languages[i]\n",
    "    print('')\n",
    "    print(i)\n",
    "    lang_class = classify_language_four_forms(lang, forms_without_noisy_variants, meanings)\n",
    "    lang_class_text = lang_classes_text[lang_class]\n",
    "    print(\"lang_class_text is:\")\n",
    "    print(lang_class_text)\n",
    "    print(\"lang is:\")\n",
    "    print(lang)\n",
    "    mrf_string = minimally_redundant_form(lang, forms_without_noisy_variants, meanings)\n",
    "    print(\"mrf_string is:\")\n",
    "    print(mrf_string)\n",
    "    coding_len = coding_length(mrf_string)\n",
    "    print(\"coding_len is:\")\n",
    "    print(round(coding_len, ndigits=2))\n",
    "    prior = prior_single_lang(lang, forms_without_noisy_variants, meanings)\n",
    "    print(\"prior this lang is:\")\n",
    "    print(prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T11:52:07.783812Z",
     "start_time": "2019-11-15T11:52:07.776761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coding_len_compositional_reduplicate_whole_signal is:\n",
      "64.24\n",
      "\n",
      "coding_len_compositional_reduplicate_segments is:\n",
      "64.24\n",
      "\n",
      "coding_len_holistic_diversify_signal is:\n",
      "84.83\n",
      "\n",
      "coding_len_compositional_repair is:\n",
      "59.2\n",
      "\n",
      "ratio_reduplication_vs_repair is:\n",
      "1.09\n",
      "\n",
      "ratio_diversify_signal_vs_repair is:\n",
      "1.43\n"
     ]
    }
   ],
   "source": [
    "# These are the same as in the table in Kirby et al. (2015, p. 92),\n",
    "# except for the coding length of the compositional language. In \n",
    "# Kirby et al. (2015) it says 55.20, when I get 59.20. But given that\n",
    "# all the other values I get are the same as in the table, I suspect\n",
    "# this is a typo in the actual paper.\n",
    "# So now that we know that these functions are coded up correctly,\n",
    "# let's have a look at the coding lengths for our example languages\n",
    "# for Possibility 1: different form lengths\n",
    "\n",
    "mrf_compositional_reduplicate_whole_signal = \"SABAB.A0a.A1b.B2a.B3b\"\n",
    "coding_len_compositional_reduplicate_whole_signal = coding_length(mrf_compositional_reduplicate_whole_signal)\n",
    "print(\"coding_len_compositional_reduplicate_whole_signal is:\")\n",
    "print(round(coding_len_compositional_reduplicate_whole_signal, ndigits=2))\n",
    "\n",
    "mrf_compositional_reduplicate_segments = \"SAABB.A0a.A1b.B2a.B3b\"\n",
    "coding_len_compositional_reduplicate_segments = coding_length(mrf_compositional_reduplicate_segments)\n",
    "print('')\n",
    "print(\"coding_len_compositional_reduplicate_segments is:\")\n",
    "print(round(coding_len_compositional_reduplicate_segments, ndigits=2))\n",
    "\n",
    "mrf_holistic_diversify_signal = \"S02aaaa.S03bbbb.S12abba.S13baab\"\n",
    "coding_len_holistic_diversify_signal = coding_length(mrf_holistic_diversify_signal)\n",
    "print('')\n",
    "print(\"coding_len_holistic_diversify_signal is:\")\n",
    "print(round(coding_len_holistic_diversify_signal, ndigits=2))\n",
    "\n",
    "mrf_compositional_repair = \"SAB.A0a.A1b.B2a.B3b\"\n",
    "coding_len_compositional_repair = coding_length(mrf_compositional_repair)\n",
    "print('')\n",
    "print(\"coding_len_compositional_repair is:\")\n",
    "print(round(coding_len_compositional_repair, ndigits=2))\n",
    "\n",
    "\n",
    "ratio_reduplication_vs_repair = coding_len_compositional_reduplicate_whole_signal/coding_len_compositional_repair\n",
    "print('')\n",
    "print(\"ratio_reduplication_vs_repair is:\")\n",
    "print(round(ratio_reduplication_vs_repair, ndigits=2))\n",
    "\n",
    "ratio_diversify_signal_vs_repair = coding_len_holistic_diversify_signal/coding_len_compositional_repair\n",
    "print('')\n",
    "print(\"ratio_diversify_signal_vs_repair is:\")\n",
    "print(round(ratio_diversify_signal_vs_repair, ndigits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so as we can see from the coding lengths above, possibility 1 of how to represent languages gives relative coding lengths that capture the intuitions we have about how hard it is to learn these different languages: the compositional languages with reduplication only have slightly longer coding lengths than the compositional language without it (ratio reduplication:repair = 1.09:1), whereas the holistic language resulting from the diversify signal strategy has a significantly longer coding length (ratio diversify_signal:repair = 1.43:1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion about how to represent languages:\n",
    "\n",
    "Based on all the above, I'd say let's go for possibility 1 of allowing for different form lengths. That keeps our way of representing languages as close as possibility to the one used by Kirby et al. (2015); it allows us to straightforwardly calculate the coding lengths; and it will not cause languages to make use of a different number of characters, as possibility 2 would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Burkett, D., & Griffiths, T. L. (2010). Iterated learning of multiple languages from multiple teachers. The Evolution of Language: Proceedings of the 8th International Conference (EVOLANG8), Utrecht, Netherlands, 14-17 April 2010, 58–65.\n",
    "\n",
    "Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87–102. https://doi.org/10.1016/j.cognition.2015.03.016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
