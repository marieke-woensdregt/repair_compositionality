{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different strategies for dealing with noise\n",
    "\n",
    "\n",
    "\n",
    "The idea here is that there are different possible strategies of dealing with noise that are attested in natural communication systems:\n",
    "\n",
    "* **Reduplication**: simply repeat the signal several times. This is compatible with a compositional system. It is not costly in terms of learnability, because the only extra thing that needs to be learned is a single extra rule that applies to all signals. However, it is relatively costly in terms of utterance length (it would thus not do well under a pressure for minimal effort). \n",
    "* **Diversify signal**: make the individual segments that each signal consists of as distinct as possible. For example, in the language shown below, all four signals can be distinguished from each other in all cases where one character is obscured by noise. This strategy however is not compatible with compositionality, because it relies on making each of the segments as distinct from each other as possible. That means these languages are necessarily holistic, and therefore less easy to learn (so they would do less well under a pressure for learnability). Below is an example of a language for which, if noise obscures a single character, each signal would still be uniquely identifiable under noise.\n",
    "    - 02 --> 'aaaa'\n",
    "    - 03 --> 'bbbb'\n",
    "    - 12 --> 'abba'\n",
    "    - 13 --> 'baab'\n",
    "* **Repair**: This strategy could be seen as a form of redundancy across turns, instead of within a signal. However, it will be initiated only when neccesary, and should therefore fare slightly better than the reduplication strategy under a pressure for minimal effort (where effort is measured as total shared utterance length of both interlocutors across a set number of interactions).\n",
    "\n",
    "The predictions of Vinicius & Seán (2016 Evolang abstract titled \"Language adapts to signal disruption in interaction\"), are that although the reduplication strategy and the repair strategy should do equally well under a pressure for learnability, adding the possibility of repair will 'lift the pressure for redundancy', such that receivers can request that speakers repeat a signal only after a problem occurs.\n",
    "---> However, we would add that in the absence of a pressure for minimal effort, the repair strategy does not have an advantage over the reduplication strategy. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions under different selection pressures:\n",
    "\n",
    "We predict that under the following assumptions:\n",
    "- There is a pressure for expressivity/mutual understanding (or rather: a pressure to get ones signal/message across; which feels like a better way to describe the pressure that frogs and song birds are under)\n",
    "- Noise regularly disrupts part of the signal (Vinicius & Seán used a 0.5 probability in their experiment)\n",
    "- Repair is a possibility\n",
    "\n",
    "\n",
    "the following strategies will become dominant under the following combinations of the presense/absence of a pressure for learnability and a pressure for minimal effort:\n",
    "\n",
    "|                | - minimal effort                                              | + minimal effort          |\n",
    "|----------------|---------------------------------------------------------------|---------------------------|\n",
    "| **- learnability** | Any of the three strategies above will do                                         | Repair + Compositional OR Holistic         |\n",
    "| **+ learnability** | Reduplication + Compositional OR Repair + Compositional | Repair + Compositional |\n",
    "\n",
    "<span class=\"mark\">Note</span> that the prediction in the {-learnability, +minimal effort} condition above only holds if we do not distinguish between open and closed requests. Because if we do, as in the model we submitted to evolang, we'd expect the Repair + compositional strategy to fare best in this condition, without the need for a pressure for learnability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to represent languages?\n",
    "\n",
    "### Possibility 1: Different form lengths\n",
    "\n",
    "If we continue with Kirby et al.'s (2015) way of representing meanings and forms (which is a minimal way of creating languages that we can classify as compositional, holistic or degenerate), where meanings consist of $f=2$ features, which can each have $v=2$ values, we can allow for each of the language strategies specified above ('reduplication' and 'diversify signal'), by simply allowing for multiple string lengths $l$, while keeping the alphabet size $|\\Sigma|$ at 2.\n",
    "\n",
    "For example, where Kirby et al. (2015) only allowed for a single possible string length, and specified $f = v = l = |\\Sigma| = 2$, we could minimally allow for two possible string lengths: one being equal to $f$ (i.e. the minimum string length required to uniquely specify each meaning feature), and one being equal to $2*f$, to enable reduplication of the signal.\n",
    "\n",
    "That would yield the following types of languages:\n",
    "\n",
    "\n",
    "**Reduplication + compositional:**\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> abab\n",
    "\n",
    "12 --> baba\n",
    "\n",
    "13 --> bbbb\n",
    "\n",
    "\n",
    "**Diversify signal + holistic:**\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> bbbb\n",
    "\n",
    "12 --> abba\n",
    "\n",
    "13 --> baab\n",
    "\n",
    "\n",
    "**Repair + compositional:**\n",
    "\n",
    "02 --> aa\n",
    "\n",
    "03 --> ab\n",
    "\n",
    "12 --> ba\n",
    "\n",
    "13 --> bb\n",
    "\n",
    "\n",
    "In order to still make it possible for iterated learning chains to transition from a language that uses forms of length 2 into a language that uses forms of length 4 and vice versa, we need to then also allow for languages that use a mixture of form lengths (e.g. three forms of length 2, and one form of length 4). This yields the following number of possible languages:\n",
    "\n",
    "$$ (2^2+2^4)^4 = 160000$$\n",
    "\n",
    "which means that compared to the Kirby et al. (2015) model (where there were ($(2^2)^4 = 256$ possible languages), the hypothesis space expands by a factor of 625. That is not ideal, because if we assume that simulation run times increase linearly with the size of the hypothesis space, a simulation that took 1 hour to run in our previous model would now take almost 4 weeks to run.\n",
    "\n",
    "However, this linear relationship between the simulation run times and the size of the hypothesis space holds when during learning, we actually loop through each hypothesis and update its posterior probability based on the data. There are a couple of ways in which this process can be optimised:\n",
    "\n",
    "1. **memoisation**: This would require enumerating all possible data points (i.e. <meaning, form> pairs) (including all possible noisy forms), and for each of them calculating its likelihood for all possible hypotheses **once**, and caching the result. Whenever the same <meaning, form> pair is then encountered by any learner, the corresponding likelihood vector is then simply retrieved from memory and multiplied with the learner's current posterior. This should be doable given that the total number of meanings is 4, and the total number of forms (including all possible noisy variants, assuming that noise is restricted to a single character) is 56; which makes 4\\*56 = 224 possible <meaning, form> pairs. For each of those 224 possible datapoints, we would then calculate its likelihood for all 160,000 hypotheses, and cache these values in a 224\\*160,000 matrix. (That matrix thus has 224\\*160,000 = 35,840,000 entries.)\n",
    "2. Intergenerational learning could be sped up by representing data as simple counts of <meaning, form> pairs, and simply updating the posterior probability distribution for the full data set in one step, by multiplying the prior of the hypothesis with the likelihood of the <meaning, form> pair to the power of the number of times it occurs in the data set. This should speed things up a little in intergenerational learning, but won't make a difference in *intra*generational learning, because there we assume that the hearer updates their posterior in each interaction. \n",
    "3. Do not do exact inference over the full hypothesis space at all, but instead use an MCMC sampling technique (e.g. Burkett & Griffiths, 2010, and Kirby et al., 2015 use Gibbs sampling) --> This would require a bit more time to figure out, and is hopefully not necessary once optimisations 1 and 2 above have been implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possibility 2: Allow reduplication as grammatical rule, and increase alphabet size\n",
    "\n",
    "If instead of allowing for multiple form lengths, we instead increase the size of the alphabet $\\Sigma$ from 2 to 4, that will make the diversify signal strategy possible. More concretely, that would mean that instead of there being an alphabet $[a, b]$, there would be an alphabet $[a, b, c, d]$. \n",
    "That would allow for the following example languages, where the bit at the end of the signal specifies whether the signal should be repeated (1) or not (0).\n",
    "\n",
    "\n",
    "**Reduplication + compositional:**\n",
    "\n",
    "02 --> aa1\n",
    "\n",
    "03 --> ab1\n",
    "\n",
    "12 --> ba1\n",
    "\n",
    "13 --> bb1\n",
    "\n",
    "\n",
    "**Diversify signal + holistic:**\n",
    "\n",
    "02 --> aa0\n",
    "\n",
    "03 --> bb0\n",
    "\n",
    "12 --> cc0\n",
    "\n",
    "13 --> dd0\n",
    "\n",
    "\n",
    "**Repair + compositional:**\n",
    "\n",
    "02 --> aa0\n",
    "\n",
    "03 --> ab0\n",
    "\n",
    "12 --> ba0\n",
    "\n",
    "13 --> bb0\n",
    "\n",
    "\n",
    "Choosing for this option would mean that instead of there being $(2ˆ2 + 2ˆ4) = 20$ possible forms, there would be $4ˆ2 = 16$ possible forms, and therefore $(4^2)^4 = 65536$ possible languages. In addition however, we'd need languages to have an extra bit that specifies whether signals are reduplicated or not (assuming there are only two options: reduplication ON versus reduplication OFF). That means that there'd be a total of $((4^2)^4)*2 = 131072$. So compared to possibility 1, possibility 2 only reduces the size of the hypothesis space by a factor of 1.22. That is not much, but an added advantage of this way of representing languages is that it allows for a straightforward way of capturing the assumed simplicity of a reduplication rule in the coding of the languages, and therefore into the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we find that despite the optimisation strategies outlined above it is still not feasible to run simulations within a reasonable time-frame, we could consider tackling the different possible strategies for dealing with noise separately. I.e. one model where we allow for the possibility to add reduplication to signals vs. repair, and another model where we allow for diversification of signal segments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressibility measure for two different language representation possibilities\n",
    "\n",
    "\n",
    "### Possibility 1: Different form lengths\n",
    "\n",
    "#### Rewrite rules:\n",
    "\n",
    "**Reduplication + compositional:**\n",
    "\n",
    "There are in fact two different ways of reduplicating a compositional language: either reduplicating the whole signal, or reduplicating each of the segments. In both cases the length of the minimally redundant form, and therefore the language type's compressibility, will be the same however, as shown below.\n",
    "\n",
    "_**Reduplicate whole signal:**_\n",
    "\n",
    "*Language:*\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> abab\n",
    "\n",
    "12 --> baba\n",
    "\n",
    "13 --> bbbb\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S --> ABAB\n",
    "\n",
    "A:0 --> a\n",
    "\n",
    "A:1 --> b\n",
    "\n",
    "B:2 --> a\n",
    "\n",
    "B:3 --> b\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "SABAB.A0a.A1b.B2a.B3b\n",
    "\n",
    "\n",
    "_**Reduplicate each segment:**_\n",
    "\n",
    "*Language:*\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> aabb\n",
    "\n",
    "12 --> bbaa\n",
    "\n",
    "13 --> bbbb\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S --> AABB\n",
    "\n",
    "A:0 --> a\n",
    "\n",
    "A:1 --> b\n",
    "\n",
    "B:2 --> a\n",
    "\n",
    "B:3 --> b\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "SAABB.A0a.A1b.B2a.B3b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Diversify signal + holistic:**\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> bbbb\n",
    "\n",
    "12 --> abba\n",
    "\n",
    "13 --> baab\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S:02 --> aaaa\n",
    "\n",
    "S:03 --> bbbb\n",
    "\n",
    "S:12 --> abba\n",
    "\n",
    "S:13 --> baab\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "S02aaaa.S03bbbb.S12abba.S13baab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Repair + compositional:**\n",
    "\n",
    "02 --> aa\n",
    "\n",
    "03 --> ab\n",
    "\n",
    "12 --> ba\n",
    "\n",
    "13 --> bb\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S --> AB\n",
    "\n",
    "A:0 --> a\n",
    "\n",
    "A:1 --> b\n",
    "\n",
    "B:2 --> a\n",
    "\n",
    "B:3 --> b\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "SAB.A0a.A1b.B2a.B3b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's calculate the actual compressibility in terms of coding length, given the strings in minimally redundant form specified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T11:07:12.716551Z",
     "start_time": "2019-11-14T11:07:12.704433Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_language_four_forms(lang, forms, meaning_list):\n",
    "    \"\"\"\n",
    "    Classify one particular language as either 0 = degenerate, 1 = holistic, 2 = hybrid, 3 = compositional, 4 = other\n",
    "    (Kirby et al., 2015). NOTE that this function is specific to classifying languages that consist of exactly 4 forms,\n",
    "    where each form consists of exactly 2 characters. For a more general version of this function, see\n",
    "    classify_language_general() below.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param forms: list of strings corresponding to all possible forms_without_noisy_variants\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :returns: integer corresponding to category that language belongs to:\n",
    "    0 = degenerate, 1 = holistic, 2 = hybrid, 3 = compositional, 4 = other (here I'm following the\n",
    "    ordering used in the Kirby et al., 2015 paper; NOT the ordering from SimLang lab 21)\n",
    "    \"\"\"\n",
    "    class_degenerate = 0\n",
    "    class_holistic = 1\n",
    "    class_hybrid = 2  # this is a hybrid between a holistic and a compositional language; where *half* of the partial\n",
    "    # forms is mapped consistently to partial meanings (instead of that being the case for *all* partial forms)\n",
    "    class_compositional = 3\n",
    "    class_other = 4\n",
    "\n",
    "    # First check whether some conditions are met, bc this function hasn't been coded up in the most general way yet:\n",
    "    if len(forms) != 4:\n",
    "        raise ValueError(\n",
    "            \"This function only works for a world in which there are 4 possible forms_without_noisy_variants\"\n",
    "        )\n",
    "    if len(forms[0]) != 2:\n",
    "        raise ValueError(\n",
    "            \"This function only works when each form consists of 2 elements\")\n",
    "    if len(lang) != len(meaning_list):\n",
    "        raise ValueError(\"Lang should have same length as meanings\")\n",
    "\n",
    "    # lang is degenerate if it uses the same form for every meaning:\n",
    "    if lang[0] == lang[1] and lang[1] == lang[2] and lang[2] == lang[3]:\n",
    "        return class_degenerate\n",
    "\n",
    "    # lang is compositional if it makes use of all possible forms_without_noisy_variants, *and* each form element maps\n",
    "    # to the same meaning element for each form:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[\n",
    "        3] in lang and lang[0][0] == lang[1][0] and lang[2][0] == lang[3][0] and lang[0][\n",
    "        1] == lang[2][1] and lang[1][1] == lang[3][1]:\n",
    "        return class_compositional\n",
    "\n",
    "    # lang is holistic if it is *not* compositional, but *does* make use of all possible forms_without_noisy_variants:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[3] in lang:\n",
    "        # within holistic languages, we can distinguish between those in which at least one part form is mapped\n",
    "        # consistently onto one part meaning. This class we will call 'hybrid' (because for the purposes of repair, it\n",
    "        # is a hybrid between a holistic and a compositional language, because for half of the possible noisy forms that\n",
    "        # a listener could receive it allows the listener to figure out *part* of the meaning, and therefore use a\n",
    "        # restricted request for repair instead of an open request.\n",
    "        if lang[0][0] == lang[1][0] and lang[2][0] == lang[3][0]:\n",
    "            return class_hybrid\n",
    "        elif lang[0][1] == lang[2][1] and lang[1][1] == lang[3][1]:\n",
    "            return class_hybrid\n",
    "        else:\n",
    "            return class_holistic\n",
    "\n",
    "    # In all other cases, a language belongs to the 'other' category:\n",
    "    else:\n",
    "        return class_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T13:33:26.106359Z",
     "start_time": "2019-11-14T13:33:26.084149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrf_string_degenerate is:\n",
      "S02,03,12,13aa\n",
      "\n",
      "n_features\n",
      "2\n",
      "categories are:\n",
      "AB\n",
      "mrf_string before for-loop is:\n",
      "SAB\n",
      "\n",
      "i is:\n",
      "0\n",
      "category is:\n",
      "A\n",
      "category_feature_values are:\n",
      "['0', '1']\n",
      "feature_value_segments are:\n",
      "['a', 'b']\n",
      "\n",
      "i is:\n",
      "1\n",
      "category is:\n",
      "B\n",
      "category_feature_values are:\n",
      "['2', '3']\n",
      "feature_value_segments are:\n",
      "['a', 'b']\n",
      "\n",
      "mrf_string_compositional is:\n",
      "SAB.A0a.A1b.B2a.B3b\n",
      "\n",
      "mrf_string_holistic is:\n",
      "S02aa.S03ab.S12bb.S13ba\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "from string import ascii_uppercase\n",
    "\n",
    "\n",
    "def mrf_degenerate(lang, meaning_list):\n",
    "    mrf_string = 'S'\n",
    "    for i in range(len(meaning_list)):\n",
    "        meaning = meaning_list[i]\n",
    "        if i != len(meaning_list)-1:\n",
    "            mrf_string += str(meaning)+','\n",
    "        else:\n",
    "            mrf_string += str(meaning)\n",
    "    mrf_string += lang[0]\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def mrf_holistic(lang, meaning_list):\n",
    "    mrf_string = ''\n",
    "    for i in range(len(meaning_list)):\n",
    "        meaning = meaning_list[i]\n",
    "        form = lang[i]\n",
    "        if i != len(meaning_list)-1:\n",
    "            mrf_string += 'S'+meaning+form+'.'\n",
    "        else:\n",
    "            mrf_string += 'S'+meaning+form\n",
    "    return mrf_string\n",
    "    \n",
    "\n",
    "def mrf_compositional(lang, meaning_list):\n",
    "    n_features = len(meaning_list[0])\n",
    "    categories = ascii_uppercase[:n_features]\n",
    "    mrf_string = 'S'+categories\n",
    "    for i in range(len(categories)):\n",
    "        category = categories[i]\n",
    "        category_feature_values = []\n",
    "        feature_value_segments = []\n",
    "        for j in range(len(meaning_list)):\n",
    "            if meaning_list[j][i] not in category_feature_values:\n",
    "                category_feature_values.append(meaning_list[j][i])\n",
    "                feature_value_segments.append(lang[j][i])\n",
    "        for k in range(len(category_feature_values)):\n",
    "            value = category_feature_values[k]\n",
    "            segment = feature_value_segments[k]\n",
    "            mrf_string += \".\"+category+value+segment\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def minimally_redundant_form(lang, forms, meaning_list):\n",
    "    lang_class = classify_language_four_forms(lang, forms, meaning_list) # 0 = degenerate, 1 = holistic, 2 = hybrid, 3 = compositional, 4 = other\n",
    "    if lang_class == 0: # the language is 'degenerate'\n",
    "        mrf_string = mrf_degenerate(lang, meaning_list)\n",
    "    elif lang_class == 1 or lang_class == 2: # the language is 'holistic' or 'hybrid'\n",
    "        mrf_string = mrf_holistic(lang, meaning_list)\n",
    "    elif lang_class == 3: # the language is 'compositional'\n",
    "        mrf_string = mrf_compositional(lang, meaning_list)\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def character_probs(mrf_string):\n",
    "    count_dict = {}\n",
    "    for character in mrf_string:\n",
    "        if character in count_dict.keys():\n",
    "            count_dict[character] += 1\n",
    "        else:\n",
    "            count_dict[character] = 1       \n",
    "    prob_dict = {}\n",
    "    for character in count_dict.keys():\n",
    "        char_prob = count_dict[character]/len(mrf_string)\n",
    "        prob_dict[character] = char_prob\n",
    "    return prob_dict\n",
    "\n",
    "\n",
    "def coding_length(mrf_string):\n",
    "    char_prob_dict = character_probs(mrf_string)\n",
    "    coding_len = 0\n",
    "    for character in mrf_string:\n",
    "        coding_len += log2(char_prob_dict[character])\n",
    "    return -coding_len\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T11:15:07.166647Z",
     "start_time": "2019-11-14T11:15:07.160859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coding_len_degenerate is:\n",
      "38.55\n",
      "\n",
      "coding_len_degenerate is:\n",
      "59.2\n",
      "\n",
      "coding_len_degenerate is:\n",
      "67.29\n"
     ]
    }
   ],
   "source": [
    "# First, let's check whether the functions defined above work correctly\n",
    "# for the example languages given in Kirby et al. (2015):\n",
    "\n",
    "meanings = ['02', '03', '12', '13']\n",
    "forms_without_noisy_variants = ['aa', 'ab', 'ba', 'bb']\n",
    "\n",
    "lang_degenerate = ['aa', 'aa', 'aa', 'aa']\n",
    "\n",
    "lang_holistic = ['aa', 'ab', 'bb', 'ba']\n",
    "\n",
    "lang_compositional = ['aa', 'ab', 'ba', 'bb']\n",
    "\n",
    "\n",
    "coding_len_degenerate = coding_length(lang_degenerate)\n",
    "print(\"coding_len_degenerate is:\")\n",
    "print(round(coding_len_degenerate, ndigits=2))\n",
    "\n",
    "\n",
    "coding_len_degenerate = coding_length(lang_holistic)\n",
    "print('')\n",
    "print(\"coding_len_degenerate is:\")\n",
    "print(round(coding_len_degenerate, ndigits=2))\n",
    "\n",
    "\n",
    "coding_len_degenerate = coding_length(lang_compositional)\n",
    "print('')\n",
    "print(\"coding_len_degenerate is:\")\n",
    "print(round(coding_len_degenerate, ndigits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T10:43:06.355216Z",
     "start_time": "2019-11-14T10:43:06.345974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coding_len_compositional_reduplicate_whole_signal is:\n",
      "64.24\n",
      "\n",
      "coding_len_compositional_reduplicate_segments is:\n",
      "64.24\n",
      "\n",
      "coding_len_holistic_diversify_signal is:\n",
      "84.83\n",
      "\n",
      "coding_len_compositional_repair is:\n",
      "59.2\n",
      "\n",
      "ratio_reduplication_vs_repair is:\n",
      "1.09\n",
      "\n",
      "ratio_diversify_signal_vs_reduplication is:\n",
      "1.32\n",
      "\n",
      "ratio_diversify_signal_vs_repair is:\n",
      "1.43\n"
     ]
    }
   ],
   "source": [
    "# And now that we know that these functions are coded up correctly,\n",
    "# let's have a look at the coding lengths for our example languages\n",
    "# for Possibility 1: different form lengths\n",
    "\n",
    "mrf_compositional_reduplicate_whole_signal = \"SABAB.A0a.A1b.B2a.B3b\"\n",
    "coding_len_compositional_reduplicate_whole_signal = coding_length(mrf_compositional_reduplicate_whole_signal)\n",
    "print(\"coding_len_compositional_reduplicate_whole_signal is:\")\n",
    "print(round(coding_len_compositional_reduplicate_whole_signal, ndigits=2))\n",
    "\n",
    "mrf_compositional_reduplicate_segments = \"SAABB.A0a.A1b.B2a.B3b\"\n",
    "coding_len_compositional_reduplicate_segments = coding_length(mrf_compositional_reduplicate_segments)\n",
    "print('')\n",
    "print(\"coding_len_compositional_reduplicate_segments is:\")\n",
    "print(round(coding_len_compositional_reduplicate_segments, ndigits=2))\n",
    "\n",
    "mrf_holistic_diversify_signal = \"S02aaaa.S03bbbb.S12abba.S13baab\"\n",
    "coding_len_holistic_diversify_signal = coding_length(mrf_holistic_diversify_signal)\n",
    "print('')\n",
    "print(\"coding_len_holistic_diversify_signal is:\")\n",
    "print(round(coding_len_holistic_diversify_signal, ndigits=2))\n",
    "\n",
    "mrf_compositional_repair = \"SAB.A0a.A1b.B2a.B3b\"\n",
    "coding_len_compositional_repair = coding_length(mrf_compositional_repair)\n",
    "print('')\n",
    "print(\"coding_len_compositional_repair is:\")\n",
    "print(round(coding_len_compositional_repair, ndigits=2))\n",
    "\n",
    "\n",
    "ratio_reduplication_vs_repair = coding_len_compositional_reduplicate_whole_signal/coding_len_compositional_repair\n",
    "print('')\n",
    "print(\"ratio_reduplication_vs_repair is:\")\n",
    "print(round(ratio_reduplication_vs_repair, ndigits=2))\n",
    "\n",
    "\n",
    "ratio_diversify_signal_vs_reduplication = coding_len_holistic_diversify_signal/coding_len_compositional_reduplicate_whole_signal\n",
    "print('')\n",
    "print(\"ratio_diversify_signal_vs_reduplication is:\")\n",
    "print(round(ratio_diversify_signal_vs_reduplication, ndigits=2))\n",
    "\n",
    "ratio_diversify_signal_vs_repair = coding_len_holistic_diversify_signal/coding_len_compositional_repair\n",
    "print('')\n",
    "print(\"ratio_diversify_signal_vs_repair is:\")\n",
    "print(round(ratio_diversify_signal_vs_repair, ndigits=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so as we can see from the coding lengths above, possibility 1 of how to represent languages gives relative coding lengths that capture the intuitions we have about how hard it is to learn these different languages: the compositional languages with reduplication only have slightly longer coding lengths than the compositional language without it (ratio reduplication:repair = 1.09:1), whereas the holistic language resulting from the diversify signal strategy has a significantly longer coding length (ratio diversify_signal:repair = 1.43:1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion about how to represent languages:\n",
    "\n",
    "Based on all the above, I'd say let's go for possibility 1 of allowing for different form lengths. That keeps our way of representing languages as close as possibility to the one used by Kirby et al. (2015); it allows us to straightforwardly calculate the coding lengths; and it will not cause languages to make use of a different number of characters, as possibility 2 would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Burkett, D., & Griffiths, T. L. (2010). Iterated learning of multiple languages from multiple teachers. The Evolution of Language: Proceedings of the 8th International Conference (EVOLANG8), Utrecht, Netherlands, 14-17 April 2010, 58–65.\n",
    "\n",
    "Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87–102. https://doi.org/10.1016/j.cognition.2015.03.016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
