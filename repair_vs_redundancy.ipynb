{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different strategies for dealing with noise\n",
    "\n",
    "\n",
    "\n",
    "The idea here is that there are different possible strategies of dealing with noise that are attested in natural communication systems:\n",
    "\n",
    "* **Reduplication**: simply repeat the signal several times. This is compatible with a compositional system. It is not costly in terms of learnability, because the only extra thing that needs to be learned is a single extra rule that applies to all signals. However, it is relatively costly in terms of utterance length (it would thus not do well under a pressure for minimal effort). \n",
    "* **Diversify signal**: make the individual segments that each signal consists of as distinct as possible. For example, in the language shown below, all four signals can be distinguished from each other in all cases where one character is obscured by noise. This strategy however is not compatible with compositionality, because it relies on making each of the segments as distinct from each other as possible. That means these languages are necessarily holistic, and therefore less easy to learn (so they would do less well under a pressure for learnability).\n",
    "    - 02 --> 'aaaa'\n",
    "    - 03 --> 'bbbb'\n",
    "    - 12 --> 'abba'\n",
    "    - 13 --> 'baab'\n",
    "* **Repair**: This strategy could be seen as a form of redundancy across turns, instead of within a signal. However, it will be initiated only when neccesary, and should therefore fare slightly better than the reduplication strategy under a pressure for minimal effort (where effort is measured as total shared utterance length of both interlocutors across a given number of interactions).\n",
    "\n",
    "The predictions of Vinicius & Seán (2016 Evolang abstract titled \"Language adapts to signal disruption in interaction\"), are that although the reduplication strategy and the repair strategy should do equally well under a pressure for learnability, adding the possibility of repair will 'lift the pressure for redundancy', such that receivers can request that speakers repeat a signal only after a problem occurs. The line of reasoning of Vinicius & Seán in this Evolang abstract is that although the reduplication strategy would do relatively well under a pressure for learnability, it has not been adopted widely in natural language. They argue that this is a result of the fact that repair was available. \n",
    "---> Another way of looking at this is that the repair strategy will do better than the reduplication strategy under a pressure for minimal effort (if measured in terms of shared utterance length of speaker and hearer across a fixed number of interactions). Such a pressure for minimal effort, in combination with a pressure for mutual understanding (a.k.a. expressivity) would provide a push for using repair, and once repair is used, it will 'lift the pressure for redundancy', as Vinicius & Seán put it. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions under different selection pressures:\n",
    "\n",
    "We predict that under the following assumptions:\n",
    "- There is a pressure for expressivity/mutual understanding\n",
    "- Noise regularly disrupts part of the signal (Vinicius & Seán used a 0.5 probability in their experiment)\n",
    "- Repair is a possibility\n",
    "\n",
    "\n",
    "the following strategies will become dominant under the following combinations of the presense/absence of a pressure for learnability and a pressure for minimal effort:\n",
    "\n",
    "|                | - minimal effort                                              | + minimal effort          |\n",
    "|----------------|---------------------------------------------------------------|---------------------------|\n",
    "| **- learnability** | Any of the three strategies above will do                                         | Repair + Compositional OR Holistic         |\n",
    "| **+ learnability** | Reduplication + Compositional OR Repair + Compositional | Repair + Compositional |\n",
    "\n",
    "<span class=\"mark\">**Note**</span> that the prediction in the {-learnability, +minimal effort} condition above only holds if we do not distinguish between open and closed requests. Because if we do, as in the model we submitted to evolang, we'd expect the Repair + Compositional strategy to fare best in this condition (i.e. better than Repair + Holistic), without the need for a pressure for learnability.\n",
    "\n",
    "<span class=\"mark\">**Note**</span> also that the coding length of the Reduplication + Compositional languages _is_ slightly higher than the coding length of the Repair + Compositional languages (ratio reduplication:repair = 1.09:1; see calculations in section 1.3.2), so depending on the bottleneck, we might expect the Repair + Compositional strategy to win out even in the {+learnability, -minimal effort} condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to represent languages?\n",
    "\n",
    "### Possibility 1: Different form lengths\n",
    "\n",
    "If we continue with Kirby et al.'s (2015) way of representing meanings and forms (which is a minimal way of creating languages that we can classify as compositional, holistic or degenerate), where meanings consist of $f=2$ features, which can each have $v=2$ values, we can allow for each of the language strategies specified above ('reduplication' and 'diversify signal'), by simply allowing for multiple string lengths $l$, while keeping the alphabet size $|\\Sigma|$ at 2.\n",
    "\n",
    "For example, where Kirby et al. (2015) only allowed for a single possible string length, and specified $f = v = l = |\\Sigma| = 2$, we could minimally allow for two possible string lengths: one being equal to $f$ (i.e. the minimum string length required to uniquely specify each meaning feature), and one being equal to $2*f$, to enable reduplication of the signal.\n",
    "\n",
    "That would yield the following types of languages:\n",
    "\n",
    "\n",
    "**Reduplication + compositional:**\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> abab\n",
    "\n",
    "12 --> baba\n",
    "\n",
    "13 --> bbbb\n",
    "\n",
    "\n",
    "**Diversify signal + holistic:**\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> bbbb\n",
    "\n",
    "12 --> abba\n",
    "\n",
    "13 --> baab\n",
    "\n",
    "\n",
    "**Repair + compositional:**\n",
    "\n",
    "02 --> aa\n",
    "\n",
    "03 --> ab\n",
    "\n",
    "12 --> ba\n",
    "\n",
    "13 --> bb\n",
    "\n",
    "\n",
    "In order to still make it possible for iterated learning chains to transition from a language that uses forms of length 2 into a language that uses forms of length 4 and vice versa, we need to then also allow for languages that use a mixture of form lengths (e.g. three forms of length 2, and one form of length 4). This yields the following number of possible languages:\n",
    "\n",
    "$$ (2^2+2^4)^4 = 160000$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:17:16.246794Z",
     "start_time": "2020-05-26T12:17:16.243283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000\n",
      "65792\n",
      "2.431906614785992\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, if we allow only for languages that use \n",
    "# either only forms of length 2, or only forms of length 4,\n",
    "# we end up with the following total number of languages:\n",
    "\n",
    "n_langs_mix = (2**2 + 2**4)**4\n",
    "print(n_langs_mix)\n",
    "n_langs_no_mix = ((2**2)**4)+((2**4)**4)\n",
    "print(n_langs_no_mix)\n",
    "print(n_langs_mix/n_langs_no_mix)\n",
    "\n",
    "# which would make the hypothesis space nearly 2.5 times smaller.\n",
    "# The problem with this however is that it really is hard to see\n",
    "# how populations would transition from having a language that uses\n",
    "# forms of length 2 to a language that uses forms of length 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means that compared to the Kirby et al. (2015) model (where there were ($(2^2)^4 = 256$ possible languages), the hypothesis space expands by a factor of 625. That is not ideal, because if we assume that simulation run times increase linearly with the size of the hypothesis space, a simulation that took 1 hour to run in our previous model would now take almost 4 weeks to run.\n",
    "\n",
    "However, this linear relationship between the simulation run times and the size of the hypothesis space holds (presumably) when during learning, we actually loop through each hypothesis and update its posterior probability based on the data. There are a couple of ways in which this process can be optimised:\n",
    "\n",
    "1. **memoisation**: This would require enumerating all possible data points (i.e. <meaning, form> pairs) (including all possible noisy forms), and for each of them calculating its likelihood for all possible hypotheses **once**, and caching the result. Whenever the same <meaning, form> pair is then encountered by any learner, the corresponding likelihood vector is then simply retrieved from memory and multiplied with the learner's current posterior. This should be doable given that the total number of meanings is 4, and the total number of forms (including all possible noisy variants, assuming that noise is restricted to a single character) is 56; which makes 4\\*56 = 224 possible <meaning, form> pairs. For each of those 224 possible datapoints, we would then calculate its likelihood for all 160,000 hypotheses, and cache these values in a 224\\*160,000 matrix. (That matrix thus has 224\\*160,000 = 35,840,000 entries.)\n",
    "2. Intergenerational learning could be sped up by representing data as simple counts of <meaning, form> pairs, and simply updating the posterior probability distribution for the full data set in one step, by multiplying the prior of the hypothesis with the likelihood of the <meaning, form> pair to the power of the number of times it occurs in the data set. This should speed things up a little in intergenerational learning, but won't make a difference in *intra*generational learning, because there we assume that the hearer updates their posterior in each interaction. \n",
    "3. Do not do exact inference over the full hypothesis space at all, but instead use an MCMC sampling technique (e.g. Burkett & Griffiths, 2010, and Kirby et al., 2015 use Gibbs sampling) --> This would require a bit more time to figure out, and is hopefully not necessary once optimisations 1 and 2 above have been implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possibility 2: Allow reduplication as grammatical rule, and increase alphabet size\n",
    "\n",
    "If instead of allowing for multiple form lengths, we instead increase the size of the alphabet $\\Sigma$ from 2 to 4, that will make the diversify signal strategy possible. More concretely, that would mean that instead of there being an alphabet $[a, b]$, there would be an alphabet $[a, b, c, d]$. \n",
    "That would allow for the following example languages, where the bit at the end of the signal specifies whether the signal should be repeated (1) or not (0).\n",
    "\n",
    "\n",
    "**Reduplication + compositional:**\n",
    "\n",
    "02 --> aa1\n",
    "\n",
    "03 --> ab1\n",
    "\n",
    "12 --> ba1\n",
    "\n",
    "13 --> bb1\n",
    "\n",
    "\n",
    "**Diversify signal + holistic:**\n",
    "\n",
    "02 --> aa0\n",
    "\n",
    "03 --> bb0\n",
    "\n",
    "12 --> cc0\n",
    "\n",
    "13 --> dd0\n",
    "\n",
    "\n",
    "**Repair + compositional:**\n",
    "\n",
    "02 --> aa0\n",
    "\n",
    "03 --> ab0\n",
    "\n",
    "12 --> ba0\n",
    "\n",
    "13 --> bb0\n",
    "\n",
    "\n",
    "Choosing for this option would mean that instead of there being $(2ˆ2 + 2ˆ4) = 20$ possible forms, there would be $4ˆ2 = 16$ possible forms, and therefore $(4^2)^4 = 65536$ possible languages. In addition however, we'd need languages to have an extra bit that specifies whether signals are reduplicated or not (assuming there are only two options: reduplication ON versus reduplication OFF). That means that there'd be a total of $((4^2)^4)*2 = 131072$. So compared to possibility 1, possibility 2 only reduces the size of the hypothesis space by a factor of 1.22. That is not much, but an added advantage of this way of representing languages is that it allows for a straightforward way of capturing the assumed simplicity of a reduplication rule in the coding of the languages, and therefore into the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we find that despite the optimisation strategies outlined above it is still not feasible to run simulations within a reasonable time-frame, we could consider tackling the different possible strategies for dealing with noise separately. I.e. one model where we allow for the possibility to add reduplication to signals vs. repair, and another model where we allow for diversification of signal segments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressibility measure for two different language representation possibilities\n",
    "\n",
    "\n",
    "### Possibility 1: Different form lengths\n",
    "\n",
    "#### Rewrite rules:\n",
    "\n",
    "**Reduplication + compositional:**\n",
    "\n",
    "There are in fact two different ways of reduplicating a compositional language: either reduplicating the whole signal, or reduplicating each of the segments. In both cases the length of the minimally redundant form, and therefore the language type's compressibility, will be the same however, as shown below.\n",
    "\n",
    "_**Reduplicate whole signal:**_\n",
    "\n",
    "*Language:*\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> abab\n",
    "\n",
    "12 --> baba\n",
    "\n",
    "13 --> bbbb\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S --> ABAB\n",
    "\n",
    "A:0 --> a\n",
    "\n",
    "A:1 --> b\n",
    "\n",
    "B:2 --> a\n",
    "\n",
    "B:3 --> b\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "SABAB.A0a.A1b.B2a.B3b\n",
    "\n",
    "\n",
    "_**Reduplicate each segment:**_\n",
    "\n",
    "*Language:*\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> aabb\n",
    "\n",
    "12 --> bbaa\n",
    "\n",
    "13 --> bbbb\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S --> AABB\n",
    "\n",
    "A:0 --> a\n",
    "\n",
    "A:1 --> b\n",
    "\n",
    "B:2 --> a\n",
    "\n",
    "B:3 --> b\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "SAABB.A0a.A1b.B2a.B3b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Diversify signal + holistic:**\n",
    "\n",
    "02 --> aaaa\n",
    "\n",
    "03 --> bbbb\n",
    "\n",
    "12 --> abba\n",
    "\n",
    "13 --> baab\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S:02 --> aaaa\n",
    "\n",
    "S:03 --> bbbb\n",
    "\n",
    "S:12 --> abba\n",
    "\n",
    "S:13 --> baab\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "S02aaaa.S03bbbb.S12abba.S13baab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Repair + compositional:**\n",
    "\n",
    "02 --> aa\n",
    "\n",
    "03 --> ab\n",
    "\n",
    "12 --> ba\n",
    "\n",
    "13 --> bb\n",
    "\n",
    "\n",
    "*Rewrite rules:*\n",
    "\n",
    "S --> AB\n",
    "\n",
    "A:0 --> a\n",
    "\n",
    "A:1 --> b\n",
    "\n",
    "B:2 --> a\n",
    "\n",
    "B:3 --> b\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "SAB.A0a.A1b.B2a.B3b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's calculate the actual compressibility in terms of coding length, given the strings in minimally redundant form specified above, starting with languages that only make use of forms of length 2, as in Kirby et al. (2015):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:17:22.303913Z",
     "start_time": "2020-05-26T12:17:22.292578Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_language_four_forms(lang, forms, meaning_list):\n",
    "    \"\"\"\n",
    "    Classify one particular language as either 0 = degenerate, 1 = holistic, 2 = hybrid, 3 = compositional, 4 = other\n",
    "    (Kirby et al., 2015). NOTE that this function is specific to classifying languages that consist of exactly 4 forms,\n",
    "    where each form consists of exactly 2 characters. For a more general version of this function, see\n",
    "    classify_language_general() below.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param forms: list of strings corresponding to all possible forms_without_noisy_variants\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :returns: integer corresponding to category that language belongs to:\n",
    "    0 = degenerate, 1 = holistic, 2 = hybrid, 3 = compositional, 4 = other (here I'm following the\n",
    "    ordering used in the Kirby et al., 2015 paper; NOT the ordering from SimLang lab 21)\n",
    "    \"\"\"\n",
    "    class_degenerate = 0\n",
    "    class_holistic = 1\n",
    "    class_hybrid = 2  # this is a hybrid between a holistic and a compositional language; where *half* of the partial\n",
    "    # forms is mapped consistently to partial meanings (instead of that being the case for *all* partial forms)\n",
    "    class_compositional = 3\n",
    "    class_other = 4\n",
    "\n",
    "    # First check whether some conditions are met, bc this function hasn't been coded up in the most general way yet:\n",
    "    if len(forms) != 4:\n",
    "        raise ValueError(\n",
    "            \"This function only works for a world in which there are 4 possible forms_without_noisy_variants\"\n",
    "        )\n",
    "    if len(forms[0]) != 2:\n",
    "        raise ValueError(\n",
    "            \"This function only works when each form consists of 2 elements\")\n",
    "    if len(lang) != len(meaning_list):\n",
    "        raise ValueError(\"Lang should have same length as meanings\")\n",
    "\n",
    "    # lang is degenerate if it uses the same form for every meaning:\n",
    "    if lang[0] == lang[1] and lang[1] == lang[2] and lang[2] == lang[3]:\n",
    "        return class_degenerate\n",
    "\n",
    "    # lang is compositional if it makes use of all possible forms_without_noisy_variants, *and* each form element maps\n",
    "    # to the same meaning element for each form:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[\n",
    "        3] in lang and lang[0][0] == lang[1][0] and lang[2][0] == lang[3][0] and lang[0][\n",
    "        1] == lang[2][1] and lang[1][1] == lang[3][1]:\n",
    "        return class_compositional\n",
    "\n",
    "    # lang is holistic if it is *not* compositional, but *does* make use of all possible forms_without_noisy_variants:\n",
    "    elif forms[0] in lang and forms[1] in lang and forms[2] in lang and forms[3] in lang:\n",
    "        # within holistic languages, we can distinguish between those in which at least one part form is mapped\n",
    "        # consistently onto one part meaning. This class we will call 'hybrid' (because for the purposes of repair, it\n",
    "        # is a hybrid between a holistic and a compositional language, because for half of the possible noisy forms that\n",
    "        # a listener could receive it allows the listener to figure out *part* of the meaning, and therefore use a\n",
    "        # restricted request for repair instead of an open request.\n",
    "        if lang[0][0] == lang[1][0] and lang[2][0] == lang[3][0]:\n",
    "            return class_hybrid\n",
    "        elif lang[0][1] == lang[2][1] and lang[1][1] == lang[3][1]:\n",
    "            return class_hybrid\n",
    "        else:\n",
    "            return class_holistic\n",
    "\n",
    "    # In all other cases, a language belongs to the 'other' category:\n",
    "    else:\n",
    "        return class_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:17:23.617001Z",
     "start_time": "2020-05-26T12:17:23.595651Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import string\n",
    "\n",
    "\n",
    "def mrf_degenerate(lang, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a degenerate language and returns a minimally redundant form description of the language's context free\n",
    "    grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    mrf_string = 'S'\n",
    "    for i in range(len(meaning_list)):\n",
    "        meaning = meaning_list[i]\n",
    "        if i != len(meaning_list) - 1:\n",
    "            mrf_string += str(meaning) + ','\n",
    "        else:\n",
    "            mrf_string += str(meaning)\n",
    "    mrf_string += lang[0]\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def mrf_holistic(lang, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a holistic OR hybrid language and returns a minimally redundant form description of the language's context\n",
    "    free grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    mrf_string = ''\n",
    "    for i in range(len(meaning_list)):\n",
    "        meaning = meaning_list[i]\n",
    "        form = lang[i]\n",
    "        if i != len(meaning_list) - 1:\n",
    "            mrf_string += 'S' + meaning + form + '.'\n",
    "        else:\n",
    "            mrf_string += 'S' + meaning + form\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def mrf_compositional(lang, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a compositional language and returns a minimally redundant form description of the language's context free\n",
    "    grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    n_features = len(meaning_list[0])\n",
    "    non_terminals = string.ascii_uppercase[:n_features]\n",
    "    mrf_string = 'S' + non_terminals\n",
    "    for i in range(len(non_terminals)):\n",
    "        non_terminal_symbol = non_terminals[i]\n",
    "        feature_values = []\n",
    "        feature_value_segments = []\n",
    "        for j in range(len(meaning_list)):\n",
    "            if meaning_list[j][i] not in feature_values:\n",
    "                feature_values.append(meaning_list[j][i])\n",
    "                feature_value_segments.append(lang[j][i])\n",
    "        for k in range(len(feature_values)):\n",
    "            value = feature_values[k]\n",
    "            segment = feature_value_segments[k]\n",
    "            mrf_string += \".\" + non_terminal_symbol + value + segment\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def mrf_other(lang, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a language of the 'other' category and returns a minimally redundant form description of the language's\n",
    "    context free grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    mapping_dict = {}\n",
    "    for i in range(len(lang)):\n",
    "        mapping_dict.setdefault(lang[i], []).append(meaning_list[i])\n",
    "    mrf_string = 'S'\n",
    "    counter = 0\n",
    "    for form in mapping_dict.keys():\n",
    "        for k in range(len(mapping_dict[form])):\n",
    "            meaning = mapping_dict[form][k]\n",
    "            if k != len(mapping_dict[form]) - 1:\n",
    "                mrf_string += meaning + ','\n",
    "            else:\n",
    "                mrf_string += meaning\n",
    "        if counter != len(mapping_dict.keys()) - 1:\n",
    "            mrf_string += form + '.S'\n",
    "        else:\n",
    "            mrf_string += form\n",
    "        counter += 1\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def minimally_redundant_form(lang, complete_forms, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a language of any class and returns a minimally redundant form description of its context free grammar.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param complete_forms: list containing all possible complete forms; corresponds to global variable\n",
    "    'forms_without_noise'\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: minimally redundant form description of the language's context free grammar (string)\n",
    "    \"\"\"\n",
    "    lang_class = classify_language_four_forms(lang, complete_forms, meaning_list)\n",
    "    if lang_class == 0:  # the language is 'degenerate'\n",
    "        mrf_string = mrf_degenerate(lang, meaning_list)\n",
    "    elif lang_class == 1 or lang_class == 2:  # the language is 'holistic' or 'hybrid'\n",
    "        mrf_string = mrf_holistic(lang, meaning_list)\n",
    "    elif lang_class == 3:  # the language is 'compositional'\n",
    "        mrf_string = mrf_compositional(lang, meaning_list)\n",
    "    elif lang_class == 4:  # the language is of the 'other' category\n",
    "        mrf_string = mrf_other(lang, meaning_list)\n",
    "    return mrf_string\n",
    "\n",
    "\n",
    "def character_probs(mrf_string):\n",
    "    \"\"\"\n",
    "    Takes a string in minimally redundant form and generates a dictionary specifying the probability of each of the\n",
    "    symbols used in the string\n",
    "\n",
    "    :param mrf_string: a string in minimally redundant form\n",
    "    :return: a dictionary with the symbols as keys and their corresponding probabilities as values\n",
    "    \"\"\"\n",
    "    count_dict = {}\n",
    "    for character in mrf_string:\n",
    "        if character in count_dict.keys():\n",
    "            count_dict[character] += 1\n",
    "        else:\n",
    "            count_dict[character] = 1\n",
    "    prob_dict = {}\n",
    "    for character in count_dict.keys():\n",
    "        char_prob = count_dict[character] / len(mrf_string)\n",
    "        prob_dict[character] = char_prob\n",
    "    return prob_dict\n",
    "\n",
    "\n",
    "def coding_length(mrf_string):\n",
    "    \"\"\"\n",
    "    Takes a string in minimally redundant form and returns its coding length in bits\n",
    "\n",
    "    :param mrf_string: a string in minimally redundant form\n",
    "    :return: coding length in bits\n",
    "    \"\"\"\n",
    "    char_prob_dict = character_probs(mrf_string)\n",
    "    coding_len = 0\n",
    "    for character in mrf_string:\n",
    "        coding_len += log2(char_prob_dict[character])\n",
    "    return -coding_len\n",
    "\n",
    "\n",
    "def prior_single_lang(lang, complete_forms, meaning_list):\n",
    "    \"\"\"\n",
    "    Takes a language and returns its PROPORTIONAL prior probability; this still needs to be normalized over all\n",
    "    languages in order to give the real prior probability.\n",
    "\n",
    "    :param lang: a language; represented as a tuple of forms_without_noisy_variants, where each form index maps to same\n",
    "    index in meanings\n",
    "    :param complete_forms: list containing all possible complete forms; corresponds to global variable\n",
    "    'forms_without_noise'\n",
    "    :param meaning_list: list of strings corresponding to all possible meanings\n",
    "    :return: PROPORTIONAL prior probability (float)\n",
    "    \"\"\"\n",
    "    mrf_string = minimally_redundant_form(lang, complete_forms, meaning_list)\n",
    "    coding_len = coding_length(mrf_string)\n",
    "    prior = 2 ** -coding_len\n",
    "    return prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:17:24.702370Z",
     "start_time": "2020-05-26T12:17:24.691842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "lang_class_text is:\n",
      "degenerate\n",
      "lang is:\n",
      "['aa', 'aa', 'aa', 'aa']\n",
      "mrf_string is:\n",
      "S02,03,12,13aa\n",
      "coding_len is:\n",
      "38.55\n",
      "prior this lang is:\n",
      "2.488119421993922e-12\n",
      "\n",
      "1\n",
      "lang_class_text is:\n",
      "degenerate\n",
      "lang is:\n",
      "['ab', 'ab', 'ab', 'ab']\n",
      "mrf_string is:\n",
      "S02,03,12,13ab\n",
      "coding_len is:\n",
      "40.55\n",
      "prior this lang is:\n",
      "6.220298554984805e-13\n",
      "\n",
      "2\n",
      "lang_class_text is:\n",
      "other\n",
      "lang is:\n",
      "['aa', 'aa', 'aa', 'ab']\n",
      "mrf_string is:\n",
      "S02,03,12aa.S13ab\n",
      "coding_len is:\n",
      "52.73\n",
      "prior this lang is:\n",
      "1.3368788379305763e-16\n",
      "\n",
      "3\n",
      "lang_class_text is:\n",
      "other\n",
      "lang is:\n",
      "['aa', 'aa', 'aa', 'bb']\n",
      "mrf_string is:\n",
      "S02,03,12aa.S13bb\n",
      "coding_len is:\n",
      "53.49\n",
      "prior this lang is:\n",
      "7.922244965514519e-17\n",
      "\n",
      "4\n",
      "lang_class_text is:\n",
      "compositional\n",
      "lang is:\n",
      "['aa', 'ab', 'ba', 'bb']\n",
      "mrf_string is:\n",
      "SAB.A0a.A1b.B2a.B3b\n",
      "coding_len is:\n",
      "59.2\n",
      "prior this lang is:\n",
      "1.5092773625944422e-18\n",
      "\n",
      "5\n",
      "lang_class_text is:\n",
      "other\n",
      "lang is:\n",
      "['aa', 'aa', 'ab', 'ba']\n",
      "mrf_string is:\n",
      "S02,03aa.S12ab.S13ba\n",
      "coding_len is:\n",
      "61.68\n",
      "prior this lang is:\n",
      "2.700000000000012e-19\n",
      "\n",
      "6\n",
      "lang_class_text is:\n",
      "other\n",
      "lang is:\n",
      "['aa', 'aa', 'ab', 'bb']\n",
      "mrf_string is:\n",
      "S02,03aa.S12ab.S13bb\n",
      "coding_len is:\n",
      "62.17\n",
      "prior this lang is:\n",
      "1.9221679687499936e-19\n",
      "\n",
      "7\n",
      "lang_class_text is:\n",
      "hybrid\n",
      "lang is:\n",
      "['aa', 'ab', 'bb', 'ba']\n",
      "mrf_string is:\n",
      "S02aa.S03ab.S12bb.S13ba\n",
      "coding_len is:\n",
      "67.29\n",
      "prior this lang is:\n",
      "5.553712540966203e-21\n"
     ]
    }
   ],
   "source": [
    "# First, let's check whether the functions defined above work correctly\n",
    "# for the example languages given in Kirby et al. (2015):\n",
    "\n",
    "\n",
    "## First some parameter settings:\n",
    "meanings = ['02', '03', '12', '13']\n",
    "forms_without_noisy_variants = ['aa', 'ab', 'ba', 'bb']\n",
    "\n",
    "\n",
    "## Then let's specify the example languages from Kirby et al. (2015)\n",
    "example_languages = [['aa', 'aa', 'aa', 'aa'],\n",
    "                     ['ab', 'ab', 'ab', 'ab'],\n",
    "                     ['aa', 'aa', 'aa', 'ab'],\n",
    "                     ['aa', 'aa', 'aa', 'bb'],\n",
    "                     ['aa', 'ab', 'ba', 'bb'],\n",
    "                     ['aa', 'aa', 'ab', 'ba'],\n",
    "                     ['aa', 'aa', 'ab', 'bb'],\n",
    "                     ['aa', 'ab', 'bb', 'ba']]\n",
    "\n",
    "\n",
    "## And now let's calculate their coding lengths:\n",
    "lang_classes_text = ['degenerate', 'holistic', 'hybrid', 'compositional', 'other']\n",
    "for i in range(len(example_languages)):\n",
    "    lang = example_languages[i]\n",
    "    print('')\n",
    "    print(i)\n",
    "    lang_class = classify_language_four_forms(lang, forms_without_noisy_variants, meanings)\n",
    "    lang_class_text = lang_classes_text[lang_class]\n",
    "    print(\"lang_class_text is:\")\n",
    "    print(lang_class_text)\n",
    "    print(\"lang is:\")\n",
    "    print(lang)\n",
    "    mrf_string = minimally_redundant_form(lang, forms_without_noisy_variants, meanings)\n",
    "    print(\"mrf_string is:\")\n",
    "    print(mrf_string)\n",
    "    coding_len = coding_length(mrf_string)\n",
    "    print(\"coding_len is:\")\n",
    "    print(round(coding_len, ndigits=2))\n",
    "    prior = prior_single_lang(lang, forms_without_noisy_variants, meanings)\n",
    "    print(\"prior this lang is:\")\n",
    "    print(prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T12:20:24.329031Z",
     "start_time": "2020-04-09T12:20:24.314499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coding_len_compositional_reduplicate_whole_signal is:\n",
      "64.24\n",
      "\n",
      "coding_len_compositional_reduplicate_segments is:\n",
      "64.24\n",
      "\n",
      "coding_len_holistic_diversify_signal is:\n",
      "84.83\n",
      "\n",
      "coding_len_compositional_repair is:\n",
      "59.2\n",
      "\n",
      "ratio_reduplication_vs_repair is:\n",
      "1.09\n",
      "\n",
      "ratio_diversify_signal_vs_repair is:\n",
      "1.43\n"
     ]
    }
   ],
   "source": [
    "# These are the same as in the table in Kirby et al. (2015, p. 92),\n",
    "# except for the coding length of the compositional language. In \n",
    "# Kirby et al. (2015) it says 55.20, when I get 59.20. But given that\n",
    "# all the other values I get are the same as in the table, I suspect\n",
    "# this is a typo in the actual paper.\n",
    "# So now that we know that these functions are coded up correctly,\n",
    "# let's have a look at the coding lengths for our example languages\n",
    "# for Possibility 1: different form lengths\n",
    "\n",
    "mrf_compositional_reduplicate_whole_signal = \"SABAB.A0a.A1b.B2a.B3b\"\n",
    "coding_len_compositional_reduplicate_whole_signal = coding_length(mrf_compositional_reduplicate_whole_signal)\n",
    "print(\"coding_len_compositional_reduplicate_whole_signal is:\")\n",
    "print(round(coding_len_compositional_reduplicate_whole_signal, ndigits=2))\n",
    "\n",
    "mrf_compositional_reduplicate_segments = \"SAABB.A0a.A1b.B2a.B3b\"\n",
    "coding_len_compositional_reduplicate_segments = coding_length(mrf_compositional_reduplicate_segments)\n",
    "print('')\n",
    "print(\"coding_len_compositional_reduplicate_segments is:\")\n",
    "print(round(coding_len_compositional_reduplicate_segments, ndigits=2))\n",
    "\n",
    "mrf_holistic_diversify_signal = \"S02aaaa.S03bbbb.S12abba.S13baab\"\n",
    "coding_len_holistic_diversify_signal = coding_length(mrf_holistic_diversify_signal)\n",
    "print('')\n",
    "print(\"coding_len_holistic_diversify_signal is:\")\n",
    "print(round(coding_len_holistic_diversify_signal, ndigits=2))\n",
    "\n",
    "mrf_compositional_repair = \"SAB.A0a.A1b.B2a.B3b\"\n",
    "coding_len_compositional_repair = coding_length(mrf_compositional_repair)\n",
    "print('')\n",
    "print(\"coding_len_compositional_repair is:\")\n",
    "print(round(coding_len_compositional_repair, ndigits=2))\n",
    "\n",
    "\n",
    "ratio_reduplication_vs_repair = coding_len_compositional_reduplicate_whole_signal/coding_len_compositional_repair\n",
    "print('')\n",
    "print(\"ratio_reduplication_vs_repair is:\")\n",
    "print(round(ratio_reduplication_vs_repair, ndigits=2))\n",
    "\n",
    "ratio_diversify_signal_vs_repair = coding_len_holistic_diversify_signal/coding_len_compositional_repair\n",
    "print('')\n",
    "print(\"ratio_diversify_signal_vs_repair is:\")\n",
    "print(round(ratio_diversify_signal_vs_repair, ndigits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so as we can see from the coding lengths above, possibility 1 of how to represent languages gives relative coding lengths that capture the intuitions we have about how hard it is to learn these different languages: the compositional languages with reduplication only have slightly longer coding lengths than the compositional language without it (ratio reduplication:repair = 1.09:1), whereas the holistic language resulting from the diversify signal strategy has a significantly longer coding length (ratio diversify_signal:repair = 1.43:1)\n",
    "\n",
    "**<span class=\"mark\">Note:</span>** What I haven't looked at yet is how these ratios scale when signal lengths are increased. For instance, if noise would regularly obscure _two_ characters instead of one, the diversify signal strategy would require longer signals, which would result in even larger coding lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to classify languages when a mixture of form lengths is allowed:\n",
    "\n",
    "The definitions of degenerate and holistic languages remain the same here: \n",
    "- a degenerate language is a language that uses the same form for each meaning\n",
    "- a holistic language is a language that uses a unique form for each meaning, but that is *not* compositional\n",
    "\n",
    "The tricky one to define is what counts as a compositional language. The part of the definition that remains the same as when only one form length is allowed that is equal to the number of meaning features (i.e. $l$ = $f$), is that each substring has to map uniquely onto each meaning feature. That is, a fully compositional system must satisfy:\n",
    "\n",
    "$$ \\forall f \\in F \\; \\exists s_{ij} \\; \\text{such that} \\; p(f | s_{ij}) = 1 \\land p(s_{ij} | f) = 1$$ \n",
    "\n",
    "(definition from Matt Spike's thesis, section 5.2.3)\n",
    "However, when languages are allowed to use forms that are longer than the number of meaning features (i.e. $l$ can be either $f$ or $2*f$), it becomes trickier to define the length of a substring. As mentioned above, there are two ways in which reduplication can be used in a compositional system. Either individual substrings are reduplicated, resulting in a language like [aaaa, aabb, bbaa, bbbb], or full signals can be reduplicated, resulting in a language like [aaaa, abab, baba, bbbb]. \n",
    "1. If we know that a given language uses the the \"reduplicate each segment\" strategy, we can check that it is compositional by dividing each form into a number of chunks that corresponds to the number of meaning features (such that if forms have length 6 and there are 2 meaning features, we get 2 chunks of length 3), and subsequently checking that each of those chunks maps uniquely onto a given meaning feature, according to the rule stated in Equation 2 above. \n",
    "2. In contrast, if we know that a given language uses the \"reduplicate full signal\" strategy, we can check that it is compositional by dividing it up into chunks that each have a length corresponding to the number of meaning features (such that if forms have length 6 and there are 2 meaning features, we get 3 chunks of length 2), and subsequently checking that *within* the first such chunk of each form, we see that each substring *within* the chunk maps uniquely onto a given meaning feature, according to the rule stated in Equation 2 above. \n",
    "\n",
    "That part of the definition of what counts as a compositional language is still quite straightforward. However, there are two more tricky types of languages that could also potentially be classed as compositional:\n",
    "3. A language that uses forms of $l = 2*f$, *doesn't* use reduplication, but still has a compositional mapping from substrings to meaning features *if* you take a substring to have length $f$. For example: [aaba, aabb, baba, babb]. However, the question there is how a listener would know that that is the appropriate substring length to base their interpretation on. With the languages that use reduplication this induction process is more straightforward: first one can determine that the language uses either one or the other form of reduplication, and then based on that, one can determine whether the language is compositional. Should we assume that if all forms in the language consistently have length $l = 2*f$, but the language doesn't use either form of reduplication, we must then analyse it as substrings having length $l / f$? I.e. the longest possible substrings? \n",
    "4. Another language type that is tricky to analyse are languages that use a mixture of form lengths. The richest possible listener model would be to assume that the listener can decide whether to do repair on a form-by-form basis, and that a language could therefore use a mixture of reduplication and repair strategies. Such a language could look like this: [aaaa, abab, ba, bb], where we could still unambiguously identify that two of the forms use the \"reduplicate whole signal\" strategy, whereas the other two forms should be combined with repair. However, this means that the corresponding rewrite rules would no longer look like those of a compositional language. Instead, the most concise way this particular example language could be captured in rewrite rules would look like the grammar of a *holistic* language: \n",
    "*Rewrite rules:*\n",
    "\n",
    "S:02 --> aaaa\n",
    "\n",
    "S:03 --> abab\n",
    "\n",
    "S:12 --> ba\n",
    "\n",
    "S:13 --> bb\n",
    "\n",
    "\n",
    "*Minimally redundant form:*\n",
    "\n",
    "S02aaaa.S03abab.S12ba.S13bb\n",
    "\n",
    "\n",
    "In sum, I think we can conclude that languages of types 1, 2 and 3 should be classed as compositional, whereas languages of type 4 should not.\n",
    "--> The complexity that is added by languages of types 3 and 4 (and the richer listener model that it requires to identify these) could be avoided however by going for Possibility 2 for extending the language model described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion about how to represent languages:\n",
    "\n",
    "Based on all the above, I'd say let's go for possibility 1 of allowing for different form lengths. That keeps our way of representing languages as close as possible to the one used by Kirby et al. (2015); it allows us to straightforwardly calculate the coding lengths; and it will not cause languages to make use of a different number of characters, as possibility 2 would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implement minimal effort pressure:\n",
    "\n",
    "As I see it, we assume that this pressure plays a role in the process of communication. If we define effort as shared utterance length between speaker and listener, the way to minimise this effort would be for the speaker to probabilistically choose signals that are shorter when they can (like in Kanwal et al. 2017), and (perhaps) for the listener to sometimes decide not to do repair. But in our simplified version of the Kirby et al. (2015) model, each learner only infers a single language, which makes it hard to work out how the speaker could choose to use a different signal. I guess there are two ways to implement this minimal effort pressure on the side of the speaker:\n",
    "1. **Language selection**: Fully replicate the Kirby et al. (2015) model, including learners inferring a probability distribution over all possible languages, instead of a single language. In each interaction round, the speaker can then pick a language from their hypothesis (where the hypothesis is a probability distribution over languages) not by sampling from that hypothesis, but instead by first somehow multiplying the hypothesis with the inverse of the total utterance length for each language, such that the speaker doesn't choose the language (and therefore the form they will use to communicate their intended meaning) not only based on the combination of prior+data, but instead on the basis of prior+data+effort\n",
    "2. **Signal selection**: If we don't want to do the full replication of Kirby et al. (2015) (including the Gibbs sampling from the posterior distribution), we could somehow implement a form of reasoning by the speaker that resembles what we think participants do in the Kanwal et al. (2017) model: We assume that a speaker only has one language, but when choosing which form to use to communicate their intended meaning, they first do the RSA-style reasoning, and *subsequently* (probabilistically) decide whether they want to shorten the form. If they have no reason to believe that shortening the form would lead to ambiguity (because the shortened form doesn't overlap with an already existing form in the language), they (probabilistically) decide to use the shortened form. This way of implementing it feels a bit more crude perhaps, but by locating the minimal effort pressure in the likelihood like this (instead of in the inference step), it does perhaps come closer to what we're trying to model: that *given* a particular language, speakers sometimes choose to use a shorter form. This could be parameterised, just like the degree to which speakers try to avoid ambiguity (i.e. by the $\\gamma$ parameter in the Kirby et al., 2017, paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look at how to implement option 2 above:\n",
    "\n",
    "#### Production in the Kirby et al. (2015) model:\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\; \\; \\textrm{if} \\; t \\; \\textrm{is mapped to} \\; f \\; \\textrm{in} \\; l\\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\quad \\quad \\quad \\textrm{if} \\; t \\; \\textrm{is not mapped to} \\; f \\; \\textrm{in} \\; l\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $f$ stands for form (in the sense of a complete signal, such as *aa*), $l$ stands for language, $t$ stands for topic, $a$ stands for ambiguity (i.e. the number of meanings in $M$ that map to form $f$ in $l$), $\\gamma$ specifies the extent to which ambiguous utterances are penalised, and $\\epsilon$ stands for production error.\n",
    "\n",
    "\n",
    "From Kirby et al. (2015, pp. 92-93):\n",
    "> \"If $a$ = 1 ($f$ is unambiguous) and/or $\\gamma$ = 0 then this yields a model of production where the 'correct' form is produced with probability $1-\\epsilon$. However, when $\\gamma$ > 0 and $f$ is ambiguous (i.e., $a$ > 1), then the 'correct' mapping from $t$ to $f$ is less likely to be produced (the probability $P(f \\mid l, t)$ is reduced by the factor $(\\frac{1}{a})^\\gamma$ ) and the remaining probability mass is spread equally over the other possible forms, leading to increased probability of producing $f'$ $\\neq$ $f$. Therefore, $\\gamma$ > 0 introduces a penalty for languages whose utterances are ambiguous.\"\n",
    "\n",
    "\n",
    "#### Production with environmental noise:\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\, (1-n) \\; \\; \\textrm{if} \\; f \\; \\textrm{is mapped to} \\; t \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact} \\\\ \n",
    "(\\frac{1}{a})^\\gamma \\, (1-\\epsilon) \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\; \\; \\textrm{if part of} \\; f \\; \\textrm{is mapped to} \\; t \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact} \\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\, (1-n) \\; \\quad \\quad \\quad \\textrm{if} \\; f \\; \\textrm{is not mapped to} \\; t \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact}\\\\\n",
    "\\frac{\\epsilon}{|F|-1} \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\quad \\quad \\quad \\textrm{if no part of} \\; f \\; \\textrm{is mapped to} \\; t \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where *n* stands for the probability of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to add minimal effort pressure to production function:\n",
    "\n",
    "The good thing about the likelihood function above, is that it considers **all** possible forms; not only the ones that are actually in the language. So the speaker may end up producing a form that is not quite the same as the form that is mapped to their intended meaning either (i) as a result of noise, where they produce only a noisy variant of the form that's mapped to their intended referent, or (ii) as a result of a production error, mediated by the ambiguity of the form (i.e. making an 'error' becomes more likely the more ambiguous the form that's associated with the intended meaning is). What we'd want to add is that in addition to the form that's mapped to the intended meaning (and its noisy variants), the production function also makes the probability of producing the form that is mapped to the intended meaning depend on the *length* of the form, in a way that is governed by a minimal effort parameter (similarly to the ambiguity avoidance parameter $\\gamma$). \n",
    "\n",
    "<span class=\"burk\">That's probably the simplest way of implementing a minimal effort pressure, and it would look like this:</span>\n",
    "\n",
    "#### Production with environmental noise and minimal effort pressure:\n",
    "\n",
    "$$\n",
    "P(f \\mid l, t) \\propto \\Bigg\\{\n",
    "\\begin{array}{ll}\n",
    "(\\frac{1}{a})^\\gamma \\, (\\frac{1}{\\ell})^\\delta \\, (1-\\epsilon) \\, (1-n) \\; \\; \\textrm{if} \\; f \\; \\textrm{is mapped to} \\; t \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact} \\\\ \n",
    "(\\frac{1}{a})^\\gamma \\, (\\frac{1}{\\ell})^\\delta \\, (1-\\epsilon) \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\; \\; \\textrm{if part of} \\; f \\; \\textrm{is mapped to} \\; t \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact} \\\\ \n",
    "\\frac{\\epsilon}{|F|-1} \\, (1-n) \\; \\quad \\quad \\quad \\textrm{if} \\; f \\; \\textrm{is not mapped to} \\; t \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is intact}\\\\\n",
    "\\frac{\\epsilon}{|F|-1} \\, \\frac{n}{|F_{noisy}|} \\quad \\quad \\quad \\quad \\quad \\textrm{if no part of} \\; f \\; \\textrm{is mapped to} \\; t \\; \\textrm{in} \\; l \\; \\textrm{and} \\; f \\; \\textrm{is not intact}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\ell$ stands for the length of the form, and $\\delta$ is the parameter that governs the strength of the minimal effort pressure. \n",
    "\n",
    "\n",
    "Another option would be for the speaker to specifically consider shorter versions of the form that maps to the intended meaning, depending on whether that would cause extra ambiguity or not (because it may be that that shorter form is already used for another meaning in the language). That would make the production function a bit more complex, but maybe it makes more sense intuitively? That the speaker is basically considering using a clipping, as in the Kanwal et al. (2017) experiment, rather than just using *any* other form because it's shorter? However, the idea of clippings doesn't really make sense in a compositional system that uses either (i) reduplication of the segments or (ii) substrings of length 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reception\n",
    "\n",
    "<span class=\"burk\">When should the listener initiate repair?</span> There are a couple of options here:\n",
    "\n",
    "1. Deterministically whenever there is ambiguity (so even if there was no noise in the form, but the listener's language just makes the form difficult to interpret)\n",
    "2. Deterministically whenever there is ambiguity *due to noise* (so only if there was actually noise obscuring the signal). \n",
    "3. Probabilistically depending on the level of ambiguity (regardless of whether noise was present\n",
    "4. Probabilistically depending on the level of ambiguity, but *only* if noise was present.\n",
    "\n",
    "A probabilistic model (i.e. depending on the level of ambiguity) might be more realistic, but it would add a bit of extra complexity to the model, so let's start with a deterministic version. Option 1 might make most sense, because the listener should mainly be driven by ambiguity (i.e. how certain they are about their interpretation of the signal) when determining whether to initiate repair or not. And indeed there is a chance that the listener will get a less ambiguous signal the second time around, because the speaker does (probabilistically) try to avoid ambiguity when choosing their signal.\n",
    "\n",
    "<span class=\"mark\">Note:</span> no matter which of these options we choose, we are assuming that the listener knows the *length of the signal* and the *location of the noise* (i.e. which part of the signal was obscured)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should repair be learned or not?\n",
    "\n",
    "### There are two options here:\n",
    "\n",
    "1. **Repair is learned:** This would mean that every possible language in the hypothesis space could in principle be combined with repair. This could be encoded as an extra bit of information added on to the language itself, so that repair can be either 'on' or 'off'. \n",
    "2. **Repair is _not_ learned:** This would mean that we assume that agents have the ability to asses whether their particular language requires using repair or not. So we could simply code up the communication function in such a way that if an agent's language uses neither the reduplication nor the diversify signal strategy, the agent automatically uses repair (when noise is encountered).\n",
    "\n",
    "### Pros and cons:\n",
    "\n",
    "We would prefer option 1 (repair is learned) because it feels like the most complete model: we allow for all possible combinations of strategies, and no strategy is a priori excluded (such as the combination of reduplication with repair). We also know from actual development that the conversational infrastructure of things like turn-taking and repair indeed needs to be learned. However, the disadvantage of option 1 is that it would double the hypothesis space (so if we go for possibility 1 in terms of representing languages, the hypothesis space would grow from 160,000 languages to 320,000 languages.\n",
    "\n",
    "\n",
    "Something that isn't particularly an advantage or a disadvantage, but simply a difference, is that adding in repair as a bit to the encoding of the language would slightly increase the coding length of the Repair + Compositionality strategy, and thereby minimise the difference in coding length between the Reduplication + Compositionality and the Repair + Compositionality strategies, which would lower the chance of the Repair + Compositionality strategy becoming dominant already in the {+learnability, -minimal effort} condition, so in the absence of the minimal effort pressure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Burkett, D., & Griffiths, T. L. (2010). Iterated learning of multiple languages from multiple teachers. The Evolution of Language: Proceedings of the 8th International Conference (EVOLANG8), Utrecht, Netherlands, 14-17 April 2010, 58–65.\n",
    "\n",
    "Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87–102. https://doi.org/10.1016/j.cognition.2015.03.016\n",
    "\n",
    "Spike, M. (2017). Thesis: Minimal requirements for the cultural evolution of language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
